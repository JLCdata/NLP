{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Entregable_Competencia 1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "E29LEMZ9zvIo",
        "OTAIEnSJzvIp",
        "kMOjYSQezvIq",
        "ECjkdgdwzvIq",
        "SJyTrr2onLOo",
        "oyj1fwJehpc3",
        "gzCUbD2ZRdJ-",
        "EYAfgeyrN2pf",
        "BsceEkM7ODs3"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:49:08.174519Z",
          "start_time": "2020-03-31T13:49:08.165989Z"
        },
        "id": "gpbvNOH0zvIi"
      },
      "source": [
        "# **Competencia 1 - CC6205 Natural Language Processing 📚**\n",
        "\n",
        "**Integrantes:** José Cadiz - Samuel Molina - Sebastián Tinoco - Stefano Schiappacasse\n",
        "\n",
        "**Usuario del equipo en CodaLab:** stinoco y stefanosch\n",
        "\n",
        "**Fecha límite de entrega 📆:** Miércoles 20 de Abril.\n",
        "\n",
        "**Tiempo estimado de dedicación:** musho (alrededor de 20-30 hrs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trabajo grupal**"
      ],
      "metadata": {
        "id": "aNFYF8O-Mhwd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:34:25.683540Z",
          "start_time": "2020-03-31T13:34:25.673430Z"
        },
        "id": "E29LEMZ9zvIo"
      },
      "source": [
        "## **1. Introducción**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "El presente reporte, contiene en detalle el desarrollo y resultados de la Competencia 1 del curso de Procesamiento de Lenguaje Natural CC6205-1. \n",
        "\n",
        "El objetivo de la competencia consiste en trabajar en la clasificación de tweets  según su intensidad de emoción, lo cual corresponde a la task de clasificación de texto. \n",
        "\n",
        "Particularmente, se cuenta con 4 datasets de tweets con distintas emociones: `anger`, `fear`, `sadness` y `joy`. Deberán crear un clasificador para cada uno de estos datasets que indique la intensidad de dicha emoción en sus tweets (`low`, `medium`, `high`).\n",
        "\n",
        "Para llevar a cabo esta tarea, se desarrollan 4 líneas de desarrollo: \n",
        "\n",
        "1. Experimentos con **Bag of Words**\n",
        "2. Experimentos con **TF-IDF**\n",
        "3. Experimentos con **Embeddings preentrenados**\n",
        "4. Experimentos con **Embeddings entrenados con la data**.\n",
        "\n",
        "En cada una de estas lineas de investigación, se probaron diferentes combinaciones de configuraciones para optimizar el poder de predicción de los clasificadores.\n",
        "\n",
        "Finalmente, se evaluan los resultados de los modelos desarrollador para posteriormente cargar el mejor en la competencia de [CodaLab](https://codalab.org/).\n"
      ],
      "metadata": {
        "id": "j0SQN6dkfEiU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:13.474238Z",
          "start_time": "2020-03-31T13:47:13.454068Z"
        },
        "id": "OTAIEnSJzvIp"
      },
      "source": [
        "## **2. Representaciones**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Representaciones**: Describir los atributos y representaciones usadas como entrada de los clasificadores, **recordar** que para entrenar modelos el input debe tener su representación numérica. Si bien, con Bag of Words (**baseline**) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluación agregando más atributos y representaciones diseñadas a mano, sean lo más creativos posible. Más abajo encontrarán una lista de estos posibles atributos que les podrá ser de utilidad. (**1 punto**)"
      ],
      "metadata": {
        "id": "Kd4bAzsag3Ye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se deja una lista de los atributos utilizados:\n",
        "\n",
        "* Una parte de los atributos extraidos fueron mediante el framework `awessome` que entrega un score que cuantifica la intensidad del sentimiento para cada frase.\n",
        "* Reconocedor de palabras negativas (no|not)\n",
        "* Cantidad de palabras positivas y negativas\n",
        "* Cantidad de emojis positivos y negativos \n",
        "* Cantidad de caracteres: #,!,?,@,* y mayusculas\n",
        "* Score de lexicon vader\n",
        "* Cantidad de palabras elongadas, cantidad de secuencias de ? y ! con un largo superior a 2.\n",
        "\n",
        "\n",
        "En cuanto a las representación del input utilizamos 3:\n",
        "* BOW\n",
        "* TF-IDF\n",
        "* Embeddings\n"
      ],
      "metadata": {
        "id": "pYNctSiBZbmU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMOjYSQezvIq"
      },
      "source": [
        "## **3. Algoritmos**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Algoritmos**: Describir **brevemente** los algoritmos de clasificación usados, tanto si fueron algoritmos ya vistos en clases o bien arquitecturas de Deep Learning. (**0.5 puntos**)"
      ],
      "metadata": {
        "id": "2aUQfbmjhFxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los algoritmos utilizados en esta competencia fueron 4; *RandomForest*, *SVC*, *MLP* y *XGBoostClassifier*. A continuación se describirá brevemente como funcionan.\n",
        "\n",
        "*   **Random Forest (RF)**: este es un algoritmo que ensambla múltiples árboles de decisión para poder realizar una predicción. Utiliza la técnica de emsamblaje denominada *bagging* que consiste en tomar muestras aleatorias del conjunto de datos de entrenamiento, y entrena un árbol para cada conjunto, luego cada uno de ellos realiza una predicción y luego se toman todas las predicciones de todos los árboles, se agregan de alguna manera, ya sea con el promedio (regresión), moda, entre otras, y este valor es el output del RF.\n",
        "*   **Support Vector Machine (SVC)**: este es un algoritmo que busca separar de mejor manera las distintas clases del conjunto de datos. Para ésto, busca un hiperplano que máximice el margen entre los vectores de soporte de cada clase. Un vector de soporte es el punto que está mas cerca del hiperplano y se tienen  por cada clase. Entonces el problema de clasificación se traduce a un problema de optimización en donde se busca encontrar el hiperplano que maximiza el margen. Como normalmente los datos no son perfectamente separables, el algoritmo se puede relajar para encontrar un hiperplano que no separe perfectamente las clases, sino que admita algunos datos mal clasificados a cambio de buscar un hiperplano que se ajuste completamente a los datos.\n",
        "*  **Multi Layer Perceptron (MLP)**: este algoritmo es una red neuronal que posee múltiples capas ocultas entre el input y el output. Entre cada capa hay parámetros, denominados pesos, que se van ajustando a medida que se va entrenando el modelo. Se necesita definir una función de pérdida que se buscar minimizar a través de algoritmos de optimización. La manera más conocida de entrenar un red es utilizando descenso de gradiente (en sus distintas versiones) en donde lo que se busca son los gradientes de la función de pérdida, que depende de los parámetros del modelo, para poder ir bajando en la dirección contraria al gradiente, de manera de buscar un mínimo. La gracia de estos modelos es que son muy versátiles y pueden aproximar a prácticamente cualquier función si se poseen muchos datos. Está basado en el Perceptrón que vendría a ser 1 neurona en la red, la cual recibe un input, le aplica una función (a definir) y retorna un output.\n",
        "*  **Extreme Gradient Boosting (XGBoost)**: al igual que el RF es un algoritmo que ensambla distintos árboles de decisión, pero no usando el *bagging* como en RF, sino que usando la técnica del *boosting* que consiste en construir varios modelos *weak* e ir potenciando los siguientes modelos en serie pasándoles como input el resultado obtenido. Luego lo que ocurre es que se parte con un árbol, se obtiene una predicción, esta predicción se guarda y aparte se le entrega como input al siguiente árbol, entonces éste vuelve a generar una predicción, también la guarda y se la pasa al siguiente árbol. Por lo tanto, al final voy a tener tantas predicciones como árboles tenga y éstas se agregan para tomar una predicción final.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ro1gCL_gu1H8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:52.064631Z",
          "start_time": "2020-03-31T13:47:52.044451Z"
        },
        "id": "ECjkdgdwzvIq"
      },
      "source": [
        "## **4. Métricas de Evaluación**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\t**Métricas de evaluación**: Describir brevemente las métricas utilizadas en la evaluación, indicando qué miden y su interpretación. (**0.5 puntos**)\n",
        "\n",
        "- **AUC**: el AUC viene de *area under the curve* cuya curva es la ROC *Receiver Operator Characteristic*, que relaciona el Tasa de verdaderos positivos (TPR) y la tasa de falsos positivos (FPR) para distintos valores de *threshold* (es el umbral de decisión que se utiliza para determinar si un ejemplo es de una clase u otra). La curva ROC evidencia la capacidad del modelo de separar las clases, y al tomar el área de ésta se genera una métrica que evalúa que tan bueno es el clasificador separando las clases. Cuando el AUC es 1, entonces el clasificador separa perfectamente las clases, cuando es 0 el clasificar entonces está identificando inversamente a las clases, de manera perfecta, y cuando es 0.5 implica que no tiene capacidad para reconocer la diferencia entre clases.\n",
        "- **Kappa**: es una metrica que evalua qué tan \"de acuerdo\" están las predicciones del clasificador con las etiquetas reales. Se basa en la matriz de confusión, al igual que el Accuracy y el AUC, y en base a esta calcula la probabilidad en que están de acuerdo (Accuracy), la probabilidad en que no están de acuerdo (todos los FN y los FP) y las suma (P_desacuerdo), y con estas dos probabilidades calcula la métrica haciendo ((P_acuerdo) - (P_desacuerdo)) / (1- (P_desacuerdo)). Esta métrica es más informativa cuando tenemos desbalance de clases.\n",
        "- **Accuracy**: el *Accuracy* calcula cuantas predicciones correctas hizo el modelo sobre el total de ejemplos en que se evaluó. Indica que tanto le achunta a la etiqueta real, pero deja invisibilizado el problema de desbalance de clase, lo cual es un problema, y por lo mismo se debe mirar en conjunto con otras métricas.\n"
      ],
      "metadata": {
        "id": "hFPbQPb1hJ-P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJyTrr2onLOo"
      },
      "source": [
        "## **5. Diseño experimental**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La metodología propuesta para la construcción de nuestro clasificador es la prueba-error de diferentes configuraciones (las cuales involucran tanto el pre procesamiento de la data como la búsqueda de los mejores hiperparámetros del clasificador) con tal de maximizar el `accuracy` del modelo. Bajo esta premisa, notamos que las posibilidades a explorar son mucho mas grandes en el preprocesamiento que en el clasificador mismo. Por lo mismo, la mayor parte de nuestro trabajo se dedica a probar diferentes configuraciones del procesamiento de los datos (es decir, diferentes combinaciones de como llega la data al clasificador) y solo al final buscar el clasificador óptimo para el sentimiento.\n",
        "\n",
        "Tomando esto en cuenta, nuestros experimentos se dividen en las siguientes etapas:\n",
        "\n",
        "1. Búsqueda de la configuración óptima (pre procesamiento):\n",
        " - Bag of Words\n",
        " - TF-IDF\n",
        " - Embeddings preentrenados\n",
        " - Embeddings entrenados con la data\n",
        "\n",
        "2. Búsqueda de clasificador por GridSearch\n",
        "\n",
        "3. Entrenamiento final\n",
        "\n",
        "A continuación, se detalla cada una de las etapas:\n",
        "\n",
        "1. Búsqueda de la configuración óptima: Para encontrar la mejor configuración de pre procesamiento, se comenzó dividiendo las configuraciones en 4 grandes metodologías de vectorización: (i) Bag of Words, (ii) TF-IDF, (iii) Embeddings preentrenados (*transfer learning*) y (iv) Embeddings entrenados con la data. Desde cada una, se prueban diferentes sub-configuraciones que abarcan: generación de features usando lexicones, normalización de tokens, reducción de dimensionalidad mediante PCA, resampling (undersampling, oversampling o la combinación de ambos), prueba de diferentes largos de vectores de embeddings, utilización de frameworks externos como `awessome`, entre otros. Como resultado, se espera tener una configuración de pre procesamiento óptima para cada sentimiento, donde la mejor configuración se eligirá en torno a la métrica `accuracy` usando un mismo clasificador y una semilla aleatoria para comparabilidad.\n",
        "\n",
        "2. Búsqueda de clasificador: Teniendo la configuración óptima fija, se busca optimizar el poder de predicción haciendo una búsqueda en grilla de los mejores hiperparámetros de los clasificadores, de nuevo optimizando la métrica `accuracy`. Un punto importante es que, como ya se encontó una configuración óptima para cada sentimiento, se \"precargan\" estas configuraciones antes de buscar la grilla con tal de reducir los tiempos de entrenamiento (en otras palabras, el objeto `HalvingGridSearch` toma solo la capa del clasificador y no la capa de preprocesamiento). A grandes rasgos, se prueban 3 algoritmos e hiperparámetros:\n",
        "\n",
        "- `Random Forest`:\n",
        "  - `n_estimators`: `[100, 500, 1000]`\n",
        "  - `max_depth`: `[80, 90, 100, 110]`\n",
        "  - `max_features`: `[2, 3]`\n",
        "  - `min_samples_leaf`: `[3, 4, 5]`\n",
        "  - `min_samples_split`: `[8, 10, 12]`\n",
        "\n",
        "- `XGBoost`:\n",
        "  - `max_depth`: `[3, 4, 5, 7]`\n",
        "  - `learning_rate`: `[0.1, 0.01, 0.05]`\n",
        "  - `gamma`: `[0, 0.25, 1]`\n",
        "  - `reg_lambda`: `[0, 1, 10]`\n",
        "  - `scale_pos_weight`: `[1, 3, 5]`\n",
        "\n",
        "- `SVM`\n",
        "  - `C`: `[1, 10, 100, 1000]`\n",
        "  - `kernel`: `['linear', 'rbf', 'poly']`\n",
        "  - `gamma`: `[0.001, 0.0001]`\n",
        "\n",
        "3. Entrenamiento final: Ya habiendo encontrado el modelo óptimo por sentimiento (tanto a nivel de pre procesamiento como en el clasificador), se entrena nuevamente este modelo pero esta vez con toda la data disponible (sin división train-test) para finalmente generar una predicción en el conjunto de test para subirlo a la plataforma **Codalab**."
      ],
      "metadata": {
        "id": "LK2W7bv6hZQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Experimentos**"
      ],
      "metadata": {
        "id": "oyj1fwJehpc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Librerías y funciones transversales**"
      ],
      "metadata": {
        "id": "iQTW6HheMrYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Librerías\n",
        "\n",
        "En esta sección se presentan e importan las librerías requeridas para el trabajo"
      ],
      "metadata": {
        "id": "n9OwU62gMza7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk import word_tokenize \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_selection import SelectPercentile, f_classif\n",
        "nltk.download('opinion_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = stopwords.words('english')\n",
        "from nltk.corpus import opinion_lexicon\n",
        "import re\n",
        "import imblearn\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from time import time  \n",
        "from collections import defaultdict \n",
        "import string \n",
        "import multiprocessing\n",
        "import os\n",
        "import requests\n",
        "\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.experimental import enable_halving_search_cv # noqa\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "import xgboost as xgb\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "!pip install umap-learn\n",
        "from umap import UMAP\n",
        "\n",
        "!pip install awessome\n",
        "from awessome.awessome_builder import *\n",
        "\n",
        "# awessome model\n",
        "builder = SentimentIntensityScorerBuilder('avg', 'bert-base-nli-mean-tokens', 'cosine', '600', True)\n",
        "scorer_vader = builder.build_scorer_from_prebuilt_lexicon('vader')\n",
        "scorer_labmt = builder.build_scorer_from_prebuilt_lexicon('labmt')\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
        "\n",
        "!pip install emosent-py\n",
        "from emosent import get_emoji_sentiment_rank\n",
        "\n",
        "!pip install emoji\n",
        "import emoji\n",
        "from emoji.core import emoji_count"
      ],
      "metadata": {
        "id": "yPNK2WnhNmNU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0285c231-c0e0-43e8-a177-4388f9b3d900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     |████████████████████████████████| 88 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 39.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.64.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=85cb67efb83f5183461777965aeb7d4eafd76114c6f17b49f784cfa2ebbd988d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=67b89615071564d3278f7e56ea79a558a8378ab37ba4e17522358401643c7ad5\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.6 umap-learn-0.5.3\n",
            "Collecting awessome\n",
            "  Downloading awessome-0.0.14-py2.py3-none-any.whl (34 kB)\n",
            "Collecting joblib==0.17.0\n",
            "  Downloading joblib-0.17.0-py3-none-any.whl (301 kB)\n",
            "\u001b[K     |████████████████████████████████| 301 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting urllib3==1.25.11\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 47.3 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.51.0\n",
            "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
            "\u001b[K     |████████████████████████████████| 70 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting dataclasses==0.6\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting regex==2020.10.23\n",
            "  Downloading regex-2020.10.23-cp37-cp37m-manylinux2010_x86_64.whl (662 kB)\n",
            "\u001b[K     |████████████████████████████████| 662 kB 53.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from awessome) (7.1.2)\n",
            "Collecting packaging==20.4\n",
            "  Downloading packaging-20.4-py2.py3-none-any.whl (37 kB)\n",
            "Collecting nltk==3.5\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 37.3 MB/s \n",
            "\u001b[?25hCollecting torch==1.7.0\n",
            "  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.7 MB 4.2 kB/s \n",
            "\u001b[?25hCollecting filelock==3.0.12\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Collecting typing-extensions==3.7.4.3\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting threadpoolctl==2.1.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting scikit-learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 40.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 47.0 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5 MB 23.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from awessome) (3.0.4)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting scipy==1.5.3\n",
            "  Downloading scipy-1.5.3-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9 MB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from awessome) (2.10)\n",
            "Collecting sentence-transformers==0.3.8\n",
            "  Downloading sentence-transformers-0.3.8.tar.gz (66 kB)\n",
            "\u001b[K     |████████████████████████████████| 66 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting certifi==2020.6.20\n",
            "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 68.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from awessome) (1.15.0)\n",
            "Collecting transformers==3.3.1\n",
            "  Downloading transformers-3.3.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 36.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.94\n",
            "  Downloading sentencepiece-0.1.94-cp37-cp37m-manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 43.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.43\n",
            "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
            "\u001b[K     |████████████████████████████████| 883 kB 47.6 MB/s \n",
            "\u001b[?25hCollecting requests==2.24.0\n",
            "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 307 kB/s \n",
            "\u001b[?25hCollecting future==0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 73.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: future, nltk, sacremoses, sentence-transformers\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=5db5653a69cdbd7517ac1f1a7e43a497a1d872930fea435a2abaafc06c105e6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434693 sha256=77829347da6169dc08f3d1bd934bb25aa2bbd3462f8603edba1fc0c9a7ce67da\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893249 sha256=c8f0123126ef3528ac8665d5edd34b8ba97e66b8b9534e67a62684873dfdeebf\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/09/d1/bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.8-py3-none-any.whl size=101994 sha256=0441a3bbe506d44d07d106b8eabc2505b630128922729901c489d580ebefb3fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/43/65/fe0f3ea9327623e749a79eb5dfad85a809c84064b1cc4682c1\n",
            "Successfully built future nltk sacremoses sentence-transformers\n",
            "Installing collected packages: urllib3, tqdm, regex, pyparsing, numpy, joblib, certifi, typing-extensions, tokenizers, threadpoolctl, sentencepiece, scipy, sacremoses, requests, packaging, future, filelock, dataclasses, transformers, torch, scikit-learn, nltk, sentence-transformers, awessome\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.8\n",
            "    Uninstalling pyparsing-3.0.8:\n",
            "      Successfully uninstalled pyparsing-3.0.8\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.10.8\n",
            "    Uninstalling certifi-2021.10.8:\n",
            "      Successfully uninstalled certifi-2021.10.8\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.1.0\n",
            "    Uninstalling threadpoolctl-3.1.0:\n",
            "      Successfully uninstalled threadpoolctl-3.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.6.0\n",
            "    Uninstalling filelock-3.6.0:\n",
            "      Successfully uninstalled filelock-3.6.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.24.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed awessome-0.0.14 certifi-2020.6.20 dataclasses-0.6 filelock-3.0.12 future-0.18.2 joblib-0.17.0 nltk-3.5 numpy-1.19.2 packaging-20.4 pyparsing-2.4.7 regex-2020.10.23 requests-2.24.0 sacremoses-0.0.43 scikit-learn-0.23.2 scipy-1.5.3 sentence-transformers-0.3.8 sentencepiece-0.1.94 threadpoolctl-2.1.0 tokenizers-0.8.1rc2 torch-1.7.0 tqdm-4.51.0 transformers-3.3.1 typing-extensions-3.7.4.3 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "dataclasses",
                  "joblib",
                  "nltk",
                  "numpy",
                  "packaging",
                  "pyparsing",
                  "regex",
                  "requests",
                  "scipy",
                  "sklearn",
                  "tqdm",
                  "typing_extensions",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 405M/405M [00:18<00:00, 21.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emosent-py\n",
            "  Downloading emosent-py-0.1.6.tar.gz (28 kB)\n",
            "Building wheels for collected packages: emosent-py\n",
            "  Building wheel for emosent-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emosent-py: filename=emosent_py-0.1.6-py3-none-any.whl size=28502 sha256=a20134b3ca9bb1c5d2db86c7c80fd3db04f37acffe89a103cee37f78c74ec141\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/37/bd/b4e67490f36c4beb85a1047d6cd13a356ffecbfa854eaf4688\n",
            "Successfully built emosent-py\n",
            "Installing collected packages: emosent-py\n",
            "Successfully installed emosent-py-0.1.6\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 5.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=85a447441bb63caa79751ab3a87147813c0dd0338b7cce7c2ed81a9b81af8d21\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Funciones de ejecución pipelines para BoW, Tf-Idf y Embeddings preentrenados"
      ],
      "metadata": {
        "id": "QMhaoJ9Klv1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resample(X, y, param_over = False, param_under = False):\n",
        "\n",
        "    '''\n",
        "    Función que recibe dataframes de entrada y los devuelve resampleados.\n",
        "    X: dataframe de entrada (X_train)\n",
        "    y: dataframe de entrada (y_train)\n",
        "    param_over: factor por el que se hará oversampling\n",
        "    param_under: factor por el que se hará subsampling\n",
        "    '''\n",
        "\n",
        "    # Oversampling\n",
        "    if param_over:\n",
        "      count_values = y.value_counts()\n",
        "      dict_over = {key: int(count_values[key] * (1 + param_over)) for key in count_values.keys()[1:]}\n",
        "\n",
        "      if X.ndim == 1:\n",
        "        X = np.array(X).reshape(-1, 1)\n",
        "      oversampler = RandomOverSampler(sampling_strategy = dict_over, random_state = 3380)\n",
        "      X, y = oversampler.fit_resample(X, y)\n",
        "      X = X.reshape(-1, )\n",
        "\n",
        "    # Subsampling\n",
        "    if param_under:\n",
        "      count_values = y.value_counts()\n",
        "      dict_under = {count_values.keys()[0]: int(count_values[0] * param_under)}\n",
        "      if X.ndim == 1:\n",
        "        X = np.array(X).reshape(-1, 1)\n",
        "\n",
        "      undersampler = RandomUnderSampler(sampling_strategy = dict_under, random_state = 3380)\n",
        "      X, y = undersampler.fit_resample(X, y)\n",
        "      X = X.reshape(-1, )\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "GN6v4HHyN6VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(dataset, dataset_name, pipeline, param_over = False, param_under = False):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test, aún no se transforma de Strings a valores numéricos.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataset.tweet,\n",
        "        dataset.sentiment_intensity,\n",
        "        shuffle=True,\n",
        "        test_size=0.33,\n",
        "        random_state = 3380)\n",
        "\n",
        "    # Balanceo de clases\n",
        "    X_train, y_train = resample(X_train, y_train, param_over, param_under)\n",
        "\n",
        "    print(f'# Datos de entrenamiento en dataset {dataset_name}: {len(X_train)}')\n",
        "    print(f'# Datos de testing en dataset {dataset_name}: {len(X_test)}')\n",
        "\n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline). \n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
        "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "    \n",
        "    # Evaluamos:\n",
        "    scores = evaluate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
        "    return pipeline, learned_labels, scores"
      ],
      "metadata": {
        "id": "teX64Gg4nzIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(pipeline, param_over = False, param_under = False):\n",
        "  classifiers = []\n",
        "  learned_labels_array = []\n",
        "  scores_array = []\n",
        "\n",
        "  # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "  for dataset_name, dataset in train.items():\n",
        "    \n",
        "    # creamos el pipeline\n",
        "    pipeline = pipeline\n",
        "    \n",
        "    # ejecutamos el pipeline sobre el dataset\n",
        "    classifier, learned_labels, scores = run(dataset, dataset_name, pipeline, param_over, param_under)\n",
        "\n",
        "    # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
        "    classifiers.append(classifier)\n",
        "\n",
        "    # guardamos las labels aprendidas por el clasificador\n",
        "    learned_labels_array.append(learned_labels)\n",
        "\n",
        "    # guardamos los scores obtenidos\n",
        "    scores_array.append(scores)\n",
        "\n",
        "  # print avg scores\n",
        "  print(\n",
        "    \"Average scores:\\n\\n\",\n",
        "    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
        "    .format(*np.array(scores_array).mean(axis=0)))\n",
        "  \n",
        "  return classifiers, learned_labels_array, scores_array"
      ],
      "metadata": {
        "id": "Yjw1WaO1oFYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FevBPus0zvIs"
      },
      "source": [
        "#### Definir métodos de evaluación (**NO tocar este código**)\n",
        "\n",
        "Estas funciones están a cargo de evaluar los resultados de la tarea. No deberían cambiarlas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.604066Z",
          "start_time": "2020-04-07T15:44:20.589106Z"
        },
        "id": "9wlllV7PzvIs"
      },
      "source": [
        "def auc_score(test_set, predicted_set):\n",
        "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
        "    medium_predicted = np.array(\n",
        "        [prediction[1] for prediction in predicted_set])\n",
        "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
        "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
        "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
        "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
        "    auc_high = roc_auc_score(high_test, high_predicted)\n",
        "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
        "    auc_low = roc_auc_score(low_test, low_predicted)\n",
        "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
        "             high_test.sum() * auc_high) / (\n",
        "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
        "    return auc_w\n",
        "\n",
        "\n",
        "def evaluate(predicted_probabilities, y_test, labels, dataset_name):\n",
        "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
        "    # entregar el arreglo de clases aprendido por el clasificador.\n",
        "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
        "    predicted_labels = [\n",
        "        labels[np.argmax(item)] for item in predicted_probabilities\n",
        "    ]\n",
        "\n",
        "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
        "    print(\n",
        "        confusion_matrix(y_test,\n",
        "                         predicted_labels,\n",
        "                         labels=['low', 'medium', 'high']))\n",
        "\n",
        "    print('\\nClassification Report:\\n')\n",
        "    print(\n",
        "        classification_report(y_test,\n",
        "                              predicted_labels,\n",
        "                              labels=['low', 'medium', 'high']))\n",
        "    # Reorder predicted probabilities array.\n",
        "    labels = labels.tolist()\n",
        "    \n",
        "    predicted_probabilities = predicted_probabilities[:, [\n",
        "        labels.index('low'),\n",
        "        labels.index('medium'),\n",
        "        labels.index('high')\n",
        "    ]]\n",
        "    \n",
        "    \n",
        "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
        "    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
        "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
        "    print(\"Kappa:\", kappa, end='\\t')\n",
        "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print('------------------------------------------------------\\n')\n",
        "    return np.array([auc, kappa, accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkOP6ugwzvIt"
      },
      "source": [
        "#### Datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.068137Z",
          "start_time": "2020-04-07T15:44:20.606061Z"
        },
        "id": "D1XhFPhrzvIt"
      },
      "source": [
        "# Datasets de entrenamiento.\n",
        "train = {\n",
        "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
        "}\n",
        "# Datasets que deberán predecir para la competencia.\n",
        "target = {\n",
        "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.088707Z",
          "start_time": "2020-04-07T15:44:21.069757Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "flg2Zw2mzvIt",
        "outputId": "8acd4a57-7c38-4507-cd1e-a2620df5eda9"
      },
      "source": [
        "# Ejemplo de algunas filas aleatorias del dataset etiquetado:\n",
        "train['anger'].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                              tweet  class  \\\n",
              "829  10829  follow my girl tiff she only got 3 followers💖💘...  anger   \n",
              "249  10249  You will never find someone who loved you like...  anger   \n",
              "280  10280  the outrage has passed and has been replaced b...  anger   \n",
              "166  10166  @SkyUK what a joke!! Cut our internet off earl...  anger   \n",
              "552  10552  Realizing that holding a grudge for long is im...  anger   \n",
              "\n",
              "    sentiment_intensity  \n",
              "829                 low  \n",
              "249              medium  \n",
              "280              medium  \n",
              "166              medium  \n",
              "552              medium  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ffbb2d0e-534f-4aa9-8bfe-0dd736b4c7be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>class</th>\n",
              "      <th>sentiment_intensity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>829</th>\n",
              "      <td>10829</td>\n",
              "      <td>follow my girl tiff she only got 3 followers💖💘...</td>\n",
              "      <td>anger</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>10249</td>\n",
              "      <td>You will never find someone who loved you like...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>10280</td>\n",
              "      <td>the outrage has passed and has been replaced b...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>10166</td>\n",
              "      <td>@SkyUK what a joke!! Cut our internet off earl...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552</th>\n",
              "      <td>10552</td>\n",
              "      <td>Realizing that holding a grudge for long is im...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ffbb2d0e-534f-4aa9-8bfe-0dd736b4c7be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ffbb2d0e-534f-4aa9-8bfe-0dd736b4c7be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ffbb2d0e-534f-4aa9-8bfe-0dd736b4c7be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XB7hb7KH2DFK",
        "outputId": "32127426-6292-4809-eeed-1e23c985cce9"
      },
      "source": [
        "# Ejemplo de algunas filas aleatorias del dataset no etiquetado\n",
        "target['anger'].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                              tweet  class  \\\n",
              "528  11469  @SMalloy_LWOS that's not easy to blow up the L...  anger   \n",
              "424  11365  There are adults commiting serious crimes in t...  anger   \n",
              "37   10978  @TheRevAl please tell us why 'protesting' inju...  anger   \n",
              "297  11238  #GBBO is such a homely pure piece of tv gold. ...  anger   \n",
              "10   10951       @shae_caitlin ur road rage gives me anxiety.  anger   \n",
              "\n",
              "     sentiment_intensity  \n",
              "528                  NaN  \n",
              "424                  NaN  \n",
              "37                   NaN  \n",
              "297                  NaN  \n",
              "10                   NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f3a137b5-187c-47e7-9ca2-8a8fe10559fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>class</th>\n",
              "      <th>sentiment_intensity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>528</th>\n",
              "      <td>11469</td>\n",
              "      <td>@SMalloy_LWOS that's not easy to blow up the L...</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>11365</td>\n",
              "      <td>There are adults commiting serious crimes in t...</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>10978</td>\n",
              "      <td>@TheRevAl please tell us why 'protesting' inju...</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>11238</td>\n",
              "      <td>#GBBO is such a homely pure piece of tv gold. ...</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10951</td>\n",
              "      <td>@shae_caitlin ur road rage gives me anxiety.</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3a137b5-187c-47e7-9ca2-8a8fe10559fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f3a137b5-187c-47e7-9ca2-8a8fe10559fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f3a137b5-187c-47e7-9ca2-8a8fe10559fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Objeto FeaturesTransform para Tf-Idf y Embeddings preentrenados"
      ],
      "metadata": {
        "id": "VCnGm_Xkw4MD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este objeto preprocesa los tweets y extrae diversas características. Esta será utilizada en Tf-Idf y Embeddings preentrenados."
      ],
      "metadata": {
        "id": "lq_hLFpT01op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload() #para cargar archivo \"Emoji Sentiment Ranking 1.0.txt\"  (está en el drive, podríamos cargarlo desde ahí pero no sé q tan cómodo sea pa revisar... c analiza)"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "4tvs7Qxn0roD",
        "outputId": "86ac7d25-116a-4ed4-8d6c-521e0c855203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-334370d1-ef2c-4862-976b-90c0d5331b41\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-334370d1-ef2c-4862-976b-90c0d5331b41\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Emoji Sentiment Ranking 1.0.txt to Emoji Sentiment Ranking 1.0.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lee archivo y normaliza rankings\n",
        "listaemojis = pd.read_csv(\"Emoji Sentiment Ranking 1.0.txt\")\n",
        "listaemojis=listaemojis[listaemojis.Occurrences>5]\n",
        "listaemojis.Negative=listaemojis.Negative/listaemojis.Occurrences\n",
        "listaemojis.Positive=listaemojis.Positive/listaemojis.Occurrences\n",
        "listaemojis.Neutral=listaemojis.Neutral/listaemojis.Occurrences\n",
        "listaemojis.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "4F5b0Li-00qZ",
        "outputId": "7f0e79b1-e057-4b94-9ec7-0a477405c790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Emoji Unicode codepoint  Occurrences  Position  Negative   Neutral  \\\n",
              "0     😂           0x1f602        14622  0.805101  0.247162  0.284708   \n",
              "1     ❤            0x2764         8050  0.746943  0.044099  0.165714   \n",
              "2     ♥            0x2665         7144  0.753806  0.035274  0.271837   \n",
              "3     😍           0x1f60d         6359  0.765292  0.051738  0.218588   \n",
              "4     😭           0x1f62d         5526  0.803352  0.436482  0.220413   \n",
              "\n",
              "   Positive                         Unicode name          Unicode block  \n",
              "0  0.468130               FACE WITH TEARS OF JOY              Emoticons  \n",
              "1  0.790186                    HEAVY BLACK HEART               Dingbats  \n",
              "2  0.692889                     BLACK HEART SUIT  Miscellaneous Symbols  \n",
              "3  0.729674  SMILING FACE WITH HEART-SHAPED EYES              Emoticons  \n",
              "4  0.343105                   LOUDLY CRYING FACE              Emoticons  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff4fd901-3f2c-44e9-b067-62c1512d0798\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emoji</th>\n",
              "      <th>Unicode codepoint</th>\n",
              "      <th>Occurrences</th>\n",
              "      <th>Position</th>\n",
              "      <th>Negative</th>\n",
              "      <th>Neutral</th>\n",
              "      <th>Positive</th>\n",
              "      <th>Unicode name</th>\n",
              "      <th>Unicode block</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>😂</td>\n",
              "      <td>0x1f602</td>\n",
              "      <td>14622</td>\n",
              "      <td>0.805101</td>\n",
              "      <td>0.247162</td>\n",
              "      <td>0.284708</td>\n",
              "      <td>0.468130</td>\n",
              "      <td>FACE WITH TEARS OF JOY</td>\n",
              "      <td>Emoticons</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>❤</td>\n",
              "      <td>0x2764</td>\n",
              "      <td>8050</td>\n",
              "      <td>0.746943</td>\n",
              "      <td>0.044099</td>\n",
              "      <td>0.165714</td>\n",
              "      <td>0.790186</td>\n",
              "      <td>HEAVY BLACK HEART</td>\n",
              "      <td>Dingbats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>♥</td>\n",
              "      <td>0x2665</td>\n",
              "      <td>7144</td>\n",
              "      <td>0.753806</td>\n",
              "      <td>0.035274</td>\n",
              "      <td>0.271837</td>\n",
              "      <td>0.692889</td>\n",
              "      <td>BLACK HEART SUIT</td>\n",
              "      <td>Miscellaneous Symbols</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>😍</td>\n",
              "      <td>0x1f60d</td>\n",
              "      <td>6359</td>\n",
              "      <td>0.765292</td>\n",
              "      <td>0.051738</td>\n",
              "      <td>0.218588</td>\n",
              "      <td>0.729674</td>\n",
              "      <td>SMILING FACE WITH HEART-SHAPED EYES</td>\n",
              "      <td>Emoticons</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>😭</td>\n",
              "      <td>0x1f62d</td>\n",
              "      <td>5526</td>\n",
              "      <td>0.803352</td>\n",
              "      <td>0.436482</td>\n",
              "      <td>0.220413</td>\n",
              "      <td>0.343105</td>\n",
              "      <td>LOUDLY CRYING FACE</td>\n",
              "      <td>Emoticons</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff4fd901-3f2c-44e9-b067-62c1512d0798')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff4fd901-3f2c-44e9-b067-62c1512d0798 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff4fd901-3f2c-44e9-b067-62c1512d0798');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El framework `awessome` es una herramienta creada por [Amal Htait](http://amalhtait.com) la cual utiliza BERT en conjunto con los lexicones `vader` y `labmt` para generar un score de intensidad de sentimiento para una frase. Como esta feature se adecua mucho al problema, se implementa al listado de features."
      ],
      "metadata": {
        "id": "eCiybUpDHapy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# awessome model\n",
        "builder = SentimentIntensityScorerBuilder('avg', 'bert-base-nli-mean-tokens', 'cosine', '600', True)\n",
        "scorer_vader = builder.build_scorer_from_prebuilt_lexicon('vader')\n",
        "scorer_labmt = builder.build_scorer_from_prebuilt_lexicon('labmt')"
      ],
      "metadata": {
        "id": "JXysHIWo1Iuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeaturesTransformer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pos_set, self.neg_set = set(opinion_lexicon.positive()), set(opinion_lexicon.negative()) # lexicon\n",
        "        self.tokenizer = TweetTokenizer(preserve_case = False) # tokenizador a usar, NO preservamos las mayusculas\n",
        "        self.NEG = r\"(?:^(?:no|not)$)|n't\" # reconoce palabras negativas\n",
        "        self.analyzer = SentimentIntensityAnalyzer() # lexicon vader\n",
        "        self.scorer_vader, self.scorer_labmt = scorer_vader, scorer_labmt # awessome models\n",
        "\n",
        "    def liu_score(self, tokenized_sent: list):\n",
        "\n",
        "        '''\n",
        "        Función que retorna la cantidad de tokens positivos y negativos dado un lexicon\n",
        "        tokenized_sent: lista con los tokens\n",
        "        '''\n",
        "\n",
        "        pos_words, neg_words = 0, 0\n",
        "        for word in tokenized_sent:\n",
        "            if word.lower() in self.pos_set:\n",
        "                pos_words += 1\n",
        "            elif word.lower() in self.neg_set:\n",
        "                neg_words += 1\n",
        "        return pos_words, neg_words\n",
        "\n",
        "    def count_sentiment_emoji(self, tweet: str):\n",
        "\n",
        "        '''\n",
        "        Función que retorna la cantidad de emojis positivos y negativos en un string (tweet).\n",
        "        tweet: string contenedor de palabras y emojis.\n",
        "        '''\n",
        "\n",
        "        count_emoji_neg = 0\n",
        "        count_emoji_pos = 0\n",
        "        list_emoji = emoji.distinct_emoji_list(tweet)\n",
        "        for i in range(len(list_emoji)):\n",
        "          emoticon = list_emoji[i]\n",
        "          if emoticon in list(listaemojis[\"Emoji\"]):\n",
        "            neg = listaemojis[listaemojis['Emoji']==emoticon].iloc[0, 4]\n",
        "            pos = listaemojis[listaemojis['Emoji']==emoticon].iloc[0, 6]\n",
        "            if neg > pos:\n",
        "              count_emoji_neg = count_emoji_neg + 1\n",
        "            else:\n",
        "              count_emoji_pos = count_emoji_pos + 1  \n",
        "              \n",
        "        return count_emoji_neg, count_emoji_pos \n",
        "\n",
        "    def pre_processing(self, tweet: str):\n",
        "\n",
        "        '''\n",
        "        Función que realiza un pre procesamiento al texto.\n",
        "        tweet: string con el texto a pre procesar\n",
        "        '''\n",
        "\n",
        "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet) #elimina urls\n",
        "        tweet = re.sub('(@[^\\s]+)|(w/)','', tweet) #elimina menciones\n",
        "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #le quita el # a los hashtags\n",
        "        tweet = re.sub('\\n','', tweet) #elimina saltos de línea\n",
        "        return tweet\n",
        "\n",
        "    def get_relevant_chars(self, tweet: str):\n",
        "\n",
        "        '''\n",
        "        Función que, a partir de un documento, devuelve una serie de features a partir de sus carácteres y sus tokens\n",
        "        tweet: documento en formato de string\n",
        "        '''\n",
        "\n",
        "        # features de carácteres\n",
        "        num_hashtags = tweet.count('#') # cantidad de #\n",
        "        num_exclamations = tweet.count('!') # cantidad de !\n",
        "        num_interrogations = tweet.count('?') # cantidad de ?\n",
        "        num_at = tweet.count('@') # cantidad de @\n",
        "        num_asterisk = tweet.count('*')\n",
        "        num_capital = sum(1 for c in tweet if c.isupper()) # cantidad de caracteres en mayúscula\n",
        "        num_emoji = emoji_count(tweet) # cantidad de emojis en el tweet\n",
        "        num_emoji_neg, num_emoji_pos = self.count_sentiment_emoji(tweet) # cantidad de emojis positivos y negativos en el tweet\n",
        "\n",
        "        tweet = self.pre_processing(tweet)\n",
        "        vader_comp, vader_neg, vader_neu, vader_pos = self.analyzer.polarity_scores(tweet).values() # scores de lexicon vader\n",
        "        awessome_vader, awessome_labmt = self.scorer_vader.score_sentence(tweet), self.scorer_labmt.score_sentence(tweet) # scores generados por framework awessome\n",
        "\n",
        "        # features de tokens\n",
        "\n",
        "        tokens = self.tokenizer.tokenize(tweet) # tokenizamos tweet\n",
        "        pos_count, neg_count = self.liu_score(tokens) # cantidad de tokens positivas y negativas\n",
        "        num_upper = sum(map(str.isupper, tokens)) # cantidad de palabras en mayúscula\n",
        "        num_elongated = sum([1 for token in tokens if re.compile(r\"(.)\\1{2}\").search(token)]) # cantidad de palabras elongadas\n",
        "        num_sequence = sum([1 for token in tokens if re.compile(r\"(.+?)[\\.?!]{2,}\").search(token)]) # cantidad de secuencias de ., ? y ! con largo 2 o superior\n",
        "        num_negwords = sum([1 for token in tokens if re.compile(self.NEG, re.VERBOSE).search(token)]) # cantidad de negaciones\n",
        "\n",
        "        # output\n",
        "        features = [num_hashtags, num_exclamations, num_interrogations, num_at, num_asterisk, num_capital, pos_count, num_emoji, num_emoji_pos, num_emoji_neg,\n",
        "                    neg_count, num_upper, num_elongated, num_sequence, num_negwords, vader_comp, vader_neg, vader_neu, vader_pos,\n",
        "                    awessome_vader, awessome_labmt]\n",
        "        return features\n",
        "\n",
        "    def transform(self, X, y = None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(self.get_relevant_chars(tweet))\n",
        "\n",
        "        return np.array(chars)\n",
        "\n",
        "    def fit(self, X, y = None):\n",
        "        return self"
      ],
      "metadata": {
        "id": "td4R6poCxCRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos que sucede si ejecutamos el transformer\n",
        "sample = train['sadness'].sample(69, random_state = 50).tweet\n",
        "sample_features = FeaturesTransformer().transform(sample)\n",
        "\n",
        "# Se puede verificar que el conteo de símbolos es consistente con el transformer creado.\n",
        "print(f'Tweet original: {sample.iloc[0]}')\n",
        "print(f'Features creados: {sample_features[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6_robZG1et9",
        "outputId": "b963b54f-68b0-4f49-a9c7-f7a8cefa1a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet original: @harowe this sad truth!\n",
            "Features creados: [ 0.          1.          0.          1.          0.          0.\n",
            "  0.          0.          0.          0.          1.          0.\n",
            "  0.          0.          0.          0.507       0.149       0.344\n",
            " -0.2714     -0.31166625 -0.16009769]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bag of Words**"
      ],
      "metadata": {
        "id": "RHjyaxf0MlPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Objetos"
      ],
      "metadata": {
        "id": "or1dgyvijaVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clase tokenizador con Stemming, usado en BOW (CountVectorizer)\n",
        "class StemmerTokenizer:\n",
        "    def __init__(self):\n",
        "        self.ps = PorterStemmer()\n",
        "    def __call__(self, doc):\n",
        "        doc_tok = word_tokenize(doc)\n",
        "        doc_tok = [t for t in doc_tok if t not in stop_words]\n",
        "        return [self.ps.stem(t) for t in doc_tok]"
      ],
      "metadata": {
        "id": "plMEhifao-l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta Clase permite contar la cantidad de palabras positivas y negatvas contenidas en el tweet para luego incorporarse como features\n",
        "class LiuFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, tokenizer=TweetTokenizer(preserve_case=False, reduce_len=True)):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pos_set = set(opinion_lexicon.positive())\n",
        "        self.neg_set = set(opinion_lexicon.negative())\n",
        "\n",
        "    def __liu_score(self,sentence):\n",
        "        tokenized_sent = self.tokenizer.tokenize(sentence)\n",
        "        pos_words, neg_words = 0, 0\n",
        "        for word in tokenized_sent:\n",
        "            if word in self.pos_set:\n",
        "                pos_words += 1\n",
        "            elif word in self.neg_set:\n",
        "                neg_words += 1\n",
        "        return [pos_words,neg_words]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        values = []\n",
        "        for tweet in X:\n",
        "            values.append(self.__liu_score(tweet))\n",
        "\n",
        "        return(np.array(values))\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ],
      "metadata": {
        "id": "_ACLFzJQpQvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
        "    def get_relevant_chars(self, tweet):\n",
        "        num_hashtags = tweet.count('#')\n",
        "        num_exclamations = tweet.count('!')\n",
        "        num_interrogations = tweet.count('?')\n",
        "        num_at = tweet.count('@')\n",
        "\n",
        "        # cuenta la cantidad de emojis que aparece para uno de los emojis aprendido del conjunto de entrenamiento\n",
        "        # Para luego obtener una proporción de emojis aparecido en el tweet\n",
        "        dic={} # diccioario que tiene como llave cada emoji del train\n",
        "        for i in self.lista_emoji: # Para cada emoji se cuentan cuantos hay en el tweet\n",
        "            dic[i]=tweet.count(i)\n",
        "        list_final= list(dic.values())\n",
        "        num_emoji=emoji_count(tweet) # número de emojis en el tweet\n",
        "        if num_emoji==0: # si no habia ningun emoji se redefine como 1 para no dividir por cero\n",
        "            num_emoji=1\n",
        "        \n",
        "        # Finalmente se agrega la tasa de emojis y el número de emojis como features adicionales\n",
        "        return [num_hashtags, num_exclamations, num_interrogations, num_at]+list(map(lambda x: x /num_emoji, list_final))+[num_emoji]\n",
        "        \n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(self.get_relevant_chars(tweet))\n",
        "\n",
        "        return np.array(chars)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Lista de emojis aprendidos en train \n",
        "        self.lista_emoji=X.apply(lambda x: emoji.distinct_emoji_list(x)).sum()\n",
        "        return self"
      ],
      "metadata": {
        "id": "yxuW7JrKM0JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experimentos"
      ],
      "metadata": {
        "id": "QOIPXithjj-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exp 1: BOW, 1,2-gramas con selección de variables del 5% con un XGboost\n",
        "def get_experiment_1_pipeline():\n",
        "    model= xgb.XGBClassifier(seed=1,eval_metric='mlogloss',learning_rate=0.01,gamma=1.3,n_estimators=700,max_depth=10)\n",
        "\n",
        "    return Pipeline(\n",
        "    [\n",
        "        (\"preprocesamiento\", FeatureUnion([('bow',CountVectorizer(tokenizer= StemmerTokenizer(),\n",
        "    ngram_range=(1,2))),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ])),\n",
        "                (\"Selection\", SelectPercentile(f_classif, percentile=5)), #mutual_info_regression\n",
        "     (\"clf\", model)]\n",
        ")"
      ],
      "metadata": {
        "id": "Uk-2ehJRnxfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_1_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUhg9gzqoYjA",
        "outputId": "bcf0efd9-aebc-4555-fa7d-e174cbdcb74c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  2  55   1]\n",
            " [  5 198   8]\n",
            " [  1  24  17]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.25      0.03      0.06        58\n",
            "      medium       0.71      0.94      0.81       211\n",
            "        high       0.65      0.40      0.50        42\n",
            "\n",
            "    accuracy                           0.70       311\n",
            "   macro avg       0.54      0.46      0.46       311\n",
            "weighted avg       0.62      0.70      0.63       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.673\tKappa: 0.204\tAccuracy: 0.698\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 21  66   0]\n",
            " [  8 213  25]\n",
            " [  1  51  30]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.70      0.24      0.36        87\n",
            "      medium       0.65      0.87      0.74       246\n",
            "        high       0.55      0.37      0.44        82\n",
            "\n",
            "    accuracy                           0.64       415\n",
            "   macro avg       0.63      0.49      0.51       415\n",
            "weighted avg       0.64      0.64      0.60       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.691\tKappa: 0.253\tAccuracy: 0.636\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 11  55   6]\n",
            " [  7 147  19]\n",
            " [  0  37  16]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.61      0.15      0.24        72\n",
            "      medium       0.62      0.85      0.71       173\n",
            "        high       0.39      0.30      0.34        53\n",
            "\n",
            "    accuracy                           0.58       298\n",
            "   macro avg       0.54      0.43      0.43       298\n",
            "weighted avg       0.57      0.58      0.53       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.678\tKappa: 0.16\tAccuracy: 0.584\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  7  64   2]\n",
            " [  5 130  25]\n",
            " [  1  31  19]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.54      0.10      0.16        73\n",
            "      medium       0.58      0.81      0.68       160\n",
            "        high       0.41      0.37      0.39        51\n",
            "\n",
            "    accuracy                           0.55       284\n",
            "   macro avg       0.51      0.43      0.41       284\n",
            "weighted avg       0.54      0.55      0.49       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.659\tKappa: 0.121\tAccuracy: 0.549\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.675\t Average Kappa: 0.184\t Average Accuracy: 0.617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exp 2: BOW 1,2-gramas  con PCA n=60 con XGboost\n",
        "def get_experiment_2_pipeline():\n",
        "    model2= xgb.XGBClassifier(seed=1,eval_metric='mlogloss',learning_rate=0.1,gamma=1.3,n_estimators=700,max_depth=10)\n",
        "    return  Pipeline(\n",
        "    [\n",
        "        (\"preprocesamiento\", FeatureUnion([('bow',CountVectorizer(tokenizer= StemmerTokenizer(),\n",
        "    ngram_range=(1,2))),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ])),\n",
        "                (\"PCA\",TruncatedSVD(n_components=60, n_iter=7, random_state=42)), \n",
        "     (\"clf\", model2)]\n",
        ")"
      ],
      "metadata": {
        "id": "U2ODxP4Kn3Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_2_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcK_SJMmu8K4",
        "outputId": "fddebb5a-db05-4217-be3a-c84684b16f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  4  54   0]\n",
            " [  8 200   3]\n",
            " [  2  31   9]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.29      0.07      0.11        58\n",
            "      medium       0.70      0.95      0.81       211\n",
            "        high       0.75      0.21      0.33        42\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.58      0.41      0.42       311\n",
            "weighted avg       0.63      0.68      0.61       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.583\tKappa: 0.136\tAccuracy: 0.685\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 20  65   2]\n",
            " [ 26 199  21]\n",
            " [  6  51  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.38      0.23      0.29        87\n",
            "      medium       0.63      0.81      0.71       246\n",
            "        high       0.52      0.30      0.38        82\n",
            "\n",
            "    accuracy                           0.59       415\n",
            "   macro avg       0.51      0.45      0.46       415\n",
            "weighted avg       0.56      0.59      0.56       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.635\tKappa: 0.177\tAccuracy: 0.588\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 18  52   2]\n",
            " [ 22 139  12]\n",
            " [  2  36  15]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.43      0.25      0.32        72\n",
            "      medium       0.61      0.80      0.69       173\n",
            "        high       0.52      0.28      0.37        53\n",
            "\n",
            "    accuracy                           0.58       298\n",
            "   macro avg       0.52      0.45      0.46       298\n",
            "weighted avg       0.55      0.58      0.54       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.658\tKappa: 0.165\tAccuracy: 0.577\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 21  47   5]\n",
            " [ 13 114  33]\n",
            " [  1  27  23]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.60      0.29      0.39        73\n",
            "      medium       0.61      0.71      0.66       160\n",
            "        high       0.38      0.45      0.41        51\n",
            "\n",
            "    accuracy                           0.56       284\n",
            "   macro avg       0.53      0.48      0.48       284\n",
            "weighted avg       0.56      0.56      0.54       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.629\tKappa: 0.203\tAccuracy: 0.556\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.626\t Average Kappa: 0.17\t Average Accuracy: 0.602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exp 3 BOW 1,2-gramas, con PCAn=60 y RN de 3 capas\n",
        "def get_experiment_3_pipeline():\n",
        "    model3 = MLPClassifier(hidden_layer_sizes=(3,3,10), max_iter=1000,random_state=1)\n",
        "    return  Pipeline(\n",
        "    [\n",
        "        (\"preprocesamiento\", FeatureUnion([('bow',CountVectorizer(tokenizer= StemmerTokenizer(),\n",
        "    ngram_range=(1,2))),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ])),\n",
        "                (\"PCA\",TruncatedSVD(n_components=60, n_iter=7, random_state=42)), \n",
        "     (\"clf\", model3)]\n",
        ")"
      ],
      "metadata": {
        "id": "KTw6mRimn4PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_3_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ATBisTzu-DL",
        "outputId": "192c8f3f-3462-4f8e-b659-28f46e69aefb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  3  54   1]\n",
            " [  8 195   8]\n",
            " [  1  30  11]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.25      0.05      0.09        58\n",
            "      medium       0.70      0.92      0.80       211\n",
            "        high       0.55      0.26      0.35        42\n",
            "\n",
            "    accuracy                           0.67       311\n",
            "   macro avg       0.50      0.41      0.41       311\n",
            "weighted avg       0.60      0.67      0.60       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.569\tKappa: 0.127\tAccuracy: 0.672\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[  6  75   6]\n",
            " [  3 196  47]\n",
            " [  3  46  33]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.50      0.07      0.12        87\n",
            "      medium       0.62      0.80      0.70       246\n",
            "        high       0.38      0.40      0.39        82\n",
            "\n",
            "    accuracy                           0.57       415\n",
            "   macro avg       0.50      0.42      0.40       415\n",
            "weighted avg       0.55      0.57      0.52       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.608\tKappa: 0.133\tAccuracy: 0.566\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[  0  70   2]\n",
            " [  0 148  25]\n",
            " [  0  35  18]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.00      0.00      0.00        72\n",
            "      medium       0.58      0.86      0.69       173\n",
            "        high       0.40      0.34      0.37        53\n",
            "\n",
            "    accuracy                           0.56       298\n",
            "   macro avg       0.33      0.40      0.35       298\n",
            "weighted avg       0.41      0.56      0.47       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.624\tKappa: 0.078\tAccuracy: 0.557\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  8  64   1]\n",
            " [ 13 121  26]\n",
            " [  3  22  26]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.33      0.11      0.16        73\n",
            "      medium       0.58      0.76      0.66       160\n",
            "        high       0.49      0.51      0.50        51\n",
            "\n",
            "    accuracy                           0.55       284\n",
            "   macro avg       0.47      0.46      0.44       284\n",
            "weighted avg       0.50      0.55      0.50       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.606\tKappa: 0.15\tAccuracy: 0.546\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.602\t Average Kappa: 0.122\t Average Accuracy: 0.585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  activations.append(np.empty((X.shape[0],\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exp 4 BOW 1,2-gramas, PCA n=60 con SVM lineal\n",
        "def get_experiment_4_pipeline():\n",
        "    model4= svm.SVC(kernel='linear', probability=True)\n",
        "    return  Pipeline(\n",
        "    [\n",
        "        (\"preprocesamiento\", FeatureUnion([('bow',CountVectorizer(tokenizer= StemmerTokenizer(),\n",
        "    ngram_range=(1,2))),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ])),\n",
        "                (\"PCA\",TruncatedSVD(n_components=60, n_iter=7, random_state=42)), \n",
        "     (\"clf\", model4)]\n",
        ")"
      ],
      "metadata": {
        "id": "ULeQtTDbn66d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_4_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--YuveYWvBVx",
        "outputId": "4a59bff5-503e-4edf-8120-b7bf479aa426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  0  58   0]\n",
            " [  0 211   0]\n",
            " [  0  42   0]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.00      0.00      0.00        58\n",
            "      medium       0.68      1.00      0.81       211\n",
            "        high       0.00      0.00      0.00        42\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.23      0.33      0.27       311\n",
            "weighted avg       0.46      0.68      0.55       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.586\tKappa: 0.0\tAccuracy: 0.678\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for fear:\n",
            "\n",
            "[[  0  87   0]\n",
            " [  0 243   3]\n",
            " [  0  77   5]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.00      0.00      0.00        87\n",
            "      medium       0.60      0.99      0.74       246\n",
            "        high       0.62      0.06      0.11        82\n",
            "\n",
            "    accuracy                           0.60       415\n",
            "   macro avg       0.41      0.35      0.29       415\n",
            "weighted avg       0.48      0.60      0.46       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.614\tKappa: 0.03\tAccuracy: 0.598\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for joy:\n",
            "\n",
            "[[  0  72   0]\n",
            " [  0 169   4]\n",
            " [  0  44   9]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.00      0.00      0.00        72\n",
            "      medium       0.59      0.98      0.74       173\n",
            "        high       0.69      0.17      0.27        53\n",
            "\n",
            "    accuracy                           0.60       298\n",
            "   macro avg       0.43      0.38      0.34       298\n",
            "weighted avg       0.47      0.60      0.48       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.653\tKappa: 0.079\tAccuracy: 0.597\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  3  70   0]\n",
            " [  1 148  11]\n",
            " [  0  42   9]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.75      0.04      0.08        73\n",
            "      medium       0.57      0.93      0.70       160\n",
            "        high       0.45      0.18      0.25        51\n",
            "\n",
            "    accuracy                           0.56       284\n",
            "   macro avg       0.59      0.38      0.35       284\n",
            "weighted avg       0.59      0.56      0.46       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.665\tKappa: 0.067\tAccuracy: 0.563\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.629\t Average Kappa: 0.044\t Average Accuracy: 0.609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exp 5 BOW 1,2-gramas, PCA n=60 con SVM rbf\n",
        "def get_experiment_5_pipeline():\n",
        "    model5= svm.SVC(kernel='rbf',probability=True)\n",
        "    return  Pipeline(\n",
        "    [\n",
        "        (\"preprocesamiento\", FeatureUnion([('bow',CountVectorizer(tokenizer= StemmerTokenizer(),\n",
        "    ngram_range=(1,2))),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ])),\n",
        "                (\"PCA\",TruncatedSVD(n_components=60, n_iter=7, random_state=42)), \n",
        "     (\"clf\", model5)]\n",
        ")"
      ],
      "metadata": {
        "id": "6F-99lobn9Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_5_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9bzGgUWvDWT",
        "outputId": "e12669a8-d272-49d3-8194-315f695f38d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  0  58   0]\n",
            " [  0 211   0]\n",
            " [  0  39   3]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.00      0.00      0.00        58\n",
            "      medium       0.69      1.00      0.81       211\n",
            "        high       1.00      0.07      0.13        42\n",
            "\n",
            "    accuracy                           0.69       311\n",
            "   macro avg       0.56      0.36      0.32       311\n",
            "weighted avg       0.60      0.69      0.57       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.583\tKappa: 0.046\tAccuracy: 0.688\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for fear:\n",
            "\n",
            "[[  1  82   4]\n",
            " [  2 231  13]\n",
            " [  0  68  14]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.33      0.01      0.02        87\n",
            "      medium       0.61      0.94      0.74       246\n",
            "        high       0.45      0.17      0.25        82\n",
            "\n",
            "    accuracy                           0.59       415\n",
            "   macro avg       0.46      0.37      0.34       415\n",
            "weighted avg       0.52      0.59      0.49       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.611\tKappa: 0.073\tAccuracy: 0.593\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[  2  68   2]\n",
            " [  2 157  14]\n",
            " [  0  29  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.50      0.03      0.05        72\n",
            "      medium       0.62      0.91      0.74       173\n",
            "        high       0.60      0.45      0.52        53\n",
            "\n",
            "    accuracy                           0.61       298\n",
            "   macro avg       0.57      0.46      0.43       298\n",
            "weighted avg       0.59      0.61      0.53       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.654\tKappa: 0.193\tAccuracy: 0.614\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  1  68   4]\n",
            " [  2 135  23]\n",
            " [  0  29  22]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.33      0.01      0.03        73\n",
            "      medium       0.58      0.84      0.69       160\n",
            "        high       0.45      0.43      0.44        51\n",
            "\n",
            "    accuracy                           0.56       284\n",
            "   macro avg       0.45      0.43      0.39       284\n",
            "weighted avg       0.49      0.56      0.47       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.632\tKappa: 0.123\tAccuracy: 0.556\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.62\t Average Kappa: 0.109\t Average Accuracy: 0.613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **TF-IDF**"
      ],
      "metadata": {
        "id": "ShfKYaxCMnSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pipelines con los distintos experimentos ejecutados con Tf-Idf"
      ],
      "metadata": {
        "id": "dMnUqTjq2e1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_experiment_0_pipeline():\n",
        "\n",
        "  return Pipeline([('features', FeatureUnion([('td_idf', TfidfVectorizer()), \n",
        "                                ('extractor', FeaturesTransformer())\n",
        "                                ])), \n",
        "                    ('clf', RandomForestClassifier())])\n",
        "\n",
        "def get_experiment_1_pipeline():\n",
        "\n",
        "  return Pipeline([('features', FeatureUnion([('td_idf', TfidfVectorizer()), \n",
        "                                ('extractor', FeaturesTransformer())\n",
        "                                ])), \n",
        "                    ('clf', RandomForestClassifier(random_state = 3380))])\n",
        "  \n",
        "def get_experiment_2_pipeline():\n",
        "\n",
        "  return Pipeline([('features', FeatureUnion([('td_idf', TfidfVectorizer()), \n",
        "                                ('extractor', FeaturesTransformer())\n",
        "                                ])), \n",
        "                    ('clf', RandomForestClassifier(n_estimators=1000))]) \n",
        "  \n",
        "def get_experiment_3_pipeline():\n",
        "\n",
        "  return Pipeline([('features', FeatureUnion([('td_idf', TfidfVectorizer()), \n",
        "                                ('extractor', FeaturesTransformer())\n",
        "                                ])), \n",
        "                    ('clf', XGBClassifier())])\n",
        "  \n",
        "def get_experiment_4_pipeline():\n",
        "\n",
        "  return Pipeline([('features', FeatureUnion([('td_idf', TfidfVectorizer()), \n",
        "                                ('extractor', FeaturesTransformer())\n",
        "                                ])), \n",
        "                    ('clf', SVC(kernel='linear', probability=True))])\n",
        "  \n",
        "def get_experiment_5_pipeline():\n",
        "\n",
        "  return Pipeline([('features', FeatureUnion([('td_idf', TfidfVectorizer()), \n",
        "                                ('extractor', FeaturesTransformer())\n",
        "                                ])), \n",
        "                    ('clf', SVC(kernel='rbf', C=250, probability=True))])\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "def get_experiment_6_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('features', FeatureUnion([('tdidf_pca', Pipeline([('td_idf', TfidfVectorizer()),\n",
        "                                                            (\"PCA\",TruncatedSVD(n_components=60, n_iter=7, random_state=42)),\n",
        "                                                            ])), \n",
        "                                  ('extractor', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "def get_experiment_7_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('features', FeatureUnion([('tdidf_pca', Pipeline([('td_idf', TfidfVectorizer()),\n",
        "                                                            (\"PCA\",TruncatedSVD(n_components=10)),\n",
        "                                                            ])), \n",
        "                                  ('extractor', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(n_estimators=1000,random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "def get_experiment_8_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('features', FeatureUnion([('tdidf_pca', Pipeline([('td_idf', TfidfVectorizer()),\n",
        "                                                            (\"PCA\",TruncatedSVD(n_components=10)),\n",
        "                                                            ])), \n",
        "                                  ('extractor', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', SVC(kernel='linear', probability=True)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "q8BRh7YP2hcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experimentos"
      ],
      "metadata": {
        "id": "8jLD8ChO3h2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comentario: demora 20min cada experimento :( ahora solo ejecuté el primero. para tenerlo en cuenta y definamos bien el tema de los expermientos."
      ],
      "metadata": {
        "id": "-FJ4x2mH8CKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 0: TF_IDF + EXTRACTOR + RF0\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_0_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VowWtwMA2PSX",
        "outputId": "da1d7bf1-2bf2-4780-869e-4ed8225e8b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 11  47   0]\n",
            " [  3 206   2]\n",
            " [  0  31  11]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.79      0.19      0.31        58\n",
            "      medium       0.73      0.98      0.83       211\n",
            "        high       0.85      0.26      0.40        42\n",
            "\n",
            "    accuracy                           0.73       311\n",
            "   macro avg       0.79      0.48      0.51       311\n",
            "weighted avg       0.75      0.73      0.68       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.733\tKappa: 0.272\tAccuracy: 0.733\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 37  48   2]\n",
            " [ 18 214  14]\n",
            " [  0  55  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.67      0.43      0.52        87\n",
            "      medium       0.68      0.87      0.76       246\n",
            "        high       0.63      0.33      0.43        82\n",
            "\n",
            "    accuracy                           0.67       415\n",
            "   macro avg       0.66      0.54      0.57       415\n",
            "weighted avg       0.67      0.67      0.65       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.727\tKappa: 0.338\tAccuracy: 0.67\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 31  41   0]\n",
            " [ 15 149   9]\n",
            " [  0  28  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.67      0.43      0.53        72\n",
            "      medium       0.68      0.86      0.76       173\n",
            "        high       0.74      0.47      0.57        53\n",
            "\n",
            "    accuracy                           0.69       298\n",
            "   macro avg       0.70      0.59      0.62       298\n",
            "weighted avg       0.69      0.69      0.67       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.763\tKappa: 0.397\tAccuracy: 0.688\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 30  43   0]\n",
            " [ 16 128  16]\n",
            " [  0  29  22]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.65      0.41      0.50        73\n",
            "      medium       0.64      0.80      0.71       160\n",
            "        high       0.58      0.43      0.49        51\n",
            "\n",
            "    accuracy                           0.63       284\n",
            "   macro avg       0.62      0.55      0.57       284\n",
            "weighted avg       0.63      0.63      0.62       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.701\tKappa: 0.319\tAccuracy: 0.634\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.731\t Average Kappa: 0.332\t Average Accuracy: 0.681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 1: TF_IDF + EXTRACTOR + RF1\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_1_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4b7a9a-1be8-4022-9dd6-8538ef930305",
        "id": "-RpSJYIi35Gz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  7  51   0]\n",
            " [  3 206   2]\n",
            " [  0  30  12]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.70      0.12      0.21        58\n",
            "      medium       0.72      0.98      0.83       211\n",
            "        high       0.86      0.29      0.43        42\n",
            "\n",
            "    accuracy                           0.72       311\n",
            "   macro avg       0.76      0.46      0.49       311\n",
            "weighted avg       0.73      0.72      0.66       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.705\tKappa: 0.236\tAccuracy: 0.723\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 40  46   1]\n",
            " [ 17 212  17]\n",
            " [  0  55  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.70      0.46      0.56        87\n",
            "      medium       0.68      0.86      0.76       246\n",
            "        high       0.60      0.33      0.43        82\n",
            "\n",
            "    accuracy                           0.67       415\n",
            "   macro avg       0.66      0.55      0.58       415\n",
            "weighted avg       0.67      0.67      0.65       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.719\tKappa: 0.348\tAccuracy: 0.672\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 27  45   0]\n",
            " [ 13 145  15]\n",
            " [  0  28  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.68      0.38      0.48        72\n",
            "      medium       0.67      0.84      0.74       173\n",
            "        high       0.62      0.47      0.54        53\n",
            "\n",
            "    accuracy                           0.66       298\n",
            "   macro avg       0.66      0.56      0.59       298\n",
            "weighted avg       0.66      0.66      0.64       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.748\tKappa: 0.347\tAccuracy: 0.661\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 31  42   0]\n",
            " [ 16 127  17]\n",
            " [  0  34  17]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.66      0.42      0.52        73\n",
            "      medium       0.63      0.79      0.70       160\n",
            "        high       0.50      0.33      0.40        51\n",
            "\n",
            "    accuracy                           0.62       284\n",
            "   macro avg       0.60      0.52      0.54       284\n",
            "weighted avg       0.61      0.62      0.60       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.695\tKappa: 0.28\tAccuracy: 0.616\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.717\t Average Kappa: 0.303\t Average Accuracy: 0.668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 2: TF_IDF + EXTRACTOR + RF2\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_2_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed1edfbd-58d7-497b-cfc4-3a67ee43d3ce",
        "id": "02vG1OGU4AU_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  8  50   0]\n",
            " [  3 208   0]\n",
            " [  0  31  11]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.73      0.14      0.23        58\n",
            "      medium       0.72      0.99      0.83       211\n",
            "        high       1.00      0.26      0.42        42\n",
            "\n",
            "    accuracy                           0.73       311\n",
            "   macro avg       0.82      0.46      0.49       311\n",
            "weighted avg       0.76      0.73      0.66       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.733\tKappa: 0.246\tAccuracy: 0.73\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 39  47   1]\n",
            " [ 16 215  15]\n",
            " [  0  55  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.71      0.45      0.55        87\n",
            "      medium       0.68      0.87      0.76       246\n",
            "        high       0.63      0.33      0.43        82\n",
            "\n",
            "    accuracy                           0.68       415\n",
            "   macro avg       0.67      0.55      0.58       415\n",
            "weighted avg       0.67      0.68      0.65       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.729\tKappa: 0.353\tAccuracy: 0.677\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 31  41   0]\n",
            " [ 11 151  11]\n",
            " [  0  27  26]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.74      0.43      0.54        72\n",
            "      medium       0.69      0.87      0.77       173\n",
            "        high       0.70      0.49      0.58        53\n",
            "\n",
            "    accuracy                           0.70       298\n",
            "   macro avg       0.71      0.60      0.63       298\n",
            "weighted avg       0.70      0.70      0.68       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.758\tKappa: 0.416\tAccuracy: 0.698\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 30  43   0]\n",
            " [ 14 130  16]\n",
            " [  0  29  22]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.68      0.41      0.51        73\n",
            "      medium       0.64      0.81      0.72       160\n",
            "        high       0.58      0.43      0.49        51\n",
            "\n",
            "    accuracy                           0.64       284\n",
            "   macro avg       0.63      0.55      0.58       284\n",
            "weighted avg       0.64      0.64      0.63       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.703\tKappa: 0.329\tAccuracy: 0.641\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.731\t Average Kappa: 0.336\t Average Accuracy: 0.686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 3: TF_IDF + EXTRACTOR + XGboost + Oversampling\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_3_pipeline(), param_over = 0.8, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJRAh8os4gWt",
        "outputId": "eb328d33-3d22-4c61-f17f-456adbfed966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset sadness: 801\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[45 27  1]\n",
            " [34 69 57]\n",
            " [ 0 14 37]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.57      0.62      0.59        73\n",
            "      medium       0.63      0.43      0.51       160\n",
            "        high       0.39      0.73      0.51        51\n",
            "\n",
            "    accuracy                           0.53       284\n",
            "   macro avg       0.53      0.59      0.54       284\n",
            "weighted avg       0.57      0.53      0.53       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.692\tKappa: 0.28\tAccuracy: 0.532\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.692\t Average Kappa: 0.28\t Average Accuracy: 0.532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 4: TF_IDF + EXTRACTOR + SVM lineal\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_4_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZuBCIZn4oZ7",
        "outputId": "3adc1f94-d2a1-472e-db89-ceb259e91737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 25  45   3]\n",
            " [ 14 117  29]\n",
            " [  0  27  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.64      0.34      0.45        73\n",
            "      medium       0.62      0.73      0.67       160\n",
            "        high       0.43      0.47      0.45        51\n",
            "\n",
            "    accuracy                           0.58       284\n",
            "   macro avg       0.56      0.51      0.52       284\n",
            "weighted avg       0.59      0.58      0.57       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.711\tKappa: 0.251\tAccuracy: 0.585\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.711\t Average Kappa: 0.251\t Average Accuracy: 0.585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 5: TF_IDF + EXTRACTOR + SVM rbf + Subsampling\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_5_pipeline(), param_over = False, param_under = 0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW04-b6g5arO",
        "outputId": "edcc5940-a5b0-4f5d-c9be-fe6b49896fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset sadness: 546\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 19  51   3]\n",
            " [ 13 121  26]\n",
            " [  1  31  19]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.58      0.26      0.36        73\n",
            "      medium       0.60      0.76      0.67       160\n",
            "        high       0.40      0.37      0.38        51\n",
            "\n",
            "    accuracy                           0.56       284\n",
            "   macro avg       0.52      0.46      0.47       284\n",
            "weighted avg       0.55      0.56      0.54       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.636\tKappa: 0.18\tAccuracy: 0.56\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.636\t Average Kappa: 0.18\t Average Accuracy: 0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 6: TF_IDF + EXTRACTOR + PCA + RF1\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_6_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvUgNeCa5r7Z",
        "outputId": "2cacb24b-9187-438d-a240-9d9593e74d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 33  40   0]\n",
            " [ 17 129  14]\n",
            " [  0  34  17]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.66      0.45      0.54        73\n",
            "      medium       0.64      0.81      0.71       160\n",
            "        high       0.55      0.33      0.41        51\n",
            "\n",
            "    accuracy                           0.63       284\n",
            "   macro avg       0.61      0.53      0.55       284\n",
            "weighted avg       0.63      0.63      0.61       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.709\tKappa: 0.306\tAccuracy: 0.63\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.709\t Average Kappa: 0.306\t Average Accuracy: 0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 7: TF_IDF + EXTRACTOR + PCA + RF2\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_7_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74dca53-1933-40de-f853-94d312a5acb0",
        "id": "9E8oaQK14P60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 12  46   0]\n",
            " [  5 203   3]\n",
            " [  0  32  10]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.71      0.21      0.32        58\n",
            "      medium       0.72      0.96      0.83       211\n",
            "        high       0.77      0.24      0.36        42\n",
            "\n",
            "    accuracy                           0.72       311\n",
            "   macro avg       0.73      0.47      0.50       311\n",
            "weighted avg       0.73      0.72      0.67       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.732\tKappa: 0.255\tAccuracy: 0.723\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 38  46   3]\n",
            " [ 22 204  20]\n",
            " [  3  55  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.60      0.44      0.51        87\n",
            "      medium       0.67      0.83      0.74       246\n",
            "        high       0.51      0.29      0.37        82\n",
            "\n",
            "    accuracy                           0.64       415\n",
            "   macro avg       0.59      0.52      0.54       415\n",
            "weighted avg       0.62      0.64      0.62       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.702\tKappa: 0.296\tAccuracy: 0.641\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 35  37   0]\n",
            " [ 28 128  17]\n",
            " [  1  24  28]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.55      0.49      0.51        72\n",
            "      medium       0.68      0.74      0.71       173\n",
            "        high       0.62      0.53      0.57        53\n",
            "\n",
            "    accuracy                           0.64       298\n",
            "   macro avg       0.62      0.58      0.60       298\n",
            "weighted avg       0.64      0.64      0.64       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.748\tKappa: 0.351\tAccuracy: 0.641\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 32  41   0]\n",
            " [ 19 120  21]\n",
            " [  0  24  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.63      0.44      0.52        73\n",
            "      medium       0.65      0.75      0.70       160\n",
            "        high       0.56      0.53      0.55        51\n",
            "\n",
            "    accuracy                           0.63       284\n",
            "   macro avg       0.61      0.57      0.59       284\n",
            "weighted avg       0.63      0.63      0.62       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.727\tKappa: 0.336\tAccuracy: 0.63\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.727\t Average Kappa: 0.309\t Average Accuracy: 0.659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 8: TF_IDF + EXTRACTOR + PCA + SVM lineal\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_8_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5183xvs50AK",
        "outputId": "5a8454eb-0b1a-49d0-9574-4b27ae734e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 30  41   2]\n",
            " [ 14 123  23]\n",
            " [  0  30  21]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.68      0.41      0.51        73\n",
            "      medium       0.63      0.77      0.69       160\n",
            "        high       0.46      0.41      0.43        51\n",
            "\n",
            "    accuracy                           0.61       284\n",
            "   macro avg       0.59      0.53      0.55       284\n",
            "weighted avg       0.61      0.61      0.60       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.713\tKappa: 0.291\tAccuracy: 0.613\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.713\t Average Kappa: 0.291\t Average Accuracy: 0.613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Embeddings preentrenados**"
      ],
      "metadata": {
        "id": "KEzZjbV3Mp-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lo largo de los experimentos, notamos que la inclusión de features es siempre mejor a generar predicciones solo con los embeddings. Además, la inclusión de resampleo logra mejores resultados para configuraciones específicas. Finalmente y dada la dimensionalidad de los datos, fue útil implementar `PCA` reducir la cantidad de columnas, evitar la maldición de la dimensionalidad y mejorar el poder de predicción de los clasificadores.\n",
        "\n",
        "Mejores resultados:\n",
        "\n",
        "- Anger: 0.756\n",
        "- Fear: 0.677\n",
        "- Joy: 0.695\n",
        "- Sadness: 0.648"
      ],
      "metadata": {
        "id": "IM37873YfIya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Objetos y funciones"
      ],
      "metadata": {
        "id": "23GC7e6XSQqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Doc2Vector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = pretrained_model\n",
        "        self.vocab = list(pretrained_model.vocab.keys())\n",
        "\n",
        "    def pre_processing(self, tweet: str):\n",
        "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet) #elimina urls\n",
        "        tweet = re.sub('(@[^\\s]+)|(w/)','', tweet) #elimina menciones\n",
        "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #le quita el # a los hashtags\n",
        "        tweet = re.sub('\\n','', tweet) #elimina saltos de línea\n",
        "        return tweet\n",
        "\n",
        "    def to_vector(self, sentence):\n",
        "        \n",
        "        sentence = self.pre_processing(sentence)\n",
        "\n",
        "        tokenized_sent = self.tokenizer.tokenize(sentence)\n",
        "        doc_vector = np.empty(self.model.vector_size, dtype=float)\n",
        "        \n",
        "        for word in tokenized_sent:\n",
        "            if word in self.vocab:\n",
        "                word_vector = self.model[word]\n",
        "                doc_vector  = np.vstack((doc_vector, word_vector))\n",
        "         \n",
        "        try:\n",
        "            doc_vector = doc_vector[1:].mean(axis=0)\n",
        "            return list(doc_vector)\n",
        "        except:\n",
        "            #print('Retornando vector de 0s!')\n",
        "            return list(np.array(np.zeros(self.model.vector_size)))\n",
        "            #return None\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        values = []\n",
        "        for tweet in X:\n",
        "            vector = self.to_vector(tweet)\n",
        "            values.append(vector)\n",
        "                \n",
        "        final_array = np.squeeze(np.array(values))\n",
        "        dataframe = pd.DataFrame(final_array)\n",
        "        return dataframe\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ],
      "metadata": {
        "id": "XM4hsImNOCzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizador= TweetTokenizer(preserve_case=False, reduce_len=True)\n",
        "\n",
        "# best models\n",
        "best_models = {sentiment: (0, 0) for sentiment in train.keys()}\n",
        "best_models\n",
        "\n",
        "FeaturesTransformer().transform(['I am ❤ #depression😭 😍 BUT I love YOU! 😂 What* estoy sad :( love www.hola.cl @ @hola']) # probamos transformer"
      ],
      "metadata": {
        "id": "Vf3AtHJdOOoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcce5259-5d8e-4fea-da31-0df43a89326f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 1.        , 0.        , 2.        , 1.        ,\n",
              "        9.        , 2.        , 4.        , 3.        , 1.        ,\n",
              "        2.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.276     , 0.318     , 0.406     , 0.8733    , 0.07504688,\n",
              "        0.02131401]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prueba de diferentes embeddings preentrenados"
      ],
      "metadata": {
        "id": "-Rj4x0oEOrYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizador= TweetTokenizer(preserve_case=False, reduce_len=True)"
      ],
      "metadata": {
        "id": "GdpYARkvM0qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline():\n",
        "  pipe = Pipeline([\n",
        "     ('embedding', Doc2Vector(tokenizer = tokenizador)), \n",
        "     ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "     ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "nOnkjokTOVwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 25d"
      ],
      "metadata": {
        "id": "7XXDCEbdO5pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = gensim.downloader.load('glove-twitter-25')\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline())"
      ],
      "metadata": {
        "id": "o-9xgjhWPT_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ce2d83-f319-4160-f275-c9a0bac91868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  6  52   0]\n",
            " [  4 201   6]\n",
            " [  0  35   7]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.60      0.10      0.18        58\n",
            "      medium       0.70      0.95      0.81       211\n",
            "        high       0.54      0.17      0.25        42\n",
            "\n",
            "    accuracy                           0.69       311\n",
            "   macro avg       0.61      0.41      0.41       311\n",
            "weighted avg       0.66      0.69      0.61       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.623\tKappa: 0.134\tAccuracy: 0.688\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 22  63   2]\n",
            " [ 26 195  25]\n",
            " [  2  59  21]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.44      0.25      0.32        87\n",
            "      medium       0.62      0.79      0.69       246\n",
            "        high       0.44      0.26      0.32        82\n",
            "\n",
            "    accuracy                           0.57       415\n",
            "   macro avg       0.50      0.43      0.45       415\n",
            "weighted avg       0.54      0.57      0.54       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.669\tKappa: 0.145\tAccuracy: 0.573\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 22  50   0]\n",
            " [ 18 144  11]\n",
            " [  0  29  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.55      0.31      0.39        72\n",
            "      medium       0.65      0.83      0.73       173\n",
            "        high       0.69      0.45      0.55        53\n",
            "\n",
            "    accuracy                           0.64       298\n",
            "   macro avg       0.63      0.53      0.56       298\n",
            "weighted avg       0.63      0.64      0.61       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.717\tKappa: 0.293\tAccuracy: 0.638\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 18  47   8]\n",
            " [ 14 126  20]\n",
            " [  2  34  15]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.53      0.25      0.34        73\n",
            "      medium       0.61      0.79      0.69       160\n",
            "        high       0.35      0.29      0.32        51\n",
            "\n",
            "    accuracy                           0.56       284\n",
            "   macro avg       0.50      0.44      0.45       284\n",
            "weighted avg       0.54      0.56      0.53       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.659\tKappa: 0.172\tAccuracy: 0.56\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.667\t Average Kappa: 0.186\t Average Accuracy: 0.615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 200d"
      ],
      "metadata": {
        "id": "ujKIW2n6Pdzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = gensim.downloader.load('glove-twitter-200')\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline())"
      ],
      "metadata": {
        "id": "i7awR7GuPfyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c329112a-d9dc-4961-de1a-3503f2c275c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 758.5/758.5MB downloaded\n",
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  5  53   0]\n",
            " [  2 200   9]\n",
            " [  0  35   7]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.71      0.09      0.15        58\n",
            "      medium       0.69      0.95      0.80       211\n",
            "        high       0.44      0.17      0.24        42\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.62      0.40      0.40       311\n",
            "weighted avg       0.66      0.68      0.61       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.661\tKappa: 0.117\tAccuracy: 0.682\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 17  65   5]\n",
            " [ 15 214  17]\n",
            " [  1  62  19]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.52      0.20      0.28        87\n",
            "      medium       0.63      0.87      0.73       246\n",
            "        high       0.46      0.23      0.31        82\n",
            "\n",
            "    accuracy                           0.60       415\n",
            "   macro avg       0.54      0.43      0.44       415\n",
            "weighted avg       0.57      0.60      0.55       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.683\tKappa: 0.166\tAccuracy: 0.602\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 18  54   0]\n",
            " [ 13 147  13]\n",
            " [  0  30  23]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.58      0.25      0.35        72\n",
            "      medium       0.64      0.85      0.73       173\n",
            "        high       0.64      0.43      0.52        53\n",
            "\n",
            "    accuracy                           0.63       298\n",
            "   macro avg       0.62      0.51      0.53       298\n",
            "weighted avg       0.62      0.63      0.60       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.713\tKappa: 0.267\tAccuracy: 0.631\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 17  52   4]\n",
            " [  8 131  21]\n",
            " [  1  33  17]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.65      0.23      0.34        73\n",
            "      medium       0.61      0.82      0.70       160\n",
            "        high       0.40      0.33      0.37        51\n",
            "\n",
            "    accuracy                           0.58       284\n",
            "   macro avg       0.56      0.46      0.47       284\n",
            "weighted avg       0.58      0.58      0.55       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.659\tKappa: 0.196\tAccuracy: 0.581\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.679\t Average Kappa: 0.186\t Average Accuracy: 0.624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding + features"
      ],
      "metadata": {
        "id": "sGf5lnrOPrWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, se probará incorporar al modelo las features creadas a mano por el objeto `FeaturesTransformer`.\n",
        "\n",
        "**Conclusión**: la incorporación de las features tiene un gran impacto en el poder de predicción del modelo."
      ],
      "metadata": {
        "id": "vOsQL57DPvri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = gensim.downloader.load('glove-twitter-200')"
      ],
      "metadata": {
        "id": "M0Dzg9XvQFeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding', Doc2Vector(tokenizer = tokenizador)), \n",
        "                                    ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "4-_wTxHSQJIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline())"
      ],
      "metadata": {
        "id": "JzDnQCsYQPGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c093fcfd-57f9-4a6a-f7ad-0b17654bea77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  6  52   0]\n",
            " [  2 204   5]\n",
            " [  0  32  10]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.75      0.10      0.18        58\n",
            "      medium       0.71      0.97      0.82       211\n",
            "        high       0.67      0.24      0.35        42\n",
            "\n",
            "    accuracy                           0.71       311\n",
            "   macro avg       0.71      0.44      0.45       311\n",
            "weighted avg       0.71      0.71      0.64       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.69\tKappa: 0.188\tAccuracy: 0.707\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 35  51   1]\n",
            " [ 13 221  12]\n",
            " [  1  59  22]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.71      0.40      0.51        87\n",
            "      medium       0.67      0.90      0.77       246\n",
            "        high       0.63      0.27      0.38        82\n",
            "\n",
            "    accuracy                           0.67       415\n",
            "   macro avg       0.67      0.52      0.55       415\n",
            "weighted avg       0.67      0.67      0.64       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.722\tKappa: 0.32\tAccuracy: 0.67\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 25  47   0]\n",
            " [ 13 149  11]\n",
            " [  0  28  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.66      0.35      0.45        72\n",
            "      medium       0.67      0.86      0.75       173\n",
            "        high       0.69      0.47      0.56        53\n",
            "\n",
            "    accuracy                           0.67       298\n",
            "   macro avg       0.67      0.56      0.59       298\n",
            "weighted avg       0.67      0.67      0.65       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.746\tKappa: 0.35\tAccuracy: 0.668\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 31  40   2]\n",
            " [  8 128  24]\n",
            " [  0  34  17]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.79      0.42      0.55        73\n",
            "      medium       0.63      0.80      0.71       160\n",
            "        high       0.40      0.33      0.36        51\n",
            "\n",
            "    accuracy                           0.62       284\n",
            "   macro avg       0.61      0.52      0.54       284\n",
            "weighted avg       0.63      0.62      0.61       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.729\tKappa: 0.292\tAccuracy: 0.62\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.722\t Average Kappa: 0.287\t Average Accuracy: 0.666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding + features + balanceo de clases"
      ],
      "metadata": {
        "id": "vaQ1fXOvQUOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se presentan los experimentos sobre diferentes configuraciones del tamaño de muestra. Para esto, nos apoyaremos de la función `resample`, donde iremos variando los parámetros `param_under` y `param_over` para probar diferentes configuraciones de undersampling y oversampling.\n",
        "\n",
        "**Conclusión**: La mejor combinación de `param_under` y `param_over` se dió cuando `param_under` = 0.9 y `param_over` = 0.1. "
      ],
      "metadata": {
        "id": "RkM77KjcQkoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = gensim.downloader.load('glove-twitter-200')"
      ],
      "metadata": {
        "id": "-4dmndTDQZIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding', Doc2Vector(tokenizer = tokenizador)), \n",
        "                                    ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "4txTPDWzQb2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### param_under = 0.9"
      ],
      "metadata": {
        "id": "Sbc6cJtHQ112"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = False, param_under = 0.9)"
      ],
      "metadata": {
        "id": "8rbAledkQd_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6250b7c2-6ac5-4f8a-8e08-1aa54ade9e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 589\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  7  51   0]\n",
            " [  2 204   5]\n",
            " [  0  34   8]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.78      0.12      0.21        58\n",
            "      medium       0.71      0.97      0.82       211\n",
            "        high       0.62      0.19      0.29        42\n",
            "\n",
            "    accuracy                           0.70       311\n",
            "   macro avg       0.70      0.43      0.44       311\n",
            "weighted avg       0.71      0.70      0.63       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.702\tKappa: 0.175\tAccuracy: 0.704\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 796\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 36  50   1]\n",
            " [ 21 208  17]\n",
            " [  3  55  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.60      0.41      0.49        87\n",
            "      medium       0.66      0.85      0.74       246\n",
            "        high       0.57      0.29      0.39        82\n",
            "\n",
            "    accuracy                           0.65       415\n",
            "   macro avg       0.61      0.52      0.54       415\n",
            "weighted avg       0.63      0.65      0.62       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.722\tKappa: 0.295\tAccuracy: 0.646\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 572\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 34  38   0]\n",
            " [ 25 135  13]\n",
            " [  0  30  23]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.58      0.47      0.52        72\n",
            "      medium       0.67      0.78      0.72       173\n",
            "        high       0.64      0.43      0.52        53\n",
            "\n",
            "    accuracy                           0.64       298\n",
            "   macro avg       0.63      0.56      0.58       298\n",
            "weighted avg       0.64      0.64      0.63       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.752\tKappa: 0.335\tAccuracy: 0.644\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 546\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 27  44   2]\n",
            " [ 10 122  28]\n",
            " [  0  29  22]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.73      0.37      0.49        73\n",
            "      medium       0.63      0.76      0.69       160\n",
            "        high       0.42      0.43      0.43        51\n",
            "\n",
            "    accuracy                           0.60       284\n",
            "   macro avg       0.59      0.52      0.54       284\n",
            "weighted avg       0.62      0.60      0.59       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.727\tKappa: 0.272\tAccuracy: 0.602\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.726\t Average Kappa: 0.269\t Average Accuracy: 0.649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### param_over = 0.1"
      ],
      "metadata": {
        "id": "gHiPAFQuQ6pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = 0.1, param_under = False)"
      ],
      "metadata": {
        "id": "_gP0QjYVQ9sf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e800a69-268b-4932-e910-d4df50c2b247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 652\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  5  52   1]\n",
            " [  2 205   4]\n",
            " [  0  31  11]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.71      0.09      0.15        58\n",
            "      medium       0.71      0.97      0.82       211\n",
            "        high       0.69      0.26      0.38        42\n",
            "\n",
            "    accuracy                           0.71       311\n",
            "   macro avg       0.70      0.44      0.45       311\n",
            "weighted avg       0.71      0.71      0.64       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.689\tKappa: 0.197\tAccuracy: 0.711\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 880\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 40  46   1]\n",
            " [ 17 219  10]\n",
            " [  2  60  20]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.68      0.46      0.55        87\n",
            "      medium       0.67      0.89      0.77       246\n",
            "        high       0.65      0.24      0.35        82\n",
            "\n",
            "    accuracy                           0.67       415\n",
            "   macro avg       0.67      0.53      0.56       415\n",
            "weighted avg       0.67      0.67      0.64       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.734\tKappa: 0.333\tAccuracy: 0.672\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 632\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 24  48   0]\n",
            " [ 14 145  14]\n",
            " [  0  28  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.63      0.33      0.44        72\n",
            "      medium       0.66      0.84      0.74       173\n",
            "        high       0.64      0.47      0.54        53\n",
            "\n",
            "    accuracy                           0.65       298\n",
            "   macro avg       0.64      0.55      0.57       298\n",
            "weighted avg       0.65      0.65      0.63       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.764\tKappa: 0.323\tAccuracy: 0.651\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 603\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 33  36   4]\n",
            " [  9 132  19]\n",
            " [  0  32  19]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.79      0.45      0.57        73\n",
            "      medium       0.66      0.82      0.73       160\n",
            "        high       0.45      0.37      0.41        51\n",
            "\n",
            "    accuracy                           0.65       284\n",
            "   macro avg       0.63      0.55      0.57       284\n",
            "weighted avg       0.66      0.65      0.63       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.726\tKappa: 0.346\tAccuracy: 0.648\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.728\t Average Kappa: 0.3\t Average Accuracy: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### param_over = 0.1, param_under = 0.9"
      ],
      "metadata": {
        "id": "ed5I5PynRAEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = 0.1, param_under = 0.9)"
      ],
      "metadata": {
        "id": "MCiLA9R1RCqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50230a1-95eb-426a-c21b-b6f1eea96090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 611\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 11  47   0]\n",
            " [  1 208   2]\n",
            " [  0  33   9]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.92      0.19      0.31        58\n",
            "      medium       0.72      0.99      0.83       211\n",
            "        high       0.82      0.21      0.34        42\n",
            "\n",
            "    accuracy                           0.73       311\n",
            "   macro avg       0.82      0.46      0.50       311\n",
            "weighted avg       0.77      0.73      0.67       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.692\tKappa: 0.258\tAccuracy: 0.733\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 834\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 39  47   1]\n",
            " [ 23 206  17]\n",
            " [  1  57  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.62      0.45      0.52        87\n",
            "      medium       0.66      0.84      0.74       246\n",
            "        high       0.57      0.29      0.39        82\n",
            "\n",
            "    accuracy                           0.65       415\n",
            "   macro avg       0.62      0.53      0.55       415\n",
            "weighted avg       0.64      0.65      0.62       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.717\tKappa: 0.304\tAccuracy: 0.648\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 600\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 39  33   0]\n",
            " [ 25 133  15]\n",
            " [  0  28  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.61      0.54      0.57        72\n",
            "      medium       0.69      0.77      0.72       173\n",
            "        high       0.62      0.47      0.54        53\n",
            "\n",
            "    accuracy                           0.66       298\n",
            "   macro avg       0.64      0.59      0.61       298\n",
            "weighted avg       0.66      0.66      0.65       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.765\tKappa: 0.38\tAccuracy: 0.661\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 573\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 28  42   3]\n",
            " [  8 128  24]\n",
            " [  1  24  26]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.76      0.38      0.51        73\n",
            "      medium       0.66      0.80      0.72       160\n",
            "        high       0.49      0.51      0.50        51\n",
            "\n",
            "    accuracy                           0.64       284\n",
            "   macro avg       0.64      0.56      0.58       284\n",
            "weighted avg       0.65      0.64      0.63       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.738\tKappa: 0.345\tAccuracy: 0.641\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.728\t Average Kappa: 0.322\t Average Accuracy: 0.671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embeddings + features + balanceo de clases + PCA"
      ],
      "metadata": {
        "id": "gzCUbD2ZRdJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección se presentan los experimentos de haber probado con diferentes combinaciones del parámetro `n_components`."
      ],
      "metadata": {
        "id": "vRKOwS1eR35A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### n_components = 20"
      ],
      "metadata": {
        "id": "9cghH8soTQWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding_pca', Pipeline([('embedding', Doc2Vector(tokenizer = tokenizador)),\n",
        "                                                            ('pca', PCA(n_components = 20)),\n",
        "                                                            ])), \n",
        "                                  ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "VIG0_BvDTY9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "id": "2iEw9Sn7TcDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca37720a-f58a-46e5-f5be-4324acde3020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 11  47   0]\n",
            " [  3 202   6]\n",
            " [  0  25  17]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.79      0.19      0.31        58\n",
            "      medium       0.74      0.96      0.83       211\n",
            "        high       0.74      0.40      0.52        42\n",
            "\n",
            "    accuracy                           0.74       311\n",
            "   macro avg       0.75      0.52      0.55       311\n",
            "weighted avg       0.75      0.74      0.69       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.727\tKappa: 0.322\tAccuracy: 0.74\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 45  42   0]\n",
            " [ 19 206  21]\n",
            " [  1  57  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.69      0.52      0.59        87\n",
            "      medium       0.68      0.84      0.75       246\n",
            "        high       0.53      0.29      0.38        82\n",
            "\n",
            "    accuracy                           0.66       415\n",
            "   macro avg       0.63      0.55      0.57       415\n",
            "weighted avg       0.65      0.66      0.64       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.731\tKappa: 0.339\tAccuracy: 0.663\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 37  35   0]\n",
            " [ 17 142  14]\n",
            " [  0  20  33]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.69      0.51      0.59        72\n",
            "      medium       0.72      0.82      0.77       173\n",
            "        high       0.70      0.62      0.66        53\n",
            "\n",
            "    accuracy                           0.71       298\n",
            "   macro avg       0.70      0.65      0.67       298\n",
            "weighted avg       0.71      0.71      0.70       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.774\tKappa: 0.47\tAccuracy: 0.711\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 38  33   2]\n",
            " [ 15 120  25]\n",
            " [  1  31  19]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.70      0.52      0.60        73\n",
            "      medium       0.65      0.75      0.70       160\n",
            "        high       0.41      0.37      0.39        51\n",
            "\n",
            "    accuracy                           0.62       284\n",
            "   macro avg       0.59      0.55      0.56       284\n",
            "weighted avg       0.62      0.62      0.62       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.736\tKappa: 0.324\tAccuracy: 0.623\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.742\t Average Kappa: 0.364\t Average Accuracy: 0.684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = 0.1, param_under = 0.9)"
      ],
      "metadata": {
        "id": "7nlBlfKnThuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad69c235-3543-47df-f8a6-cfe2ba712bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 611\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 16  42   0]\n",
            " [  3 201   7]\n",
            " [  0  27  15]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.84      0.28      0.42        58\n",
            "      medium       0.74      0.95      0.84       211\n",
            "        high       0.68      0.36      0.47        42\n",
            "\n",
            "    accuracy                           0.75       311\n",
            "   macro avg       0.76      0.53      0.57       311\n",
            "weighted avg       0.75      0.75      0.71       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.732\tKappa: 0.349\tAccuracy: 0.746\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 834\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 47  39   1]\n",
            " [ 29 191  26]\n",
            " [  2  47  33]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.60      0.54      0.57        87\n",
            "      medium       0.69      0.78      0.73       246\n",
            "        high       0.55      0.40      0.46        82\n",
            "\n",
            "    accuracy                           0.65       415\n",
            "   macro avg       0.61      0.57      0.59       415\n",
            "weighted avg       0.64      0.65      0.64       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.748\tKappa: 0.353\tAccuracy: 0.653\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 600\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 43  29   0]\n",
            " [ 31 125  17]\n",
            " [  0  21  32]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.58      0.60      0.59        72\n",
            "      medium       0.71      0.72      0.72       173\n",
            "        high       0.65      0.60      0.63        53\n",
            "\n",
            "    accuracy                           0.67       298\n",
            "   macro avg       0.65      0.64      0.64       298\n",
            "weighted avg       0.67      0.67      0.67       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.766\tKappa: 0.423\tAccuracy: 0.671\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 573\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 41  31   1]\n",
            " [ 21 111  28]\n",
            " [  2  22  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.64      0.56      0.60        73\n",
            "      medium       0.68      0.69      0.69       160\n",
            "        high       0.48      0.53      0.50        51\n",
            "\n",
            "    accuracy                           0.63       284\n",
            "   macro avg       0.60      0.59      0.60       284\n",
            "weighted avg       0.63      0.63      0.63       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.74\tKappa: 0.364\tAccuracy: 0.63\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.746\t Average Kappa: 0.372\t Average Accuracy: 0.675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### n_components = 15"
      ],
      "metadata": {
        "id": "UUg-raAETT2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding_pca', Pipeline([('embedding', Doc2Vector(tokenizer = tokenizador)),\n",
        "                                                            ('pca', PCA(n_components = 15)),\n",
        "                                                            ])), \n",
        "                                  ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "CsflxiujTTea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = 0.1, param_under = 0.9)"
      ],
      "metadata": {
        "id": "xOyfYJJSTq2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "218053cc-f5c7-45d6-822a-2a94ccb78118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 611\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 15  43   0]\n",
            " [  5 200   6]\n",
            " [  0  23  19]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.75      0.26      0.38        58\n",
            "      medium       0.75      0.95      0.84       211\n",
            "        high       0.76      0.45      0.57        42\n",
            "\n",
            "    accuracy                           0.75       311\n",
            "   macro avg       0.75      0.55      0.60       311\n",
            "weighted avg       0.75      0.75      0.72       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.727\tKappa: 0.376\tAccuracy: 0.752\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 834\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 48  38   1]\n",
            " [ 29 194  23]\n",
            " [  5  46  31]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.59      0.55      0.57        87\n",
            "      medium       0.70      0.79      0.74       246\n",
            "        high       0.56      0.38      0.45        82\n",
            "\n",
            "    accuracy                           0.66       415\n",
            "   macro avg       0.62      0.57      0.59       415\n",
            "weighted avg       0.65      0.66      0.65       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.746\tKappa: 0.361\tAccuracy: 0.658\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 600\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 44  28   0]\n",
            " [ 29 128  16]\n",
            " [  1  21  31]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.59      0.61      0.60        72\n",
            "      medium       0.72      0.74      0.73       173\n",
            "        high       0.66      0.58      0.62        53\n",
            "\n",
            "    accuracy                           0.68       298\n",
            "   macro avg       0.66      0.65      0.65       298\n",
            "weighted avg       0.68      0.68      0.68       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.79\tKappa: 0.438\tAccuracy: 0.681\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 573\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 38  34   1]\n",
            " [ 21 108  31]\n",
            " [  1  19  31]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.63      0.52      0.57        73\n",
            "      medium       0.67      0.68      0.67       160\n",
            "        high       0.49      0.61      0.54        51\n",
            "\n",
            "    accuracy                           0.62       284\n",
            "   macro avg       0.60      0.60      0.60       284\n",
            "weighted avg       0.63      0.62      0.62       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.747\tKappa: 0.358\tAccuracy: 0.623\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.752\t Average Kappa: 0.383\t Average Accuracy: 0.679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### n_components = 10"
      ],
      "metadata": {
        "id": "8YVWK3_1TVtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding_pca', Pipeline([('embedding', Doc2Vector(tokenizer = tokenizador)),\n",
        "                                                            ('pca', PCA(n_components = 15)),\n",
        "                                                            ])), \n",
        "                                  ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "3-AxgX4lTYl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = 0.1, param_under = 0.9)"
      ],
      "metadata": {
        "id": "9W3oIYmGT8jK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324c0ca6-9693-49c3-9fca-28ff3a67b550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 611\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 16  42   0]\n",
            " [  5 201   5]\n",
            " [  0  24  18]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.76      0.28      0.41        58\n",
            "      medium       0.75      0.95      0.84       211\n",
            "        high       0.78      0.43      0.55        42\n",
            "\n",
            "    accuracy                           0.76       311\n",
            "   macro avg       0.77      0.55      0.60       311\n",
            "weighted avg       0.76      0.76      0.72       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.729\tKappa: 0.381\tAccuracy: 0.756\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 834\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 47  38   2]\n",
            " [ 27 194  25]\n",
            " [  3  49  30]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.61      0.54      0.57        87\n",
            "      medium       0.69      0.79      0.74       246\n",
            "        high       0.53      0.37      0.43        82\n",
            "\n",
            "    accuracy                           0.65       415\n",
            "   macro avg       0.61      0.56      0.58       415\n",
            "weighted avg       0.64      0.65      0.64       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.738\tKappa: 0.348\tAccuracy: 0.653\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 600\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 43  29   0]\n",
            " [ 35 120  18]\n",
            " [  1  20  32]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.54      0.60      0.57        72\n",
            "      medium       0.71      0.69      0.70       173\n",
            "        high       0.64      0.60      0.62        53\n",
            "\n",
            "    accuracy                           0.65       298\n",
            "   macro avg       0.63      0.63      0.63       298\n",
            "weighted avg       0.66      0.65      0.66       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.778\tKappa: 0.401\tAccuracy: 0.654\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 573\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 40  32   1]\n",
            " [ 19 109  32]\n",
            " [  0  24  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.68      0.55      0.61        73\n",
            "      medium       0.66      0.68      0.67       160\n",
            "        high       0.45      0.53      0.49        51\n",
            "\n",
            "    accuracy                           0.62       284\n",
            "   macro avg       0.60      0.59      0.59       284\n",
            "weighted avg       0.63      0.62      0.62       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.746\tKappa: 0.346\tAccuracy: 0.62\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.748\t Average Kappa: 0.369\t Average Accuracy: 0.671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Embeddings entrenados con la data disponible**"
      ],
      "metadata": {
        "id": "nosZAw48L7re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este apartado se utilizarán embeddings entrenados con los mismos corpus de los sentimientos. La idea que hay por detrás es generar representaciones de tokens dentro de un mismo sentimiento porque al tener todas un contexto parecido\n",
        "hará que el vector que represente cada palabra sea más parecido a las palabras que ve constantemente en su ventana *k* dimensional, pero en el contexto del sentimiento, por lo que será una representación más a la medida de la palabra, dado su contexto (sentimiento). Esto se contrasta con los embeddings de glove, los cuales fueron entrenados con corpus gigantescos, lo cual genera representaciones de palabras mucho más \"robustas\" ya que contienen información de muchos más contextos en los cuales se han visto esas palabras juntas, lo que genera una vector mucho más \"general\" o \"semántico\" de la palabra.\n",
        "La utilización de embeddings por corpus se justifica debido a que se asume que la competencia es clara en que el desafío consiste en clasificar la intensidad de un tweet mediante las clases \"low\", \"medium\" y \"high\", **sabiendo** la categoría o sentimiento al cual corresponde el documento a clasificar, y que por lo tanto, tiene sentido generar representaciones de palabras dentro de un mismo sentimiento. **Es cierto que los embedding tienen un sesgo**, pero es un sesgo que se genera a propósito en este caso, porque tenemos información sobre el sentimiento al cual pertenece el documento que vamos a predecir y al final a la task de clasificar, que es la que nos compete, no le interesa de donde se consiguieron los embeddings si es que estoy teniendo buenos resultados con ellos.\n",
        "\n",
        "Partiremos experimentando sólo con los embeddings, y luego le iremos agregando transformaciones vistas anteriormente. En el siguiente código se generan los embeddings para cada sentimiento, para esto será necesario crear una clase que transforme un documento en un solo vector de embedding."
      ],
      "metadata": {
        "id": "VducZwpwMkTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseFeature(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "class Doc2VecTransformer(BaseFeature):\n",
        "    \"\"\" Transforma tweets a representaciones vectoriales usando algún modelo de Word Embeddings.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, aggregation_func):\n",
        "        # extraemos los embeddings desde el objeto contenedor. ojo con esta parte.\n",
        "        self.model = model.wv \n",
        "        \n",
        "        # indicamos la función de agregación (np.min, np.max, np.mean, np.sum, ...)\n",
        "        self.aggregation_func = aggregation_func\n",
        "\n",
        "    def simple_tokenizer(self, doc, lower=False):\n",
        "        \"\"\"Tokenizador. Elimina signos de puntuación, lleva las letras a minúscula(opcional) y \n",
        "           separa el tweet por espacios.\n",
        "        \"\"\"\n",
        "        return tokenizador.tokenize(doc)\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \n",
        "        doc_embeddings = []\n",
        "        \n",
        "        for doc in X:\n",
        "            tokens = self.simple_tokenizer(doc, lower = True) \n",
        "            \n",
        "            selected_wv = []\n",
        "            for token in tokens:\n",
        "                if token in self.model.vocab:\n",
        "                    selected_wv.append(self.model[token])\n",
        "                    \n",
        "            if len(selected_wv) > 0:\n",
        "                doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n",
        "                doc_embeddings.append(doc_embedding)\n",
        "            else: \n",
        "                print('No pude encontrar ningún embedding en el tweet: {}. Agregando vector de ceros.'.format(doc))\n",
        "                doc_embeddings.append(np.zeros(self.model.vector_size)) # la dimension del modelo \n",
        "\n",
        "        return np.array(doc_embeddings)"
      ],
      "metadata": {
        "id": "9HG16hiNiVIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se generaliza la función *get_pipeline* para que acepte como argumento el objeto Doc2Vector de cada sentimiento y a continuación se generan los embeddings para cada uno de ellos."
      ],
      "metadata": {
        "id": "gsOskZ49jlZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline(doc2vect):\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([\n",
        "                                   ('doc2vect', doc2vect)\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "L6PID3yTYPVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generación de los embeddings por cada sentimiento.\n",
        "embeddingsSentiment = {}\n",
        "pipelines = {}\n",
        "for sentimiento, datos in train.items():\n",
        "  content = train[sentimiento]['tweet']\n",
        "  #se tokeniza el documento\n",
        "  cleaned_content = [TweetTokenizer(preserve_case=False, reduce_len=True).tokenize(documento) for documento in content]\n",
        "  #se buscan bigramas\n",
        "  phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) \n",
        "  #se tokenizan bigramas\n",
        "  bigram = Phraser(phrases)\n",
        "  sentences = bigram[cleaned_content]\n",
        "  #definir el modelo\n",
        "  biobio_w2v = Word2Vec(min_count=10,\n",
        "                        window=4,\n",
        "                        size=200,\n",
        "                        sample=6e-5,\n",
        "                        alpha=0.03,\n",
        "                        min_alpha=0.0007,\n",
        "                        negative=20,\n",
        "                        workers=multiprocessing.cpu_count())\n",
        "  #construir el vocabulario\n",
        "  biobio_w2v.build_vocab(sentences, progress_per=10000)\n",
        "  #entrenar el modelo\n",
        "  t = time()\n",
        "  biobio_w2v.train(sentences, total_examples=biobio_w2v.corpus_count, epochs=15, report_delay=10)\n",
        "  print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "  #se indicamos que terminó el entrenamiento\n",
        "  biobio_w2v.init_sims(replace=True)\n",
        "  embeddingsSentiment[sentimiento] = Doc2VecTransformer(biobio_w2v, np.mean)\n",
        "  pipelines[sentimiento] = get_pipeline(embeddingsSentiment[sentimiento])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmXV8L2Gc0_8",
        "outputId": "247df0cb-3726-4f06-840a-498d8dce1998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to train the model: 0.02 mins\n",
            "Time to train the model: 0.02 mins\n",
            "Time to train the model: 0.01 mins\n",
            "Time to train the model: 0.02 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora se entrenará un RandomForest con estos embeddings generador. Debido a como fue creada la función auxiliar *evaluate_model*, en donde recibe un sólo pipe, necesitaremos extenderla para que sea capaz de recibir un dictionario de pipes y los vaya eligiendo a medida que van pasando los sentimientos."
      ],
      "metadata": {
        "id": "vbtphMatdjms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  n_resample = dataset['category'].value_counts().mean()\n",
        "  NUM_SAMPLES = int(n_resample)\n",
        "  g = dataset.groupby('category')\n",
        "  dataset = pd.DataFrame(\n",
        "    g.apply(lambda x: x.sample(NUM_SAMPLES, replace=True).reset_index(drop=True))\n",
        "  ).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ehYVMf8qoOco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['anger']"
      ],
      "metadata": {
        "id": "4jKgOIbJErOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([train['anger']['tweet'], train['anger']['sentiment_intensity']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxTXJC-GFAlC",
        "outputId": "3f50c79f-1a7b-4736-a5f7-36a9ddbc47cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      How the fu*k! Who the heck! moved my fridge!.....\n",
              "1      So my Indian Uber driver just called someone t...\n",
              "2      @DPD_UK I asked for my parcel to be delivered ...\n",
              "3      so ef whichever butt wipe pulled the fire alar...\n",
              "4      Don't join @BTCare they put the phone down on ...\n",
              "                             ...                        \n",
              "936                                               medium\n",
              "937                                               medium\n",
              "938                                                  low\n",
              "939                                                  low\n",
              "940                                               medium\n",
              "Length: 1882, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([train['anger']['tweet'], train['anger']['sentiment_intensity']], axis = 1).groupby('sentiment_intensity')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEkdRw3dEaV3",
        "outputId": "c99e640e-dfae-43f5-936a-f6b604080f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fca7dffacd0>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run(dataset, dataset_name, pipeline, param_over = False, param_under = False):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test, aún no se transforma de Strings a valores numéricos.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataset.tweet,\n",
        "        dataset.sentiment_intensity,\n",
        "        shuffle=True,\n",
        "        test_size=0.33,\n",
        "        random_state = 3380)\n",
        "\n",
        "    # Balanceo de clases\n",
        "    # X_train, y_train = resample(X_train, y_train, param_over, param_under)\n",
        "    n_resample = y_train.value_counts().mean()\n",
        "    NUM_SAMPLES = int(n_resample)\n",
        "    g = pd.concat([X_train, y_train], axis = 1).groupby('sentiment_intensity')\n",
        "    datasetResampleado = pd.DataFrame(\n",
        "      g.apply(lambda x: x.sample(NUM_SAMPLES, replace=True).reset_index(drop=True))\n",
        "    ).reset_index(drop=True)\n",
        "    X_train = datasetResampleado['tweet']\n",
        "    y_train = datasetResampleado['sentiment_intensity']\n",
        "    \n",
        "    print(f'# Datos de entrenamiento en dataset {dataset_name}: {len(X_train)}')\n",
        "    print(f'# Datos de testing en dataset {dataset_name}: {len(X_test)}')\n",
        "\n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline). \n",
        "    # En este caso el Bag of Words es el encargado de transformar de Strings a vectores numéricos.\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
        "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "    \n",
        "    # Evaluamos:\n",
        "    scores = evaluate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
        "    return pipeline, learned_labels, scores"
      ],
      "metadata": {
        "id": "gxL7SxJUDJL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(pipeline, param_over = False, param_under = False):\n",
        "  classifiers = []\n",
        "  learned_labels_array = []\n",
        "  scores_array = []\n",
        "\n",
        "  # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "  for dataset_name, dataset in train.items():\n",
        "    #resampleado de dataset para balancear clases.\n",
        "    # n_resample = dataset['sentiment_intensity'].value_counts().mean()\n",
        "    # NUM_SAMPLES = int(n_resample)\n",
        "    # g = dataset.groupby('sentiment_intensity')\n",
        "    # datasetResampleado = pd.DataFrame(\n",
        "    #   g.apply(lambda x: x.sample(NUM_SAMPLES, replace=True).reset_index(drop=True))\n",
        "    # ).reset_index(drop=True)\n",
        "    # creamos el pipeline\n",
        "    if isinstance(pipeline, dict):\n",
        "      pipeline = pipeline[dataset_name]\n",
        "    else:\n",
        "      pipeline = pipeline\n",
        "    \n",
        "    # ejecutamos el pipeline sobre el dataset\n",
        "    classifier, learned_labels, scores = run(dataset, dataset_name, pipeline, param_over, param_under)\n",
        "    accuracy = scores[2]\n",
        "    if accuracy > best_models[dataset_name][1]: # si accuracy del modelo es mejor que el mejor accuracy logrado\n",
        "      best_models.update({dataset_name: (classifier, accuracy)})\n",
        "\n",
        "    # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
        "    classifiers.append(classifier)\n",
        "\n",
        "    # guardamos las labels aprendidas por el clasificador\n",
        "    learned_labels_array.append(learned_labels)\n",
        "\n",
        "    # guardamos los scores obtenidos\n",
        "    scores_array.append(scores)\n",
        "\n",
        "  # print avg scores\n",
        "  print(\n",
        "    \"Average scores:\\n\\n\",\n",
        "    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
        "    .format(*np.array(scores_array).mean(axis=0)))\n",
        "  return classifiers, learned_labels_array, scores_array"
      ],
      "metadata": {
        "id": "3CRy55Sigvo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(pipelines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omQGhAm3kY86",
        "outputId": "af55358e-5eb3-4449-c396-2980a6035b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @TzumiXIV huff puff. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Bloody parking ticket 😒💸 . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: #Hudcomedy #AdamRowe  #insult Slutfaceshlongnugget. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  2  56   0]\n",
            " [  5 201   5]\n",
            " [  0  33   9]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.29      0.03      0.06        58\n",
            "      medium       0.69      0.95      0.80       211\n",
            "        high       0.64      0.21      0.32        42\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.54      0.40      0.40       311\n",
            "weighted avg       0.61      0.68      0.60       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.585\tKappa: 0.109\tAccuracy: 0.682\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "No pude encontrar ningún embedding en el tweet: @lukeshawtime terrible. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: dread pitt. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Rooney shocking attempted cross. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @CSTrey4 thanks brotein shake 😈😈😈. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @ImJim_YoureNot  cyber bully. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Penny dreadful 3 temporada. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: #smackdev #ptp #start word. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: shake fries. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @casillasbreanna awe thanks girl 😊😊. Agregando vector de ceros.\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 12  72   3]\n",
            " [ 18 213  15]\n",
            " [  0  66  16]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.40      0.14      0.21        87\n",
            "      medium       0.61      0.87      0.71       246\n",
            "        high       0.47      0.20      0.28        82\n",
            "\n",
            "    accuracy                           0.58       415\n",
            "   macro avg       0.49      0.40      0.40       415\n",
            "weighted avg       0.54      0.58      0.52       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.548\tKappa: 0.103\tAccuracy: 0.581\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "No pude encontrar ningún embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @weebtard sparkling water wyd. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: 4-2 Canada final tomorrow #WCH #Predictions #optimism #Canadian 🇨🇦. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @jimadair3 Guitar shop owners everywhere rejoice. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LeBatardShow #heyday Race war 2016. Agregando vector de ceros.\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 14  57   1]\n",
            " [ 11 150  12]\n",
            " [  2  36  15]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.52      0.19      0.28        72\n",
            "      medium       0.62      0.87      0.72       173\n",
            "        high       0.54      0.28      0.37        53\n",
            "\n",
            "    accuracy                           0.60       298\n",
            "   macro avg       0.56      0.45      0.46       298\n",
            "weighted avg       0.58      0.60      0.55       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.657\tKappa: 0.182\tAccuracy: 0.601\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "No pude encontrar ningún embedding en el tweet: Ffs dreadful defending. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RossKemp great programme tonight #sad #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RossKemp great programme tonight  #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @Hails_Berry8 @sajedhariri23 varsity pine riding. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither  #unwoke. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Baaarissshhhhh + sad song =  prefect night — feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @KatelynKolsrud thanks mucho kate💕 #sober. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RyhenMessedUp goodbye despair. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: untypical kinda Friday #dull. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither #lost #unwoke. Agregando vector de ceros.\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  7  58   8]\n",
            " [  4 135  21]\n",
            " [  0  39  12]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.64      0.10      0.17        73\n",
            "      medium       0.58      0.84      0.69       160\n",
            "        high       0.29      0.24      0.26        51\n",
            "\n",
            "    accuracy                           0.54       284\n",
            "   macro avg       0.50      0.39      0.37       284\n",
            "weighted avg       0.54      0.54      0.48       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.593\tKappa: 0.092\tAccuracy: 0.542\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.596\t Average Kappa: 0.121\t Average Accuracy: 0.601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# con resampleado\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(pipelines)"
      ],
      "metadata": {
        "id": "vWE5GJxQo47Z",
        "outputId": "ff93e28d-01db-4476-8cdc-53a1ee8db181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 629\n",
            "# Datos de testing en dataset anger: 310\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: #Hudcomedy #AdamRowe  #insult Slutfaceshlongnugget. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[84 25  4]\n",
            " [ 8 83 11]\n",
            " [ 1 13 81]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.90      0.74      0.82       113\n",
            "      medium       0.69      0.81      0.74       102\n",
            "        high       0.84      0.85      0.85        95\n",
            "\n",
            "    accuracy                           0.80       310\n",
            "   macro avg       0.81      0.80      0.80       310\n",
            "weighted avg       0.81      0.80      0.80       310\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.937\tKappa: 0.7\tAccuracy: 0.8\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "No pude encontrar ningún embedding en el tweet: Penny dreadful 3 temporada. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 99  38   9]\n",
            " [ 23  83  20]\n",
            " [  4  20 119]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.79      0.68      0.73       146\n",
            "      medium       0.59      0.66      0.62       126\n",
            "        high       0.80      0.83      0.82       143\n",
            "\n",
            "    accuracy                           0.73       415\n",
            "   macro avg       0.73      0.72      0.72       415\n",
            "weighted avg       0.73      0.73      0.73       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.896\tKappa: 0.588\tAccuracy: 0.725\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 603\n",
            "# Datos de testing en dataset joy: 297\n",
            "No pude encontrar ningún embedding en el tweet: @LeBatardShow #heyday Race war 2016. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @weebtard sparkling water wyd. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @weebtard sparkling water wyd. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[75 19 11]\n",
            " [18 64 19]\n",
            " [ 4 14 73]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.77      0.71      0.74       105\n",
            "      medium       0.66      0.63      0.65       101\n",
            "        high       0.71      0.80      0.75        91\n",
            "\n",
            "    accuracy                           0.71       297\n",
            "   macro avg       0.71      0.72      0.71       297\n",
            "weighted avg       0.71      0.71      0.71       297\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.898\tKappa: 0.571\tAccuracy: 0.714\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 574\n",
            "# Datos de testing en dataset sadness: 284\n",
            "No pude encontrar ningún embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Baaarissshhhhh + sad song =  prefect night — feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Ffs dreadful defending. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Baaarissshhhhh + sad song =  prefect night — feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @Hails_Berry8 @sajedhariri23 varsity pine riding. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @Hails_Berry8 @sajedhariri23 varsity pine riding. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: untypical kinda Friday #dull. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither #lost #unwoke. Agregando vector de ceros.\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[69 27 10]\n",
            " [11 67 14]\n",
            " [ 2 11 73]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.84      0.65      0.73       106\n",
            "      medium       0.64      0.73      0.68        92\n",
            "        high       0.75      0.85      0.80        86\n",
            "\n",
            "    accuracy                           0.74       284\n",
            "   macro avg       0.74      0.74      0.74       284\n",
            "weighted avg       0.75      0.74      0.74       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.89\tKappa: 0.605\tAccuracy: 0.736\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.905\t Average Kappa: 0.616\t Average Accuracy: 0.744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# con con over_param y under_param\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(pipelines, param_over = 0.5, param_under = 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgDtX5NVCEpb",
        "outputId": "0170ce99-74e4-45a9-9428-6177a38a080d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 538\n",
            "# Datos de testing en dataset anger: 311\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Bloody parking ticket 😒💸 . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: #Hudcomedy #AdamRowe  #insult Slutfaceshlongnugget. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  7  45   6]\n",
            " [ 15 170  26]\n",
            " [  2  20  20]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.29      0.12      0.17        58\n",
            "      medium       0.72      0.81      0.76       211\n",
            "        high       0.38      0.48      0.43        42\n",
            "\n",
            "    accuracy                           0.63       311\n",
            "   macro avg       0.47      0.47      0.45       311\n",
            "weighted avg       0.60      0.63      0.61       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.636\tKappa: 0.186\tAccuracy: 0.633\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 809\n",
            "# Datos de testing en dataset fear: 415\n",
            "No pude encontrar ningún embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @CSTrey4 thanks brotein shake 😈😈😈. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: dread pitt. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Rooney shocking attempted cross. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: #smackdev #ptp #start word. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: shake fries. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @casillasbreanna awe thanks girl 😊😊. Agregando vector de ceros.\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[33 32 22]\n",
            " [91 94 61]\n",
            " [20 30 32]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.23      0.38      0.29        87\n",
            "      medium       0.60      0.38      0.47       246\n",
            "        high       0.28      0.39      0.32        82\n",
            "\n",
            "    accuracy                           0.38       415\n",
            "   macro avg       0.37      0.38      0.36       415\n",
            "weighted avg       0.46      0.38      0.40       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.557\tKappa: 0.051\tAccuracy: 0.383\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 590\n",
            "# Datos de testing en dataset joy: 298\n",
            "No pude encontrar ningún embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @weebtard sparkling water wyd. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: 4-2 Canada final tomorrow #WCH #Predictions #optimism #Canadian 🇨🇦. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @jimadair3 Guitar shop owners everywhere rejoice. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LeBatardShow #heyday Race war 2016. Agregando vector de ceros.\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[50 16  6]\n",
            " [69 62 42]\n",
            " [15 12 26]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.37      0.69      0.49        72\n",
            "      medium       0.69      0.36      0.47       173\n",
            "        high       0.35      0.49      0.41        53\n",
            "\n",
            "    accuracy                           0.46       298\n",
            "   macro avg       0.47      0.51      0.46       298\n",
            "weighted avg       0.55      0.46      0.46       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.628\tKappa: 0.201\tAccuracy: 0.463\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 570\n",
            "# Datos de testing en dataset sadness: 284\n",
            "No pude encontrar ningún embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Baaarissshhhhh + sad song =  prefect night — feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @Hails_Berry8 @sajedhariri23 varsity pine riding. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither  #unwoke. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RossKemp great programme tonight #sad #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @KatelynKolsrud thanks mucho kate💕 #sober. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RyhenMessedUp goodbye despair. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: untypical kinda Friday #dull. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither #lost #unwoke. Agregando vector de ceros.\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[26 15 32]\n",
            " [50 43 67]\n",
            " [18 13 20]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.28      0.36      0.31        73\n",
            "      medium       0.61      0.27      0.37       160\n",
            "        high       0.17      0.39      0.24        51\n",
            "\n",
            "    accuracy                           0.31       284\n",
            "   macro avg       0.35      0.34      0.31       284\n",
            "weighted avg       0.44      0.31      0.33       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.549\tKappa: 0.017\tAccuracy: 0.313\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.593\t Average Kappa: 0.114\t Average Accuracy: 0.448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# con con over_param y under_param\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(pipelines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gANNTn7vEG0i",
        "outputId": "afa9c03a-4632-4c80-ef8f-81a0abaacce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Bloody parking ticket 😒💸 . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: #Hudcomedy #AdamRowe  #insult Slutfaceshlongnugget. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  9  46   3]\n",
            " [ 20 158  33]\n",
            " [  1  21  20]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.30      0.16      0.20        58\n",
            "      medium       0.70      0.75      0.72       211\n",
            "        high       0.36      0.48      0.41        42\n",
            "\n",
            "    accuracy                           0.60       311\n",
            "   macro avg       0.45      0.46      0.45       311\n",
            "weighted avg       0.58      0.60      0.58       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.575\tKappa: 0.146\tAccuracy: 0.601\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 840\n",
            "# Datos de testing en dataset fear: 415\n",
            "No pude encontrar ningún embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @CSTrey4 thanks brotein shake 😈😈😈. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @CSTrey4 thanks brotein shake 😈😈😈. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Penny dreadful 3 temporada. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: #smackdev #ptp #start word. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: shake fries. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @casillasbreanna awe thanks girl 😊😊. Agregando vector de ceros.\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 22  46  19]\n",
            " [ 45 151  50]\n",
            " [  7  48  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.30      0.25      0.27        87\n",
            "      medium       0.62      0.61      0.62       246\n",
            "        high       0.28      0.33      0.30        82\n",
            "\n",
            "    accuracy                           0.48       415\n",
            "   macro avg       0.40      0.40      0.40       415\n",
            "weighted avg       0.48      0.48      0.48       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.558\tKappa: 0.086\tAccuracy: 0.482\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 603\n",
            "# Datos de testing en dataset joy: 298\n",
            "No pude encontrar ningún embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: 4-2 Canada final tomorrow #WCH #Predictions #optimism #Canadian 🇨🇦. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @jimadair3 Guitar shop owners everywhere rejoice. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LeBatardShow #heyday Race war 2016. Agregando vector de ceros.\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[36 33  3]\n",
            " [49 95 29]\n",
            " [ 9 26 18]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.38      0.50      0.43        72\n",
            "      medium       0.62      0.55      0.58       173\n",
            "        high       0.36      0.34      0.35        53\n",
            "\n",
            "    accuracy                           0.50       298\n",
            "   macro avg       0.45      0.46      0.45       298\n",
            "weighted avg       0.51      0.50      0.50       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.614\tKappa: 0.158\tAccuracy: 0.5\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "No pude encontrar ningún embedding en el tweet: Baaarissshhhhh + sad song =  prefect night — feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Baaarissshhhhh + sad song =  prefect night — feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Baaarissshhhhh + sad song =  prefect night — feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Ffs dreadful defending. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RossKemp great programme tonight #sad #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither  #unwoke. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RossKemp great programme tonight  #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @KatelynKolsrud thanks mucho kate💕 #sober. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RyhenMessedUp goodbye despair. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: untypical kinda Friday #dull. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither #lost #unwoke. Agregando vector de ceros.\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[15 37 21]\n",
            " [35 75 50]\n",
            " [ 9 21 21]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.25      0.21      0.23        73\n",
            "      medium       0.56      0.47      0.51       160\n",
            "        high       0.23      0.41      0.29        51\n",
            "\n",
            "    accuracy                           0.39       284\n",
            "   macro avg       0.35      0.36      0.34       284\n",
            "weighted avg       0.42      0.39      0.40       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.54\tKappa: 0.025\tAccuracy: 0.391\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.572\t Average Kappa: 0.104\t Average Accuracy: 0.493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se observa que los resultados no son los mejores, si se comparan con los obtenidos en los experimentos anteriores. Se probará agregando el bag of words para ver cómo responde el modelo."
      ],
      "metadata": {
        "id": "oD9BfI_RnMWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline(doc2vect):\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([\n",
        "                                   ('bow', CountVectorizer(analyzer='word', ngram_range=(1, 2))),\n",
        "                                   ('doc2vect', doc2vect)\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "for sentimiento, pipe in pipelines.items():\n",
        "  pipelines[sentimiento] = get_pipeline(embeddingsSentiment[sentimiento])"
      ],
      "metadata": {
        "id": "yACdZ-bqMS2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(pipelines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkaQ38fkoG5y",
        "outputId": "ceb6c19d-c98b-40cd-a850-0c4f09f9a886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @TzumiXIV huff puff. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Bloody parking ticket 😒💸 . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: #Hudcomedy #AdamRowe  #insult Slutfaceshlongnugget. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  2  56   0]\n",
            " [  4 202   5]\n",
            " [  0  35   7]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.33      0.03      0.06        58\n",
            "      medium       0.69      0.96      0.80       211\n",
            "        high       0.58      0.17      0.26        42\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.54      0.39      0.37       311\n",
            "weighted avg       0.61      0.68      0.59       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.615\tKappa: 0.087\tAccuracy: 0.678\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "No pude encontrar ningún embedding en el tweet: @lukeshawtime terrible. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: dread pitt. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Rooney shocking attempted cross. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @CSTrey4 thanks brotein shake 😈😈😈. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @ImJim_YoureNot  cyber bully. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Penny dreadful 3 temporada. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: #smackdev #ptp #start word. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: shake fries. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @casillasbreanna awe thanks girl 😊😊. Agregando vector de ceros.\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 10  73   4]\n",
            " [ 14 218  14]\n",
            " [  2  67  13]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.38      0.11      0.18        87\n",
            "      medium       0.61      0.89      0.72       246\n",
            "        high       0.42      0.16      0.23        82\n",
            "\n",
            "    accuracy                           0.58       415\n",
            "   macro avg       0.47      0.39      0.38       415\n",
            "weighted avg       0.52      0.58      0.51       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.564\tKappa: 0.09\tAccuracy: 0.581\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "No pude encontrar ningún embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @weebtard sparkling water wyd. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: 4-2 Canada final tomorrow #WCH #Predictions #optimism #Canadian 🇨🇦. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @jimadair3 Guitar shop owners everywhere rejoice. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LeBatardShow #heyday Race war 2016. Agregando vector de ceros.\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[  9  63   0]\n",
            " [  8 157   8]\n",
            " [  1  41  11]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.50      0.12      0.20        72\n",
            "      medium       0.60      0.91      0.72       173\n",
            "        high       0.58      0.21      0.31        53\n",
            "\n",
            "    accuracy                           0.59       298\n",
            "   macro avg       0.56      0.41      0.41       298\n",
            "weighted avg       0.57      0.59      0.52       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.641\tKappa: 0.128\tAccuracy: 0.594\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "No pude encontrar ningún embedding en el tweet: Ffs dreadful defending. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RossKemp great programme tonight #sad #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RossKemp great programme tonight  #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @Hails_Berry8 @sajedhariri23 varsity pine riding. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither  #unwoke. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Baaarissshhhhh + sad song =  prefect night — feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @KatelynKolsrud thanks mucho kate💕 #sober. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RyhenMessedUp goodbye despair. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: untypical kinda Friday #dull. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither #lost #unwoke. Agregando vector de ceros.\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  4  66   3]\n",
            " [  6 141  13]\n",
            " [  0  44   7]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.40      0.05      0.10        73\n",
            "      medium       0.56      0.88      0.69       160\n",
            "        high       0.30      0.14      0.19        51\n",
            "\n",
            "    accuracy                           0.54       284\n",
            "   macro avg       0.42      0.36      0.32       284\n",
            "weighted avg       0.47      0.54      0.45       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.599\tKappa: 0.029\tAccuracy: 0.535\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.605\t Average Kappa: 0.0835\t Average Accuracy: 0.597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se observa como mejoraron los resultados al agregar atributos que provienen del BoW. A continuación se experimentará agregando las features creadas por el clase FeatureTransformer creada anteriormente."
      ],
      "metadata": {
        "id": "w-iyIAs8oMZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline(doc2vect):\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([\n",
        "                                   ('bow', CountVectorizer(analyzer='word', ngram_range=(1, 2))),\n",
        "                                   ('doc2vect', doc2vect),\n",
        "                                   ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "for sentimiento, pipe in pipelines.items():\n",
        "  pipelines[sentimiento] = get_pipeline(embeddingsSentiment[sentimiento])"
      ],
      "metadata": {
        "id": "2-FToeN9oawC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(pipelines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGpwyLUjokQ-",
        "outputId": "94d9cb73-1fc5-4b70-f980-8bf9d9027d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @TzumiXIV huff puff. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Bloody parking ticket 😒💸 . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: #Hudcomedy #AdamRowe  #insult Slutfaceshlongnugget. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  2  56   0]\n",
            " [  5 204   2]\n",
            " [  0  35   7]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.29      0.03      0.06        58\n",
            "      medium       0.69      0.97      0.81       211\n",
            "        high       0.78      0.17      0.27        42\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.59      0.39      0.38       311\n",
            "weighted avg       0.63      0.68      0.60       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.648\tKappa: 0.095\tAccuracy: 0.685\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "No pude encontrar ningún embedding en el tweet: @lukeshawtime terrible. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: dread pitt. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Rooney shocking attempted cross. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @CSTrey4 thanks brotein shake 😈😈😈. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @ImJim_YoureNot  cyber bully. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Penny dreadful 3 temporada. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: #smackdev #ptp #start word. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: shake fries. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @casillasbreanna awe thanks girl 😊😊. Agregando vector de ceros.\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 15  69   3]\n",
            " [ 11 221  14]\n",
            " [  1  68  13]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.56      0.17      0.26        87\n",
            "      medium       0.62      0.90      0.73       246\n",
            "        high       0.43      0.16      0.23        82\n",
            "\n",
            "    accuracy                           0.60       415\n",
            "   macro avg       0.54      0.41      0.41       415\n",
            "weighted avg       0.57      0.60      0.53       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.613\tKappa: 0.132\tAccuracy: 0.6\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "No pude encontrar ningún embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @weebtard sparkling water wyd. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: 4-2 Canada final tomorrow #WCH #Predictions #optimism #Canadian 🇨🇦. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @jimadair3 Guitar shop owners everywhere rejoice. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @LeBatardShow #heyday Race war 2016. Agregando vector de ceros.\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[  9  62   1]\n",
            " [ 10 155   8]\n",
            " [  1  37  15]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.45      0.12      0.20        72\n",
            "      medium       0.61      0.90      0.73       173\n",
            "        high       0.62      0.28      0.39        53\n",
            "\n",
            "    accuracy                           0.60       298\n",
            "   macro avg       0.56      0.43      0.44       298\n",
            "weighted avg       0.57      0.60      0.54       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.704\tKappa: 0.159\tAccuracy: 0.601\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "No pude encontrar ningún embedding en el tweet: Ffs dreadful defending. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RossKemp great programme tonight #sad #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RossKemp great programme tonight  #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @Hails_Berry8 @sajedhariri23 varsity pine riding. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither  #unwoke. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Baaarissshhhhh + sad song =  prefect night — feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @KatelynKolsrud thanks mucho kate💕 #sober. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @RyhenMessedUp goodbye despair. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: untypical kinda Friday #dull. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither #lost #unwoke. Agregando vector de ceros.\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  8  61   4]\n",
            " [  3 138  19]\n",
            " [  0  43   8]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.73      0.11      0.19        73\n",
            "      medium       0.57      0.86      0.69       160\n",
            "        high       0.26      0.16      0.20        51\n",
            "\n",
            "    accuracy                           0.54       284\n",
            "   macro avg       0.52      0.38      0.36       284\n",
            "weighted avg       0.55      0.54      0.47       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.663\tKappa: 0.067\tAccuracy: 0.542\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.657\t Average Kappa: 0.113\t Average Accuracy: 0.607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#variables n_resample, sentimiento, agg_func\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "clasificadores = {'anger': [],\n",
        "                  'fear': [],\n",
        "                  'joy': [],\n",
        "                  'sadness': []}\n",
        "\n",
        "bog_ = True\n",
        "for sentimiento in train.keys():\n",
        "  content = train[sentimiento]['tweet']\n",
        "  #se tokeniza el documento\n",
        "  cleaned_content = [tokenizador.tokenize(documento) for documento in content]\n",
        "  #se buscan bigramas\n",
        "  phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) \n",
        "  #se tokenizan bigramas\n",
        "  bigram = Phraser(phrases)\n",
        "  sentences = bigram[cleaned_content]\n",
        "  #definir el modelo\n",
        "  biobio_w2v = Word2Vec(min_count=10,\n",
        "                        window=4,\n",
        "                        size=200,\n",
        "                        sample=6e-5,\n",
        "                        alpha=0.03,\n",
        "                        min_alpha=0.0007,\n",
        "                        negative=20,\n",
        "                        workers=multiprocessing.cpu_count())\n",
        "  #construir el vocabulario\n",
        "  biobio_w2v.build_vocab(sentences, progress_per=10000)\n",
        "  #entrenar el modelo\n",
        "  t = time()\n",
        "  biobio_w2v.train(sentences, total_examples=biobio_w2v.corpus_count, epochs=15, report_delay=10)\n",
        "  print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "  #se indicamos que terminó el entrenamiento\n",
        "  biobio_w2v.init_sims(replace=True)\n",
        "\n",
        "  #problema de clasificación\n",
        "  #generación dataset\n",
        "  dataset = pd.DataFrame({'content': train[sentimiento]['tweet'], 'category': train[sentimiento]['sentiment_intensity']})\n",
        "  # dataset['category'].value_counts()\n",
        "  #resampleo\n",
        "  n_resample = dataset['category'].value_counts().mean()\n",
        "  NUM_SAMPLES = int(n_resample)\n",
        "  g = dataset.groupby('category')\n",
        "  dataset = pd.DataFrame(\n",
        "    g.apply(lambda x: x.sample(NUM_SAMPLES, replace=True).reset_index(drop=True))\n",
        "  ).reset_index(drop=True)\n",
        "  #train y test\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "  from google.colab import output\n",
        "  output.disable_custom_widget_manager()\n",
        "  X_train, X_test, y_train, y_test = train_test_split(dataset.content,\n",
        "                                                      dataset.category,\n",
        "                                                      test_size=0.33,\n",
        "                                                      random_state=3380, \n",
        "                                                      stratify = dataset.category)\n",
        "\n",
        "  #pipeline\n",
        "  clf = RandomForestClassifier(random_state = 3380)\n",
        "  # clf = XGBClassifier(n_estimators = 2000)\n",
        "  for agg_func in [('mean', np.mean)]:\n",
        "                  #  ('max', np.max), ('sum', np.sum)]:\n",
        "    doc2vec_func = Doc2VecTransformer(biobio_w2v, agg_func[1])\n",
        "    if bog_:\n",
        "      pipeline = Pipeline([('features',\n",
        "                        FeatureUnion([\n",
        "                                      # ('tf-idf', TfidfVectorizer()),\n",
        "                                      # ('bow', CountVectorizer(analyzer='word', ngram_range=(1, 2))),\n",
        "                                      ('doc2vec', doc2vec_func),\n",
        "                                      # ('features', FeaturesTransformer())\n",
        "                                      ])), \n",
        "                            ('clf', clf)])\n",
        "    else:\n",
        "      pipeline = Pipeline([('doc2vec', doc2vec_func), ('clf', clf)])\n",
        "\n",
        "    #fit\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    print(f'\\nResultados sentimiento: {sentimiento} - Función de agregación {agg_func[0]}')\n",
        "    print(conf_matrix)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    clasificadores[sentimiento].append(pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj2S2WVKZC1r",
        "outputId": "140b2a5b-02cc-45b0-ee99-12f7091b59c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to train the model: 0.02 mins\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: testing . Agregando vector de ceros.\n",
            "\n",
            "Resultados sentimiento: anger - Función de agregación mean\n",
            "[[89  0 14]\n",
            " [ 5 76 23]\n",
            " [13  8 82]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.83      0.86      0.85       103\n",
            "         low       0.90      0.73      0.81       104\n",
            "      medium       0.69      0.80      0.74       103\n",
            "\n",
            "    accuracy                           0.80       310\n",
            "   macro avg       0.81      0.80      0.80       310\n",
            "weighted avg       0.81      0.80      0.80       310\n",
            "\n",
            "Time to train the model: 0.02 mins\n",
            "\n",
            "Resultados sentimiento: fear - Función de agregación mean\n",
            "[[101  12  25]\n",
            " [ 10 105  24]\n",
            " [ 23  26  89]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.75      0.73      0.74       138\n",
            "         low       0.73      0.76      0.74       139\n",
            "      medium       0.64      0.64      0.64       138\n",
            "\n",
            "    accuracy                           0.71       415\n",
            "   macro avg       0.71      0.71      0.71       415\n",
            "weighted avg       0.71      0.71      0.71       415\n",
            "\n",
            "Time to train the model: 0.02 mins\n",
            "No pude encontrar ningún embedding en el tweet: Riggs dumb ass hell lolol  #LethalWeapon. Agregando vector de ceros.\n",
            "\n",
            "Resultados sentimiento: joy - Función de agregación mean\n",
            "[[81  3 15]\n",
            " [ 5 72 22]\n",
            " [ 7 18 74]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.87      0.82      0.84        99\n",
            "         low       0.77      0.73      0.75        99\n",
            "      medium       0.67      0.75      0.70        99\n",
            "\n",
            "    accuracy                           0.76       297\n",
            "   macro avg       0.77      0.76      0.77       297\n",
            "weighted avg       0.77      0.76      0.77       297\n",
            "\n",
            "Time to train the model: 0.02 mins\n",
            "No pude encontrar ningún embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ningún embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "\n",
            "Resultados sentimiento: sadness - Función de agregación mean\n",
            "[[79  1 14]\n",
            " [ 4 70 21]\n",
            " [17  8 70]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.79      0.84      0.81        94\n",
            "         low       0.89      0.74      0.80        95\n",
            "      medium       0.67      0.74      0.70        95\n",
            "\n",
            "    accuracy                           0.77       284\n",
            "   macro avg       0.78      0.77      0.77       284\n",
            "weighted avg       0.78      0.77      0.77       284\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Búsqueda de hiperparámetros**"
      ],
      "metadata": {
        "id": "bCd5lN52M4yZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección realizaremos una búsqueda de hiperparámetros a través de `HalvingGridSearchCV`. Para esto, usaremos como input la configuraciones óptimas encontradas en la sección anterior. Además, nos fijaremos en 3 modelos a ocupar: `RandomForestClassifier`, `XGBClassifier` y `SVC`."
      ],
      "metadata": {
        "id": "M1HupaA2NLRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = gensim.downloader.load('glove-twitter-200')\n",
        "pipeline = Pipeline([\n",
        "                     ('clf', RandomForestClassifier())\n",
        "                     ])"
      ],
      "metadata": {
        "id": "PHOvH4esz9lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = [\n",
        "              # Random Forest\n",
        "              {'clf': [RandomForestClassifier(random_state = 3380)],\n",
        "               'clf__n_estimators': [100, 500, 1000],\n",
        "               'clf__bootstrap': [True],\n",
        "               'clf__max_depth': [80, 90, 100, 110],\n",
        "               'clf__max_features': [2, 3],\n",
        "               'clf__min_samples_leaf': [3, 4, 5],\n",
        "               'clf__min_samples_split': [8, 10, 12],\n",
        "               },\n",
        "              # XGBoost\n",
        "              {'clf': [XGBClassifier(random_state = 3380)],\n",
        "               \"clf__max_depth\": [3, 4, 5, 7],\n",
        "               \"clf__learning_rate\": [0.1, 0.01, 0.05],\n",
        "               \"clf__gamma\": [0, 0.25, 1],\n",
        "               \"clf__reg_lambda\": [0, 1, 10],\n",
        "               \"clf__scale_pos_weight\": [1, 3, 5],\n",
        "               \"clf__subsample\": [0.8],\n",
        "               \"clf__colsample_bytree\": [0.5],\n",
        "               },\n",
        "              # SVM\n",
        "              {'clf': [SVC(probability = True, random_state = 3380)],\n",
        "               'clf__C': [1, 10, 100, 1000],\n",
        "               'clf__kernel': ['linear', 'rbf', 'poly'],\n",
        "               'clf__gamma': [0.001, 0.0001]}\n",
        "]"
      ],
      "metadata": {
        "id": "gGPJ-Tta0scZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd0Bfonr1Ajp"
      },
      "source": [
        "#### **Anger**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn3XqDF_1Ajq"
      },
      "outputs": [],
      "source": [
        "grid = HalvingGridSearchCV(pipeline, param_grid, cv = 5, verbose = 1, scoring = 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXBbuu5W1Ajq"
      },
      "outputs": [],
      "source": [
        "X, y = resample(train['anger']['tweet'], train['anger']['sentiment_intensity'], param_over = False, param_under = False)\n",
        "embedding_anger = Doc2Vector(tokenizer = tokenizador).transform(X)\n",
        "embedding_anger = PCA(n_components = 15).fit_transform(embedding_anger)\n",
        "features_anger = FeaturesTransformer().transform(X)\n",
        "data_anger = np.hstack([embedding_anger, features_anger])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704f1272-775c-4958-b431-53e7076c5eb2",
        "id": "AFTJVMGP1Ajr"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 4\n",
            "n_required_iterations: 6\n",
            "n_possible_iterations: 4\n",
            "min_resources_: 30\n",
            "max_resources_: 941\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 564\n",
            "n_resources: 30\n",
            "Fitting 5 folds for each of 564 candidates, totalling 2820 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 188\n",
            "n_resources: 90\n",
            "Fitting 5 folds for each of 188 candidates, totalling 940 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 63\n",
            "n_resources: 270\n",
            "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 21\n",
            "n_resources: 810\n",
            "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
          ]
        }
      ],
      "source": [
        "anger_model = grid.fit(data_anger, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "ZvVldMXt4QaU",
        "outputId": "5c3a2574-6961-4bd1-f04f-17ea7f2988f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
              "751     1           90       0.049146      0.004337         0.001262   \n",
              "748     1           90       0.038550      0.000296         0.001127   \n",
              "747     1           90       0.042893      0.004091         0.001242   \n",
              "746     1           90       0.041899      0.000848         0.001184   \n",
              "745     1           90       0.043049      0.001973         0.001140   \n",
              "..    ...          ...            ...           ...              ...   \n",
              "555     0           30       0.002431      0.000421         0.000556   \n",
              "552     0           30       0.002780      0.000586         0.000604   \n",
              "549     0           30       0.002454      0.000499         0.000533   \n",
              "546     0           30       0.002420      0.000420         0.000544   \n",
              "559     0           30       0.002140      0.000072         0.000534   \n",
              "\n",
              "     std_score_time                                          param_clf  \\\n",
              "751        0.000253  XGBClassifier(colsample_bytree=0.5, random_sta...   \n",
              "748        0.000044  XGBClassifier(colsample_bytree=0.5, random_sta...   \n",
              "747        0.000191  XGBClassifier(colsample_bytree=0.5, random_sta...   \n",
              "746        0.000069  XGBClassifier(colsample_bytree=0.5, random_sta...   \n",
              "745        0.000013  XGBClassifier(colsample_bytree=0.5, random_sta...   \n",
              "..              ...                                                ...   \n",
              "555        0.000018           SVC(probability=True, random_state=3380)   \n",
              "552        0.000133           SVC(probability=True, random_state=3380)   \n",
              "549        0.000016           SVC(probability=True, random_state=3380)   \n",
              "546        0.000009           SVC(probability=True, random_state=3380)   \n",
              "559        0.000018           SVC(probability=True, random_state=3380)   \n",
              "\n",
              "    param_clf__bootstrap param_clf__max_depth param_clf__max_features  ...  \\\n",
              "751                  NaN                    7                     NaN  ...   \n",
              "748                  NaN                    3                     NaN  ...   \n",
              "747                  NaN                    4                     NaN  ...   \n",
              "746                  NaN                    4                     NaN  ...   \n",
              "745                  NaN                    4                     NaN  ...   \n",
              "..                   ...                  ...                     ...  ...   \n",
              "555                  NaN                  NaN                     NaN  ...   \n",
              "552                  NaN                  NaN                     NaN  ...   \n",
              "549                  NaN                  NaN                     NaN  ...   \n",
              "546                  NaN                  NaN                     NaN  ...   \n",
              "559                  NaN                  NaN                     NaN  ...   \n",
              "\n",
              "    mean_test_score std_test_score rank_test_score split0_train_score  \\\n",
              "751        0.700654       0.138488               1                1.0   \n",
              "748        0.700654       0.138488               1                1.0   \n",
              "747        0.700654       0.138488               1                1.0   \n",
              "746        0.700654       0.138488               1                1.0   \n",
              "745        0.700654       0.138488               1                1.0   \n",
              "..              ...            ...             ...                ...   \n",
              "555        0.566667       0.240370             830                1.0   \n",
              "552        0.566667       0.240370             830                1.0   \n",
              "549        0.566667       0.240370             830                1.0   \n",
              "546        0.566667       0.240370             830                1.0   \n",
              "559        0.460000       0.149666             836                1.0   \n",
              "\n",
              "    split1_train_score split2_train_score split3_train_score  \\\n",
              "751                1.0                1.0           1.000000   \n",
              "748                1.0                1.0           1.000000   \n",
              "747                1.0                1.0           1.000000   \n",
              "746                1.0                1.0           1.000000   \n",
              "745                1.0                1.0           1.000000   \n",
              "..                 ...                ...                ...   \n",
              "555                1.0                1.0           1.000000   \n",
              "552                1.0                1.0           1.000000   \n",
              "549                1.0                1.0           1.000000   \n",
              "546                1.0                1.0           1.000000   \n",
              "559                1.0                1.0           0.958333   \n",
              "\n",
              "    split4_train_score mean_train_score std_train_score  \n",
              "751                1.0         1.000000        0.000000  \n",
              "748                1.0         1.000000        0.000000  \n",
              "747                1.0         1.000000        0.000000  \n",
              "746                1.0         1.000000        0.000000  \n",
              "745                1.0         1.000000        0.000000  \n",
              "..                 ...              ...             ...  \n",
              "555                1.0         1.000000        0.000000  \n",
              "552                1.0         1.000000        0.000000  \n",
              "549                1.0         1.000000        0.000000  \n",
              "546                1.0         1.000000        0.000000  \n",
              "559                1.0         0.991667        0.016667  \n",
              "\n",
              "[836 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-31e2ff97-2c6b-4cc5-8bec-639a09b4df12\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iter</th>\n",
              "      <th>n_resources</th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_clf</th>\n",
              "      <th>param_clf__bootstrap</th>\n",
              "      <th>param_clf__max_depth</th>\n",
              "      <th>param_clf__max_features</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>split1_train_score</th>\n",
              "      <th>split2_train_score</th>\n",
              "      <th>split3_train_score</th>\n",
              "      <th>split4_train_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>751</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.049146</td>\n",
              "      <td>0.004337</td>\n",
              "      <td>0.001262</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, random_sta...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.700654</td>\n",
              "      <td>0.138488</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>748</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.038550</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.001127</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, random_sta...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.700654</td>\n",
              "      <td>0.138488</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>747</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.042893</td>\n",
              "      <td>0.004091</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, random_sta...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.700654</td>\n",
              "      <td>0.138488</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.041899</td>\n",
              "      <td>0.000848</td>\n",
              "      <td>0.001184</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, random_sta...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.700654</td>\n",
              "      <td>0.138488</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>745</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.043049</td>\n",
              "      <td>0.001973</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, random_sta...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.700654</td>\n",
              "      <td>0.138488</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002431</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000556</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.240370</td>\n",
              "      <td>830</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002780</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>0.000604</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.240370</td>\n",
              "      <td>830</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>549</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002454</td>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.000533</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.240370</td>\n",
              "      <td>830</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002420</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.240370</td>\n",
              "      <td>830</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>559</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002140</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000534</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>0.149666</td>\n",
              "      <td>836</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.991667</td>\n",
              "      <td>0.016667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>836 rows × 37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31e2ff97-2c6b-4cc5-8bec-639a09b4df12')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-31e2ff97-2c6b-4cc5-8bec-639a09b4df12 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-31e2ff97-2c6b-4cc5-8bec-639a09b4df12');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "ranking = pd.DataFrame(anger_model.cv_results_)\n",
        "ranking = ranking.sort_values('rank_test_score')\n",
        "ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UmFL2wU4nlo",
        "outputId": "63f4e1ab-f0ef-4853-e1dc-1b3463d83809"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf': XGBClassifier(colsample_bytree=0.5, random_state=3380, reg_lambda=0,\n",
              "               subsample=0.8),\n",
              " 'clf__colsample_bytree': 0.5,\n",
              " 'clf__gamma': 0,\n",
              " 'clf__learning_rate': 0.1,\n",
              " 'clf__max_depth': 3,\n",
              " 'clf__reg_lambda': 0,\n",
              " 'clf__scale_pos_weight': 1,\n",
              " 'clf__subsample': 0.8}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "anger_model.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xAgO-Qbw7Ev"
      },
      "source": [
        "#### **Joy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D32G5chvxChd"
      },
      "outputs": [],
      "source": [
        "grid = HalvingGridSearchCV(pipeline, param_grid, cv = 5, verbose = 1, scoring = 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2nPyf_AeYV7"
      },
      "outputs": [],
      "source": [
        "X, y = resample(train['joy']['tweet'], train['joy']['sentiment_intensity'], param_over = False, param_under = False)\n",
        "embedding_joy = Doc2Vector(tokenizer = tokenizador).transform(X)\n",
        "embedding_joy = PCA(n_components = 15).fit_transform(embedding_joy)\n",
        "features_joy = FeaturesTransformer().transform(X)\n",
        "data_joy = np.hstack([embedding_joy, features_joy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ri37ZVF3ZXs",
        "outputId": "ef778c88-c94d-49a4-d0e9-4ae9ecf7985b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 4\n",
            "n_required_iterations: 6\n",
            "n_possible_iterations: 4\n",
            "min_resources_: 30\n",
            "max_resources_: 902\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 564\n",
            "n_resources: 30\n",
            "Fitting 5 folds for each of 564 candidates, totalling 2820 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 188\n",
            "n_resources: 90\n",
            "Fitting 5 folds for each of 188 candidates, totalling 940 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 63\n",
            "n_resources: 270\n",
            "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 21\n",
            "n_resources: 810\n",
            "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
          ]
        }
      ],
      "source": [
        "joy_model = grid.fit(data_joy, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "qzBR2Kq0e3LL",
        "outputId": "1cd5b3c6-0985-4a66-ca6a-66b165820cdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
              "290     0           30       0.023400      0.000644         0.000881   \n",
              "289     0           30       0.024462      0.001580         0.000845   \n",
              "288     0           30       0.023808      0.000622         0.000887   \n",
              "605     1           90       0.047792      0.003131         0.001085   \n",
              "606     1           90       0.046639      0.000366         0.001087   \n",
              "..    ...          ...            ...           ...              ...   \n",
              "552     0           30       0.002005      0.000239         0.000511   \n",
              "549     0           30       0.002375      0.000689         0.000614   \n",
              "546     0           30       0.002440      0.000597         0.000739   \n",
              "555     0           30       0.002872      0.000648         0.000754   \n",
              "558     0           30       0.002721      0.000531         0.000691   \n",
              "\n",
              "     std_score_time                                          param_clf  \\\n",
              "290        0.000053  XGBClassifier(colsample_bytree=0.5, learning_r...   \n",
              "289        0.000055  XGBClassifier(colsample_bytree=0.5, learning_r...   \n",
              "288        0.000035  XGBClassifier(colsample_bytree=0.5, learning_r...   \n",
              "605        0.000019  XGBClassifier(colsample_bytree=0.5, learning_r...   \n",
              "606        0.000025  XGBClassifier(colsample_bytree=0.5, learning_r...   \n",
              "..              ...                                                ...   \n",
              "552        0.000009           SVC(probability=True, random_state=3380)   \n",
              "549        0.000108           SVC(probability=True, random_state=3380)   \n",
              "546        0.000258           SVC(probability=True, random_state=3380)   \n",
              "555        0.000151           SVC(probability=True, random_state=3380)   \n",
              "558        0.000158           SVC(probability=True, random_state=3380)   \n",
              "\n",
              "    param_clf__bootstrap param_clf__max_depth param_clf__max_features  ...  \\\n",
              "290                  NaN                    3                     NaN  ...   \n",
              "289                  NaN                    3                     NaN  ...   \n",
              "288                  NaN                    3                     NaN  ...   \n",
              "605                  NaN                    4                     NaN  ...   \n",
              "606                  NaN                    4                     NaN  ...   \n",
              "..                   ...                  ...                     ...  ...   \n",
              "552                  NaN                  NaN                     NaN  ...   \n",
              "549                  NaN                  NaN                     NaN  ...   \n",
              "546                  NaN                  NaN                     NaN  ...   \n",
              "555                  NaN                  NaN                     NaN  ...   \n",
              "558                  NaN                  NaN                     NaN  ...   \n",
              "\n",
              "    mean_test_score std_test_score rank_test_score split0_train_score  \\\n",
              "290        0.680000       0.220706               1                1.0   \n",
              "289        0.680000       0.220706               1                1.0   \n",
              "288        0.680000       0.220706               1                1.0   \n",
              "605        0.656209       0.055243               4                1.0   \n",
              "606        0.656209       0.055243               4                1.0   \n",
              "..              ...            ...             ...                ...   \n",
              "552        0.513333       0.222711             831                1.0   \n",
              "549        0.513333       0.222711             831                1.0   \n",
              "546        0.513333       0.222711             831                1.0   \n",
              "555        0.513333       0.222711             831                1.0   \n",
              "558        0.513333       0.222711             831                1.0   \n",
              "\n",
              "    split1_train_score split2_train_score split3_train_score  \\\n",
              "290           1.000000                1.0                1.0   \n",
              "289           1.000000                1.0                1.0   \n",
              "288           1.000000                1.0                1.0   \n",
              "605           0.985915                1.0                1.0   \n",
              "606           0.985915                1.0                1.0   \n",
              "..                 ...                ...                ...   \n",
              "552           1.000000                1.0                1.0   \n",
              "549           1.000000                1.0                1.0   \n",
              "546           1.000000                1.0                1.0   \n",
              "555           1.000000                1.0                1.0   \n",
              "558           1.000000                1.0                1.0   \n",
              "\n",
              "    split4_train_score mean_train_score std_train_score  \n",
              "290                1.0         1.000000        0.000000  \n",
              "289                1.0         1.000000        0.000000  \n",
              "288                1.0         1.000000        0.000000  \n",
              "605                1.0         0.997183        0.005634  \n",
              "606                1.0         0.997183        0.005634  \n",
              "..                 ...              ...             ...  \n",
              "552                1.0         1.000000        0.000000  \n",
              "549                1.0         1.000000        0.000000  \n",
              "546                1.0         1.000000        0.000000  \n",
              "555                1.0         1.000000        0.000000  \n",
              "558                1.0         1.000000        0.000000  \n",
              "\n",
              "[836 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7c1350f-1c60-45d3-a82b-e0673ca6aff8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iter</th>\n",
              "      <th>n_resources</th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_clf</th>\n",
              "      <th>param_clf__bootstrap</th>\n",
              "      <th>param_clf__max_depth</th>\n",
              "      <th>param_clf__max_features</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>split1_train_score</th>\n",
              "      <th>split2_train_score</th>\n",
              "      <th>split3_train_score</th>\n",
              "      <th>split4_train_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.023400</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>0.000881</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, learning_r...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.220706</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.024462</td>\n",
              "      <td>0.001580</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, learning_r...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.220706</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.023808</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>0.000887</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, learning_r...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.220706</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.047792</td>\n",
              "      <td>0.003131</td>\n",
              "      <td>0.001085</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, learning_r...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.656209</td>\n",
              "      <td>0.055243</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.985915</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.997183</td>\n",
              "      <td>0.005634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>606</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.046639</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>0.001087</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, learning_r...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.656209</td>\n",
              "      <td>0.055243</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.985915</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.997183</td>\n",
              "      <td>0.005634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002005</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.000511</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.513333</td>\n",
              "      <td>0.222711</td>\n",
              "      <td>831</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>549</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002375</td>\n",
              "      <td>0.000689</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.513333</td>\n",
              "      <td>0.222711</td>\n",
              "      <td>831</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002440</td>\n",
              "      <td>0.000597</td>\n",
              "      <td>0.000739</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.513333</td>\n",
              "      <td>0.222711</td>\n",
              "      <td>831</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002872</td>\n",
              "      <td>0.000648</td>\n",
              "      <td>0.000754</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.513333</td>\n",
              "      <td>0.222711</td>\n",
              "      <td>831</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.000691</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.513333</td>\n",
              "      <td>0.222711</td>\n",
              "      <td>831</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>836 rows × 37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7c1350f-1c60-45d3-a82b-e0673ca6aff8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a7c1350f-1c60-45d3-a82b-e0673ca6aff8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a7c1350f-1c60-45d3-a82b-e0673ca6aff8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "ranking = pd.DataFrame(joy_model.cv_results_)\n",
        "ranking = ranking.sort_values('rank_test_score')\n",
        "ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slf24N0Se3zB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88477599-a6fc-4f37-f576-d0dfcffe0954"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf': XGBClassifier(colsample_bytree=0.5, learning_rate=0.05, random_state=3380,\n",
              "               reg_lambda=0, scale_pos_weight=5, subsample=0.8),\n",
              " 'clf__colsample_bytree': 0.5,\n",
              " 'clf__gamma': 0,\n",
              " 'clf__learning_rate': 0.05,\n",
              " 'clf__max_depth': 3,\n",
              " 'clf__reg_lambda': 0,\n",
              " 'clf__scale_pos_weight': 5,\n",
              " 'clf__subsample': 0.8}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "joy_model.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88EjZhfYw8sO"
      },
      "source": [
        "#### **Fear**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DPQeYKOxC6j"
      },
      "outputs": [],
      "source": [
        "grid = HalvingGridSearchCV(pipeline, param_grid, cv = 5, verbose = 1, scoring = 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I_nU-78yl7v"
      },
      "outputs": [],
      "source": [
        "X, y = resample(train['fear']['tweet'], train['fear']['sentiment_intensity'], param_over = False, param_under = False)\n",
        "embedding_fear = Doc2Vector(tokenizer = tokenizador).transform(X)\n",
        "embedding_fear = PCA(n_components = 20).fit_transform(embedding_fear)\n",
        "features_fear = FeaturesTransformer().transform(X)\n",
        "data_fear = np.hstack([embedding_fear, features_fear])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpnRyfSq3b1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eabe4670-3afd-4760-ad7b-b0be6ca023f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 4\n",
            "n_required_iterations: 6\n",
            "n_possible_iterations: 4\n",
            "min_resources_: 30\n",
            "max_resources_: 1257\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 564\n",
            "n_resources: 30\n",
            "Fitting 5 folds for each of 564 candidates, totalling 2820 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 188\n",
            "n_resources: 90\n",
            "Fitting 5 folds for each of 188 candidates, totalling 940 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 63\n",
            "n_resources: 270\n",
            "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 21\n",
            "n_resources: 810\n",
            "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
          ]
        }
      ],
      "source": [
        "fear_model = grid.fit(data_fear, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ranking = pd.DataFrame(fear_model.cv_results_)\n",
        "ranking = ranking.sort_values('rank_test_score')\n",
        "ranking"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "Hc-aW6gVbKqp",
        "outputId": "064bcd3b-b237-4381-c2d7-a31b245213fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
              "711     1           90       0.743417      0.005757         0.073839   \n",
              "707     1           90       0.732907      0.009431         0.067356   \n",
              "686     1           90       0.732202      0.006730         0.074616   \n",
              "682     1           90       0.738754      0.006167         0.075639   \n",
              "737     1           90       0.148516      0.006317         0.014789   \n",
              "..    ...          ...            ...           ...              ...   \n",
              "555     0           30       0.002242      0.000228         0.000534   \n",
              "558     0           30       0.002175      0.000144         0.000523   \n",
              "559     0           30       0.003101      0.000727         0.000762   \n",
              "561     0           30       0.002152      0.000183         0.000512   \n",
              "546     0           30       0.002218      0.000069         0.000541   \n",
              "\n",
              "     std_score_time                                  param_clf  \\\n",
              "711        0.010863  RandomForestClassifier(random_state=3380)   \n",
              "707        0.002275  RandomForestClassifier(random_state=3380)   \n",
              "686        0.012616  RandomForestClassifier(random_state=3380)   \n",
              "682        0.010849  RandomForestClassifier(random_state=3380)   \n",
              "737        0.001461  RandomForestClassifier(random_state=3380)   \n",
              "..              ...                                        ...   \n",
              "555        0.000015   SVC(probability=True, random_state=3380)   \n",
              "558        0.000017   SVC(probability=True, random_state=3380)   \n",
              "559        0.000152   SVC(probability=True, random_state=3380)   \n",
              "561        0.000011   SVC(probability=True, random_state=3380)   \n",
              "546        0.000024   SVC(probability=True, random_state=3380)   \n",
              "\n",
              "    param_clf__bootstrap param_clf__max_depth param_clf__max_features  ...  \\\n",
              "711                 True                  110                       3  ...   \n",
              "707                 True                  100                       3  ...   \n",
              "686                 True                   80                       3  ...   \n",
              "682                 True                   90                       3  ...   \n",
              "737                 True                   90                       2  ...   \n",
              "..                   ...                  ...                     ...  ...   \n",
              "555                  NaN                  NaN                     NaN  ...   \n",
              "558                  NaN                  NaN                     NaN  ...   \n",
              "559                  NaN                  NaN                     NaN  ...   \n",
              "561                  NaN                  NaN                     NaN  ...   \n",
              "546                  NaN                  NaN                     NaN  ...   \n",
              "\n",
              "    mean_test_score std_test_score rank_test_score split0_train_score  \\\n",
              "711        0.690196       0.113251               1           0.957746   \n",
              "707        0.690196       0.113251               1           0.957746   \n",
              "686        0.690196       0.113251               1           0.957746   \n",
              "682        0.690196       0.113251               1           0.957746   \n",
              "737        0.689542       0.115077               5           0.816901   \n",
              "..              ...            ...             ...                ...   \n",
              "555        0.326667       0.189033             828           1.000000   \n",
              "558        0.326667       0.189033             828           1.000000   \n",
              "559        0.326667       0.189033             828           1.000000   \n",
              "561        0.326667       0.189033             828           1.000000   \n",
              "546        0.326667       0.189033             828           1.000000   \n",
              "\n",
              "    split1_train_score split2_train_score split3_train_score  \\\n",
              "711           0.887324           0.847222           0.875000   \n",
              "707           0.887324           0.847222           0.875000   \n",
              "686           0.887324           0.847222           0.875000   \n",
              "682           0.887324           0.847222           0.875000   \n",
              "737           0.788732           0.736111           0.722222   \n",
              "..                 ...                ...                ...   \n",
              "555           1.000000           1.000000           1.000000   \n",
              "558           1.000000           1.000000           1.000000   \n",
              "559           1.000000           1.000000           1.000000   \n",
              "561           1.000000           1.000000           1.000000   \n",
              "546           1.000000           1.000000           1.000000   \n",
              "\n",
              "    split4_train_score mean_train_score std_train_score  \n",
              "711           0.833333         0.880125        0.043302  \n",
              "707           0.833333         0.880125        0.043302  \n",
              "686           0.833333         0.880125        0.043302  \n",
              "682           0.833333         0.880125        0.043302  \n",
              "737           0.694444         0.751682        0.044750  \n",
              "..                 ...              ...             ...  \n",
              "555           1.000000         1.000000        0.000000  \n",
              "558           1.000000         1.000000        0.000000  \n",
              "559           1.000000         1.000000        0.000000  \n",
              "561           1.000000         1.000000        0.000000  \n",
              "546           1.000000         1.000000        0.000000  \n",
              "\n",
              "[836 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d02e73f-64a3-405b-ad3d-237bd9697692\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iter</th>\n",
              "      <th>n_resources</th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_clf</th>\n",
              "      <th>param_clf__bootstrap</th>\n",
              "      <th>param_clf__max_depth</th>\n",
              "      <th>param_clf__max_features</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>split1_train_score</th>\n",
              "      <th>split2_train_score</th>\n",
              "      <th>split3_train_score</th>\n",
              "      <th>split4_train_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>711</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.743417</td>\n",
              "      <td>0.005757</td>\n",
              "      <td>0.073839</td>\n",
              "      <td>0.010863</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>110</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.113251</td>\n",
              "      <td>1</td>\n",
              "      <td>0.957746</td>\n",
              "      <td>0.887324</td>\n",
              "      <td>0.847222</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.880125</td>\n",
              "      <td>0.043302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>707</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.732907</td>\n",
              "      <td>0.009431</td>\n",
              "      <td>0.067356</td>\n",
              "      <td>0.002275</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.113251</td>\n",
              "      <td>1</td>\n",
              "      <td>0.957746</td>\n",
              "      <td>0.887324</td>\n",
              "      <td>0.847222</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.880125</td>\n",
              "      <td>0.043302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>686</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.732202</td>\n",
              "      <td>0.006730</td>\n",
              "      <td>0.074616</td>\n",
              "      <td>0.012616</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>80</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.113251</td>\n",
              "      <td>1</td>\n",
              "      <td>0.957746</td>\n",
              "      <td>0.887324</td>\n",
              "      <td>0.847222</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.880125</td>\n",
              "      <td>0.043302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>682</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.738754</td>\n",
              "      <td>0.006167</td>\n",
              "      <td>0.075639</td>\n",
              "      <td>0.010849</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.113251</td>\n",
              "      <td>1</td>\n",
              "      <td>0.957746</td>\n",
              "      <td>0.887324</td>\n",
              "      <td>0.847222</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.880125</td>\n",
              "      <td>0.043302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>737</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.148516</td>\n",
              "      <td>0.006317</td>\n",
              "      <td>0.014789</td>\n",
              "      <td>0.001461</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>90</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.689542</td>\n",
              "      <td>0.115077</td>\n",
              "      <td>5</td>\n",
              "      <td>0.816901</td>\n",
              "      <td>0.788732</td>\n",
              "      <td>0.736111</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.694444</td>\n",
              "      <td>0.751682</td>\n",
              "      <td>0.044750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002242</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000534</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.326667</td>\n",
              "      <td>0.189033</td>\n",
              "      <td>828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002175</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>0.000523</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.326667</td>\n",
              "      <td>0.189033</td>\n",
              "      <td>828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>559</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.003101</td>\n",
              "      <td>0.000727</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.326667</td>\n",
              "      <td>0.189033</td>\n",
              "      <td>828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>561</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002152</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.326667</td>\n",
              "      <td>0.189033</td>\n",
              "      <td>828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002218</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000541</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.326667</td>\n",
              "      <td>0.189033</td>\n",
              "      <td>828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>836 rows × 37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d02e73f-64a3-405b-ad3d-237bd9697692')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3d02e73f-64a3-405b-ad3d-237bd9697692 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3d02e73f-64a3-405b-ad3d-237bd9697692');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fear_model.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3N6poKXvRFU",
        "outputId": "27598db0-e320-4c15-e11a-f5b046c75248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf': XGBClassifier(colsample_bytree=0.5, gamma=1, max_depth=4, random_state=3380,\n",
              "               reg_lambda=10, scale_pos_weight=5, subsample=0.8),\n",
              " 'clf__colsample_bytree': 0.5,\n",
              " 'clf__gamma': 1,\n",
              " 'clf__learning_rate': 0.1,\n",
              " 'clf__max_depth': 4,\n",
              " 'clf__reg_lambda': 10,\n",
              " 'clf__scale_pos_weight': 5,\n",
              " 'clf__subsample': 0.8}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E152EsRWw_xK"
      },
      "source": [
        "#### **Sadness**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmHSTYElxDXG"
      },
      "outputs": [],
      "source": [
        "grid = HalvingGridSearchCV(pipeline, param_grid, cv = 5, verbose = 1, scoring = 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvZAb2PPymhM"
      },
      "outputs": [],
      "source": [
        "X, y = resample(train['sadness']['tweet'], train['sadness']['sentiment_intensity'], param_over = False, param_under = False) # evitamos resampleo pues no queremos ensuciar los split\n",
        "embedding_sadness = Doc2Vector(tokenizer = tokenizador).transform(X)\n",
        "features_sadness = FeaturesTransformer().transform(X)\n",
        "data_sadness = np.hstack([embedding_sadness, features_sadness])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE0w9u913f1w",
        "outputId": "b00d62cc-ca4f-42ee-d4bb-b2130bcfffae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 4\n",
            "n_required_iterations: 6\n",
            "n_possible_iterations: 4\n",
            "min_resources_: 30\n",
            "max_resources_: 860\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 564\n",
            "n_resources: 30\n",
            "Fitting 5 folds for each of 564 candidates, totalling 2820 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 188\n",
            "n_resources: 90\n",
            "Fitting 5 folds for each of 188 candidates, totalling 940 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 63\n",
            "n_resources: 270\n",
            "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 21\n",
            "n_resources: 810\n",
            "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
          ]
        }
      ],
      "source": [
        "sadness_model = grid.fit(data_sadness, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "cTjy5P-5LDYx",
        "outputId": "0a16808b-8dff-4969-ac94-9cd47333acc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
              "762     2          270       0.444558      0.004305         0.001821   \n",
              "785     2          270       0.598441      0.003625         0.002032   \n",
              "782     2          270       0.599032      0.005539         0.002057   \n",
              "781     2          270       0.601370      0.006744         0.002036   \n",
              "759     2          270       0.725932      0.007959         0.001965   \n",
              "..    ...          ...            ...           ...              ...   \n",
              "103     0           30       0.727339      0.011571         0.073304   \n",
              "184     0           30       0.727123      0.010713         0.081510   \n",
              "101     0           30       1.464791      0.009670         0.155428   \n",
              "71      0           30       1.450002      0.017268         0.147436   \n",
              "79      0           30       0.716358      0.009638         0.081437   \n",
              "\n",
              "     std_score_time                                          param_clf  \\\n",
              "762        0.000018  XGBClassifier(colsample_bytree=0.5, gamma=1, l...   \n",
              "785        0.000030  XGBClassifier(colsample_bytree=0.5, gamma=1, l...   \n",
              "782        0.000060  XGBClassifier(colsample_bytree=0.5, gamma=1, l...   \n",
              "781        0.000017  XGBClassifier(colsample_bytree=0.5, gamma=1, l...   \n",
              "759        0.000026  XGBClassifier(colsample_bytree=0.5, gamma=1, l...   \n",
              "..              ...                                                ...   \n",
              "103        0.008294          RandomForestClassifier(random_state=3380)   \n",
              "184        0.015027          RandomForestClassifier(random_state=3380)   \n",
              "101        0.014966          RandomForestClassifier(random_state=3380)   \n",
              "71         0.010547          RandomForestClassifier(random_state=3380)   \n",
              "79         0.009578          RandomForestClassifier(random_state=3380)   \n",
              "\n",
              "    param_clf__bootstrap param_clf__max_depth param_clf__max_features  ...  \\\n",
              "762                  NaN                    3                     NaN  ...   \n",
              "785                  NaN                    4                     NaN  ...   \n",
              "782                  NaN                    4                     NaN  ...   \n",
              "781                  NaN                    4                     NaN  ...   \n",
              "759                  NaN                    7                     NaN  ...   \n",
              "..                   ...                  ...                     ...  ...   \n",
              "103                 True                   90                       3  ...   \n",
              "184                 True                  110                       2  ...   \n",
              "101                 True                   90                       3  ...   \n",
              "71                  True                   90                       2  ...   \n",
              "79                  True                   90                       2  ...   \n",
              "\n",
              "    mean_test_score std_test_score rank_test_score split0_train_score  \\\n",
              "762        0.659259       0.027716               1           1.000000   \n",
              "785        0.651852       0.048855               2           1.000000   \n",
              "782        0.651852       0.048855               2           1.000000   \n",
              "781        0.651852       0.048855               2           1.000000   \n",
              "759        0.651852       0.048855               2           1.000000   \n",
              "..              ...            ...             ...                ...   \n",
              "103        0.300000       0.124722             703           0.833333   \n",
              "184        0.300000       0.124722             703           0.750000   \n",
              "101        0.300000       0.124722             703           0.833333   \n",
              "71         0.300000       0.124722             703           0.791667   \n",
              "79         0.300000       0.124722             703           0.625000   \n",
              "\n",
              "    split1_train_score split2_train_score split3_train_score  \\\n",
              "762           1.000000           1.000000           1.000000   \n",
              "785           1.000000           1.000000           1.000000   \n",
              "782           1.000000           1.000000           1.000000   \n",
              "781           1.000000           1.000000           1.000000   \n",
              "759           1.000000           1.000000           1.000000   \n",
              "..                 ...                ...                ...   \n",
              "103           0.541667           0.666667           0.583333   \n",
              "184           0.541667           0.666667           0.583333   \n",
              "101           0.541667           0.666667           0.583333   \n",
              "71            0.541667           0.666667           0.583333   \n",
              "79            0.541667           0.666667           0.583333   \n",
              "\n",
              "    split4_train_score mean_train_score std_train_score  \n",
              "762           1.000000         1.000000        0.000000  \n",
              "785           1.000000         1.000000        0.000000  \n",
              "782           1.000000         1.000000        0.000000  \n",
              "781           1.000000         1.000000        0.000000  \n",
              "759           1.000000         1.000000        0.000000  \n",
              "..                 ...              ...             ...  \n",
              "103           0.583333         0.641667        0.104083  \n",
              "184           0.541667         0.616667        0.080795  \n",
              "101           0.666667         0.658333        0.100000  \n",
              "71            0.541667         0.625000        0.095015  \n",
              "79            0.541667         0.591667        0.048591  \n",
              "\n",
              "[836 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2357b629-7be1-418e-a9e0-44522f7ac2c8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iter</th>\n",
              "      <th>n_resources</th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_clf</th>\n",
              "      <th>param_clf__bootstrap</th>\n",
              "      <th>param_clf__max_depth</th>\n",
              "      <th>param_clf__max_features</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>split1_train_score</th>\n",
              "      <th>split2_train_score</th>\n",
              "      <th>split3_train_score</th>\n",
              "      <th>split4_train_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>762</th>\n",
              "      <td>2</td>\n",
              "      <td>270</td>\n",
              "      <td>0.444558</td>\n",
              "      <td>0.004305</td>\n",
              "      <td>0.001821</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, gamma=1, l...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.659259</td>\n",
              "      <td>0.027716</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785</th>\n",
              "      <td>2</td>\n",
              "      <td>270</td>\n",
              "      <td>0.598441</td>\n",
              "      <td>0.003625</td>\n",
              "      <td>0.002032</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, gamma=1, l...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.651852</td>\n",
              "      <td>0.048855</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782</th>\n",
              "      <td>2</td>\n",
              "      <td>270</td>\n",
              "      <td>0.599032</td>\n",
              "      <td>0.005539</td>\n",
              "      <td>0.002057</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, gamma=1, l...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.651852</td>\n",
              "      <td>0.048855</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>2</td>\n",
              "      <td>270</td>\n",
              "      <td>0.601370</td>\n",
              "      <td>0.006744</td>\n",
              "      <td>0.002036</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, gamma=1, l...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.651852</td>\n",
              "      <td>0.048855</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>2</td>\n",
              "      <td>270</td>\n",
              "      <td>0.725932</td>\n",
              "      <td>0.007959</td>\n",
              "      <td>0.001965</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, gamma=1, l...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.651852</td>\n",
              "      <td>0.048855</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.727339</td>\n",
              "      <td>0.011571</td>\n",
              "      <td>0.073304</td>\n",
              "      <td>0.008294</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.124722</td>\n",
              "      <td>703</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.104083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.727123</td>\n",
              "      <td>0.010713</td>\n",
              "      <td>0.081510</td>\n",
              "      <td>0.015027</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>110</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.124722</td>\n",
              "      <td>703</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.616667</td>\n",
              "      <td>0.080795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>1.464791</td>\n",
              "      <td>0.009670</td>\n",
              "      <td>0.155428</td>\n",
              "      <td>0.014966</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.124722</td>\n",
              "      <td>703</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.658333</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>1.450002</td>\n",
              "      <td>0.017268</td>\n",
              "      <td>0.147436</td>\n",
              "      <td>0.010547</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>90</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.124722</td>\n",
              "      <td>703</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.095015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.716358</td>\n",
              "      <td>0.009638</td>\n",
              "      <td>0.081437</td>\n",
              "      <td>0.009578</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>90</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.124722</td>\n",
              "      <td>703</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.591667</td>\n",
              "      <td>0.048591</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>836 rows × 37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2357b629-7be1-418e-a9e0-44522f7ac2c8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2357b629-7be1-418e-a9e0-44522f7ac2c8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2357b629-7be1-418e-a9e0-44522f7ac2c8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "ranking = pd.DataFrame(sadness_model.cv_results_)\n",
        "ranking = ranking.sort_values('rank_test_score')\n",
        "ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pi_XaJBK7kx",
        "outputId": "2719b9c9-32b3-4dd0-ca4f-91e3c96c8a79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf': XGBClassifier(colsample_bytree=0.5, gamma=1, learning_rate=0.05, max_depth=4,\n",
              "               random_state=3380, reg_lambda=0, scale_pos_weight=3,\n",
              "               subsample=0.8),\n",
              " 'clf__colsample_bytree': 0.5,\n",
              " 'clf__gamma': 1,\n",
              " 'clf__learning_rate': 0.05,\n",
              " 'clf__max_depth': 4,\n",
              " 'clf__reg_lambda': 0,\n",
              " 'clf__scale_pos_weight': 3,\n",
              " 'clf__subsample': 0.8}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "sadness_model.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Último entrenamiento**"
      ],
      "metadata": {
        "id": "dHbFdpSj6v4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente y con tal de lograr una mejor predicción, entrenamos nuestros modelos con toda la data disponible para luego generar las predicciones."
      ],
      "metadata": {
        "id": "bOYJYNcqIlvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_evaluate_model(pipeline, sentiment: str, param_over = False, param_under = False):\n",
        "\n",
        "  dataset = train[sentiment]\n",
        "  \n",
        "  # ejecutamos el pipeline sobre el dataset\n",
        "  classifier, learned_labels = final_run(dataset, sentiment, pipeline, param_over, param_under)\n",
        "  \n",
        "  return classifier, learned_labels"
      ],
      "metadata": {
        "id": "OzGG5ffb6059"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_run(dataset, dataset_name, pipeline, param_over = False, param_under = False):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test, aún no se transforma de Strings a valores numéricos.\n",
        "\n",
        "    X = dataset.tweet\n",
        "    y = dataset.sentiment_intensity\n",
        "\n",
        "    # Balanceo de clases\n",
        "    X, y = resample(X, y, param_over, param_under)\n",
        "\n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline). \n",
        "    # En este caso el Bag of Words es el encargado de transformar de Strings a vectores numéricos.\n",
        "    pipeline.fit(X, y)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "    \n",
        "    # Evaluamos:\n",
        "    return pipeline, learned_labels"
      ],
      "metadata": {
        "id": "Gtg5AIEs6-ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline_anger():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding_pca', Pipeline([('embedding', Doc2Vector(tokenizer = tokenizador)),\n",
        "                                                            ('pca', PCA(n_components = 15)),\n",
        "                                                            ])), \n",
        "                                  ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', XGBClassifier(colsample_bytree = 0.5, gamma = 0, learning_rate = 0.05, max_depth = 3, \n",
        "                            reg_lambda = 10, scale_pos_weight = 5, subsample = 0.8, random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "final_anger, labels_anger = final_evaluate_model(get_pipeline_anger(), 'anger', param_over = False, param_under = False)"
      ],
      "metadata": {
        "id": "rOnCmUsrGAWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline_joy():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding_pca', Pipeline([('embedding', Doc2Vector(tokenizer = tokenizador)),\n",
        "                                                            ('pca', PCA(n_components = 15)),\n",
        "                                                            ])), \n",
        "                                  ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(max_depth = 80, max_features = 3, min_samples_leaf = 3, \n",
        "                                     min_samples_split = 12, n_estimators = 500, random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "final_joy, labels_joy = final_evaluate_model(get_pipeline_joy(), 'joy', param_over = False, param_under = False)"
      ],
      "metadata": {
        "id": "yn4r0JoCF5Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline_fear():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding_pca', Pipeline([('embedding', Doc2Vector(tokenizer = tokenizador)),\n",
        "                                                            ('pca', PCA(n_components = 20)),\n",
        "                                                            ])), \n",
        "                                  ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(max_depth = 110, max_features = 3, min_samples_leaf = 3, \n",
        "                                     min_samples_split = 12, n_estimators = 1000, random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "final_fear, labels_fear = final_evaluate_model(get_pipeline_fear(), 'fear', param_over = False, param_under = False)"
      ],
      "metadata": {
        "id": "zk0dVXdFEmUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline_sadness():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding', Doc2Vector(tokenizer = tokenizador)), \n",
        "                                    ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', XGBClassifier(colsample_bytree = 0.5, max_depth = 7, subsample = 0.8, \n",
        "                                     gamma = 0, learning_rate = 0.1, reg_lambda = 1, scale_pos_weight = 1,\n",
        "                                     random_state = 3380)),\n",
        "      ])                  \n",
        "  return pipe\n",
        "\n",
        "final_sadness, labels_sadness = final_evaluate_model(get_pipeline_sadness(), 'sadness', param_over = 0.1, param_under = False)"
      ],
      "metadata": {
        "id": "3GlrKf24CnV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_models = {'anger': final_anger, 'joy': final_joy, 'sadness': final_sadness, 'fear': final_fear}\n",
        "final_labels = {'anger': labels_anger, 'joy': labels_joy, 'sadness': labels_sadness, 'fear': labels_fear}"
      ],
      "metadata": {
        "id": "CCOyVD7gHahH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKwcde_zvI0"
      },
      "source": [
        "### **Predecir los target set y crear la submission**\n",
        "\n",
        "Aquí predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWDUoSmbzvI1"
      },
      "outputs": [],
      "source": [
        "def predict_target(dataset, classifier, labels):\n",
        "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
        "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
        "    \n",
        "    # Agregar ids\n",
        "    predicted['id'] = dataset.id.values\n",
        "    # Reordenar las columnas\n",
        "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
        "    return predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CJ4PTwZzvI1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "predicted_target = {}\n",
        "\n",
        "# Crear carpeta ./predictions\n",
        "if (not os.path.exists('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "# por cada target set:\n",
        "for idx, key in enumerate(target):\n",
        "    # Predecirlo\n",
        "    predicted_target[key] = predict_target(target[key], final_models[key],\n",
        "                                           final_labels[key])\n",
        "    # Guardar predicciones en archivos separados. \n",
        "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
        "                                 sep='\\t',\n",
        "                                 header=False,\n",
        "                                 index=False)\n",
        "\n",
        "# Crear archivo zip\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Resultados**\n"
      ],
      "metadata": {
        "id": "EYAfgeyrN2pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Resultados**: Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones.  Pueden mostrar los resultados sobre la partición de validación en caso que la generen o sobre los resultados del conjunto de testing. Mostrar los resultados en alguna tabla, pueden poner aquí también los resultados obtenidos al realizar la submission. (**0.5 puntos**)\n"
      ],
      "metadata": {
        "id": "lm1IO9EHN51j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, se presenta una muestra de los clasificadores entrenados con sus respectivos puntajes:\n",
        "\n",
        "| No. | Approach                       || Dataset   | AUC   | Kappa | Accuracy |\n",
        "|-----|--------------------------------||-----------|-------|-------|----------|\n",
        "|     | Features        | Clasifier     |           |       |       |          |\n",
        "| 0   | bow+chars_count | MultinomialNB | anger     | 0.622 | 0.163 | 0.688    |\n",
        "|     |                 |               | fear      | 0.597 | 0.091 | 0.559    |\n",
        "|     |                 |               | joy       | 0.728 | 0.251 | 0.601    |\n",
        "|     |                 |               | sadness   | 0.645 | 0.166 | 0.581    |\n",
        "|     |                 |               |**average**| 0.648 | 0.168 | 0.607    |\n",
        "| 1   | pretrained embedding + features+ PCA (15)+ resampling  | RandomForest  | anger     |   0.734    |   0.366    |     0.756     |\n",
        "|     |                 |               | fear      |   0.746    |  0.354     |     0.67     |\n",
        "|     |                 |               | joy       |   0.776    |    0.448   |      0.695    |\n",
        "|     |                 |               | sadness   |    0.728   |   0.249    |   0.588       |\n",
        "|     |                 |               |**average**|    0.746   |    0.354   |    0.677      |\n",
        "| 2   | pretrained embedding + features | RandomForest | anger  | 0.688 | 0.223 | 0.72    |\n",
        "|     |                 |               | fear      | 0.735 | 0.288 | 0.653    |\n",
        "|     |                 |               | joy       | 0.755 | 0.343 | 0.664    |\n",
        "|     |                 |               | sadness   | 0.704 | 0.264 | 0.606    |\n",
        "|     |                 |               |**average**| 0.72 | 0.28 | 0.661    |\n",
        "| 3   | BOW 1,2-gramas  con PCA n=60 | XGboost | anger     | 0.583 | 0.136 | 0.685    |\n",
        "|     |                 |               | fear      | 0.635 | 0.177 | 0.588    |\n",
        "|     |                 |               | joy       | 0.658 | 0.165 | 0.577    |\n",
        "|     |                 |               | sadness   | 0.629 | 0.203 | 0.556    |\n",
        "|     |                 |               |**average**| 0.626 | 0.17 | 0.602    |\n",
        "| 4   | embedding propios + resampling | RandomForest | anger     | 0.636 | 0.186 | 0.633    |\n",
        "|     |                 |               | fear      | 0.557 | 0.051 | 0.383    |\n",
        "|     |                 |               | joy       | 0.628 | 0.201 | 0.463    |\n",
        "|     |                 |               | sadness   | 0.549 | 0.017 | 0.313    |\n",
        "|     |                 |               |**average**| 0.593 | 0.114 | 0.448    |\n",
        "| 5   | bow+embedding propios + features| RandomForest | anger     | 0.648 | 0.095 | 0.685    |\n",
        "|     |                 |               | fear      | 0.613 | 0.132 | 0.6    |\n",
        "|     |                 |               | joy       | 0.704 | 0.159 | 0.601    |\n",
        "|     |                 |               | sadness   | 0.663 | 0.067 | 0.542    |\n",
        "|     |                 |               |**average**| 0.657 | 0.113 | 0.607    |\n",
        "| 6   | TF_IDF + Features Extractor + PCA       | RandomForest | anger     | 0.732 | 0.255 | 0.723    |\n",
        "|     |                 |               | fear      | 0.702 | 0.296 | 0.641    |\n",
        "|     |                 |               | joy       | 0.748 | 0.351 | 0.641    |\n",
        "|     |                 |               | sadness   | 0.727 | 0.336 | 0.63    |\n",
        "|     |                 |               |**average**| 0.727 | 0.309 | 0.659    |"
      ],
      "metadata": {
        "id": "a23by_SJORMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir de la tabla se puede concluir que, a pesar de usar `RandomForest` en 5 de los 7 modelos mostrados, existe una amplia diferencia entre cada uno que se explica en el **pre procesamiento** de los datos. En primer lugar, notamos que utilizar embeddings propios no tiene un rendimiento tan bueno, pues este es menor al ofrecido por técnicas como Bag of Words o Td-Idf. Sin embargo, cuando al embedding se le añade también las features generadas, el clasificador posee un mayor poder de predicción que Bag of Words o el embedding por sí solo. En paralelo, se encuentran resultados sorprendentes con TF-IDF que, a pesar de ser una metodología de vectorización que resulta en representaciones *sparse* logra llegar a buenos resultados, incluso mejores que los embeddings entrenados con data propia. Por otro lado, es posible ver el poder de predicción de los embedding pre entrenados (*transfer learning*), pues con la mera implementación de estos embedding se logran resultados mejores que las técnicas ya citadas. Finalmente, es posible mejorar el poder de predicción de estos embedding concatenando features, resampling y reducción de dimensionalidad, logrando ser la mejor combinación de la tabla."
      ],
      "metadata": {
        "id": "wssolCDzuWV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 8. **Conclusiones**"
      ],
      "metadata": {
        "id": "BsceEkM7ODs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lo largo de todo el trabajo hemos visto como ciertas configuraciones han demostrado ser bastante superiores que otras alternativas. En primer lugar, vimos como los embeddings logran generar una mejor representación de los datos en comparación a metodologías como Bag of Words o TF-IDF. También vimos como el **Feature Engineering** logra mejorar mucho el poder de predicción de los clasificadores, sobre todo cuando se incorporan lexicones y librerías externas¡. Además, a pesar de que el set de datos está desbalanceado, metodologías de *resampling* lograron mejoras poco significativas sobre el poder de predicción de los modelos. Finalmente, vimos como la reducción de dimensionalidad logra un mejor aprendizaje por parte de los clasificadores (`RandomForest` y `XGBoost`).\n",
        "\n",
        "Se plantean las siguientes lineas de investigación para seguir mejorardo el clasificador:\n",
        "\n",
        "1. Probar con clasificadores basados en **redes neuronales**, los cuales no pudieron ser implementados a cabalidad por falta de conocimiento en librerias como `Pytorch`.\n",
        "\n",
        "2. Probar con metodologías de vectorización como `BERT`, los cuales logran capturar el contexto mejor que los `Embeddings`.\n",
        "\n",
        "3. Incorporar a los clasificadores los features generados por **Bag of Words** y **TF-IDF**."
      ],
      "metadata": {
        "id": "AXW1fLu5OGej"
      }
    }
  ]
}