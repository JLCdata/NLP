{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Entregable_Competencia 1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "E29LEMZ9zvIo",
        "OTAIEnSJzvIp",
        "kMOjYSQezvIq",
        "ECjkdgdwzvIq",
        "SJyTrr2onLOo",
        "oyj1fwJehpc3",
        "gzCUbD2ZRdJ-",
        "EYAfgeyrN2pf",
        "BsceEkM7ODs3"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:49:08.174519Z",
          "start_time": "2020-03-31T13:49:08.165989Z"
        },
        "id": "gpbvNOH0zvIi"
      },
      "source": [
        "# **Competencia 1 - CC6205 Natural Language Processing **\n",
        "\n",
        "**Integrantes:** Jos茅 Cadiz - Samuel Molina - Sebasti谩n Tinoco - Stefano Schiappacasse\n",
        "\n",
        "**Usuario del equipo en CodaLab:** stinoco y stefanosch\n",
        "\n",
        "**Fecha l铆mite de entrega :** Mi茅rcoles 20 de Abril.\n",
        "\n",
        "**Tiempo estimado de dedicaci贸n:** musho (alrededor de 20-30 hrs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Trabajo grupal**"
      ],
      "metadata": {
        "id": "aNFYF8O-Mhwd"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:34:25.683540Z",
          "start_time": "2020-03-31T13:34:25.673430Z"
        },
        "id": "E29LEMZ9zvIo"
      },
      "source": [
        "## **1. Introducci贸n**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "El presente reporte, contiene en detalle el desarrollo y resultados de la Competencia 1 del curso de Procesamiento de Lenguaje Natural CC6205-1. \n",
        "\n",
        "El objetivo de la competencia consiste en trabajar en la clasificaci贸n de tweets  seg煤n su intensidad de emoci贸n, lo cual corresponde a la task de clasificaci贸n de texto. \n",
        "\n",
        "Particularmente, se cuenta con 4 datasets de tweets con distintas emociones: `anger`, `fear`, `sadness` y `joy`. Deber谩n crear un clasificador para cada uno de estos datasets que indique la intensidad de dicha emoci贸n en sus tweets (`low`, `medium`, `high`).\n",
        "\n",
        "Para llevar a cabo esta tarea, se desarrollan 4 l铆neas de desarrollo: \n",
        "\n",
        "1. Experimentos con **Bag of Words**\n",
        "2. Experimentos con **TF-IDF**\n",
        "3. Experimentos con **Embeddings preentrenados**\n",
        "4. Experimentos con **Embeddings entrenados con la data**.\n",
        "\n",
        "En cada una de estas lineas de investigaci贸n, se probaron diferentes combinaciones de configuraciones para optimizar el poder de predicci贸n de los clasificadores.\n",
        "\n",
        "Finalmente, se evaluan los resultados de los modelos desarrollador para posteriormente cargar el mejor en la competencia de [CodaLab](https://codalab.org/).\n"
      ],
      "metadata": {
        "id": "j0SQN6dkfEiU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:13.474238Z",
          "start_time": "2020-03-31T13:47:13.454068Z"
        },
        "id": "OTAIEnSJzvIp"
      },
      "source": [
        "## **2. Representaciones**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Representaciones**: Describir los atributos y representaciones usadas como entrada de los clasificadores, **recordar** que para entrenar modelos el input debe tener su representaci贸n num茅rica. Si bien, con Bag of Words (**baseline**) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluaci贸n agregando m谩s atributos y representaciones dise帽adas a mano, sean lo m谩s creativos posible. M谩s abajo encontrar谩n una lista de estos posibles atributos que les podr谩 ser de utilidad. (**1 punto**)"
      ],
      "metadata": {
        "id": "Kd4bAzsag3Ye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci贸n se deja una lista de los atributos utilizados:\n",
        "\n",
        "* Una parte de los atributos extraidos fueron mediante el framework `awessome` que entrega un score que cuantifica la intensidad del sentimiento para cada frase.\n",
        "* Reconocedor de palabras negativas (no|not)\n",
        "* Cantidad de palabras positivas y negativas\n",
        "* Cantidad de emojis positivos y negativos \n",
        "* Cantidad de caracteres: #,!,?,@,* y mayusculas\n",
        "* Score de lexicon vader\n",
        "* Cantidad de palabras elongadas, cantidad de secuencias de ? y ! con un largo superior a 2.\n",
        "\n",
        "\n",
        "En cuanto a las representaci贸n del input utilizamos 3:\n",
        "* BOW\n",
        "* TF-IDF\n",
        "* Embeddings\n"
      ],
      "metadata": {
        "id": "pYNctSiBZbmU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMOjYSQezvIq"
      },
      "source": [
        "## **3. Algoritmos**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Algoritmos**: Describir **brevemente** los algoritmos de clasificaci贸n usados, tanto si fueron algoritmos ya vistos en clases o bien arquitecturas de Deep Learning. (**0.5 puntos**)"
      ],
      "metadata": {
        "id": "2aUQfbmjhFxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los algoritmos utilizados en esta competencia fueron 4; *RandomForest*, *SVC*, *MLP* y *XGBoostClassifier*. A continuaci贸n se describir谩 brevemente como funcionan.\n",
        "\n",
        "*   **Random Forest (RF)**: este es un algoritmo que ensambla m煤ltiples 谩rboles de decisi贸n para poder realizar una predicci贸n. Utiliza la t茅cnica de emsamblaje denominada *bagging* que consiste en tomar muestras aleatorias del conjunto de datos de entrenamiento, y entrena un 谩rbol para cada conjunto, luego cada uno de ellos realiza una predicci贸n y luego se toman todas las predicciones de todos los 谩rboles, se agregan de alguna manera, ya sea con el promedio (regresi贸n), moda, entre otras, y este valor es el output del RF.\n",
        "*   **Support Vector Machine (SVC)**: este es un algoritmo que busca separar de mejor manera las distintas clases del conjunto de datos. Para 茅sto, busca un hiperplano que m谩ximice el margen entre los vectores de soporte de cada clase. Un vector de soporte es el punto que est谩 mas cerca del hiperplano y se tienen  por cada clase. Entonces el problema de clasificaci贸n se traduce a un problema de optimizaci贸n en donde se busca encontrar el hiperplano que maximiza el margen. Como normalmente los datos no son perfectamente separables, el algoritmo se puede relajar para encontrar un hiperplano que no separe perfectamente las clases, sino que admita algunos datos mal clasificados a cambio de buscar un hiperplano que se ajuste completamente a los datos.\n",
        "*  **Multi Layer Perceptron (MLP)**: este algoritmo es una red neuronal que posee m煤ltiples capas ocultas entre el input y el output. Entre cada capa hay par谩metros, denominados pesos, que se van ajustando a medida que se va entrenando el modelo. Se necesita definir una funci贸n de p茅rdida que se buscar minimizar a trav茅s de algoritmos de optimizaci贸n. La manera m谩s conocida de entrenar un red es utilizando descenso de gradiente (en sus distintas versiones) en donde lo que se busca son los gradientes de la funci贸n de p茅rdida, que depende de los par谩metros del modelo, para poder ir bajando en la direcci贸n contraria al gradiente, de manera de buscar un m铆nimo. La gracia de estos modelos es que son muy vers谩tiles y pueden aproximar a pr谩cticamente cualquier funci贸n si se poseen muchos datos. Est谩 basado en el Perceptr贸n que vendr铆a a ser 1 neurona en la red, la cual recibe un input, le aplica una funci贸n (a definir) y retorna un output.\n",
        "*  **Extreme Gradient Boosting (XGBoost)**: al igual que el RF es un algoritmo que ensambla distintos 谩rboles de decisi贸n, pero no usando el *bagging* como en RF, sino que usando la t茅cnica del *boosting* que consiste en construir varios modelos *weak* e ir potenciando los siguientes modelos en serie pas谩ndoles como input el resultado obtenido. Luego lo que ocurre es que se parte con un 谩rbol, se obtiene una predicci贸n, esta predicci贸n se guarda y aparte se le entrega como input al siguiente 谩rbol, entonces 茅ste vuelve a generar una predicci贸n, tambi茅n la guarda y se la pasa al siguiente 谩rbol. Por lo tanto, al final voy a tener tantas predicciones como 谩rboles tenga y 茅stas se agregan para tomar una predicci贸n final.\n",
        "\n"
      ],
      "metadata": {
        "id": "Ro1gCL_gu1H8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:52.064631Z",
          "start_time": "2020-03-31T13:47:52.044451Z"
        },
        "id": "ECjkdgdwzvIq"
      },
      "source": [
        "## **4. M茅tricas de Evaluaci贸n**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\t**M茅tricas de evaluaci贸n**: Describir brevemente las m茅tricas utilizadas en la evaluaci贸n, indicando qu茅 miden y su interpretaci贸n. (**0.5 puntos**)\n",
        "\n",
        "- **AUC**: el AUC viene de *area under the curve* cuya curva es la ROC *Receiver Operator Characteristic*, que relaciona el Tasa de verdaderos positivos (TPR) y la tasa de falsos positivos (FPR) para distintos valores de *threshold* (es el umbral de decisi贸n que se utiliza para determinar si un ejemplo es de una clase u otra). La curva ROC evidencia la capacidad del modelo de separar las clases, y al tomar el 谩rea de 茅sta se genera una m茅trica que eval煤a que tan bueno es el clasificador separando las clases. Cuando el AUC es 1, entonces el clasificador separa perfectamente las clases, cuando es 0 el clasificar entonces est谩 identificando inversamente a las clases, de manera perfecta, y cuando es 0.5 implica que no tiene capacidad para reconocer la diferencia entre clases.\n",
        "- **Kappa**: es una metrica que evalua qu茅 tan \"de acuerdo\" est谩n las predicciones del clasificador con las etiquetas reales. Se basa en la matriz de confusi贸n, al igual que el Accuracy y el AUC, y en base a esta calcula la probabilidad en que est谩n de acuerdo (Accuracy), la probabilidad en que no est谩n de acuerdo (todos los FN y los FP) y las suma (P_desacuerdo), y con estas dos probabilidades calcula la m茅trica haciendo ((P_acuerdo) - (P_desacuerdo)) / (1- (P_desacuerdo)). Esta m茅trica es m谩s informativa cuando tenemos desbalance de clases.\n",
        "- **Accuracy**: el *Accuracy* calcula cuantas predicciones correctas hizo el modelo sobre el total de ejemplos en que se evalu贸. Indica que tanto le achunta a la etiqueta real, pero deja invisibilizado el problema de desbalance de clase, lo cual es un problema, y por lo mismo se debe mirar en conjunto con otras m茅tricas.\n"
      ],
      "metadata": {
        "id": "hFPbQPb1hJ-P"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJyTrr2onLOo"
      },
      "source": [
        "## **5. Dise帽o experimental**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La metodolog铆a propuesta para la construcci贸n de nuestro clasificador es la prueba-error de diferentes configuraciones (las cuales involucran tanto el pre procesamiento de la data como la b煤squeda de los mejores hiperpar谩metros del clasificador) con tal de maximizar el `accuracy` del modelo. Bajo esta premisa, notamos que las posibilidades a explorar son mucho mas grandes en el preprocesamiento que en el clasificador mismo. Por lo mismo, la mayor parte de nuestro trabajo se dedica a probar diferentes configuraciones del procesamiento de los datos (es decir, diferentes combinaciones de como llega la data al clasificador) y solo al final buscar el clasificador 贸ptimo para el sentimiento.\n",
        "\n",
        "Tomando esto en cuenta, nuestros experimentos se dividen en las siguientes etapas:\n",
        "\n",
        "1. B煤squeda de la configuraci贸n 贸ptima (pre procesamiento):\n",
        " - Bag of Words\n",
        " - TF-IDF\n",
        " - Embeddings preentrenados\n",
        " - Embeddings entrenados con la data\n",
        "\n",
        "2. B煤squeda de clasificador por GridSearch\n",
        "\n",
        "3. Entrenamiento final\n",
        "\n",
        "A continuaci贸n, se detalla cada una de las etapas:\n",
        "\n",
        "1. B煤squeda de la configuraci贸n 贸ptima: Para encontrar la mejor configuraci贸n de pre procesamiento, se comenz贸 dividiendo las configuraciones en 4 grandes metodolog铆as de vectorizaci贸n: (i) Bag of Words, (ii) TF-IDF, (iii) Embeddings preentrenados (*transfer learning*) y (iv) Embeddings entrenados con la data. Desde cada una, se prueban diferentes sub-configuraciones que abarcan: generaci贸n de features usando lexicones, normalizaci贸n de tokens, reducci贸n de dimensionalidad mediante PCA, resampling (undersampling, oversampling o la combinaci贸n de ambos), prueba de diferentes largos de vectores de embeddings, utilizaci贸n de frameworks externos como `awessome`, entre otros. Como resultado, se espera tener una configuraci贸n de pre procesamiento 贸ptima para cada sentimiento, donde la mejor configuraci贸n se eligir谩 en torno a la m茅trica `accuracy` usando un mismo clasificador y una semilla aleatoria para comparabilidad.\n",
        "\n",
        "2. B煤squeda de clasificador: Teniendo la configuraci贸n 贸ptima fija, se busca optimizar el poder de predicci贸n haciendo una b煤squeda en grilla de los mejores hiperpar谩metros de los clasificadores, de nuevo optimizando la m茅trica `accuracy`. Un punto importante es que, como ya se encont贸 una configuraci贸n 贸ptima para cada sentimiento, se \"precargan\" estas configuraciones antes de buscar la grilla con tal de reducir los tiempos de entrenamiento (en otras palabras, el objeto `HalvingGridSearch` toma solo la capa del clasificador y no la capa de preprocesamiento). A grandes rasgos, se prueban 3 algoritmos e hiperpar谩metros:\n",
        "\n",
        "- `Random Forest`:\n",
        "  - `n_estimators`: `[100, 500, 1000]`\n",
        "  - `max_depth`: `[80, 90, 100, 110]`\n",
        "  - `max_features`: `[2, 3]`\n",
        "  - `min_samples_leaf`: `[3, 4, 5]`\n",
        "  - `min_samples_split`: `[8, 10, 12]`\n",
        "\n",
        "- `XGBoost`:\n",
        "  - `max_depth`: `[3, 4, 5, 7]`\n",
        "  - `learning_rate`: `[0.1, 0.01, 0.05]`\n",
        "  - `gamma`: `[0, 0.25, 1]`\n",
        "  - `reg_lambda`: `[0, 1, 10]`\n",
        "  - `scale_pos_weight`: `[1, 3, 5]`\n",
        "\n",
        "- `SVM`\n",
        "  - `C`: `[1, 10, 100, 1000]`\n",
        "  - `kernel`: `['linear', 'rbf', 'poly']`\n",
        "  - `gamma`: `[0.001, 0.0001]`\n",
        "\n",
        "3. Entrenamiento final: Ya habiendo encontrado el modelo 贸ptimo por sentimiento (tanto a nivel de pre procesamiento como en el clasificador), se entrena nuevamente este modelo pero esta vez con toda la data disponible (sin divisi贸n train-test) para finalmente generar una predicci贸n en el conjunto de test para subirlo a la plataforma **Codalab**."
      ],
      "metadata": {
        "id": "LK2W7bv6hZQz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Experimentos**"
      ],
      "metadata": {
        "id": "oyj1fwJehpc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Librer铆as y funciones transversales**"
      ],
      "metadata": {
        "id": "iQTW6HheMrYg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Librer铆as\n",
        "\n",
        "En esta secci贸n se presentan e importan las librer铆as requeridas para el trabajo"
      ],
      "metadata": {
        "id": "n9OwU62gMza7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "!pip install nltk\n",
        "import nltk\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk import word_tokenize \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_selection import SelectPercentile, f_classif\n",
        "nltk.download('opinion_lexicon')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop_words = stopwords.words('english')\n",
        "from nltk.corpus import opinion_lexicon\n",
        "import re\n",
        "import imblearn\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from time import time  \n",
        "from collections import defaultdict \n",
        "import string \n",
        "import multiprocessing\n",
        "import os\n",
        "import requests\n",
        "\n",
        "!pip install vaderSentiment\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.experimental import enable_halving_search_cv # noqa\n",
        "from sklearn.model_selection import HalvingGridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "import xgboost as xgb\n",
        "from sklearn import svm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "\n",
        "!pip install umap-learn\n",
        "from umap import UMAP\n",
        "\n",
        "!pip install awessome\n",
        "from awessome.awessome_builder import *\n",
        "\n",
        "# awessome model\n",
        "builder = SentimentIntensityScorerBuilder('avg', 'bert-base-nli-mean-tokens', 'cosine', '600', True)\n",
        "scorer_vader = builder.build_scorer_from_prebuilt_lexicon('vader')\n",
        "scorer_labmt = builder.build_scorer_from_prebuilt_lexicon('labmt')\n",
        "\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import Word2Vec, KeyedVectors\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
        "\n",
        "!pip install emosent-py\n",
        "from emosent import get_emoji_sentiment_rank\n",
        "\n",
        "!pip install emoji\n",
        "import emoji\n",
        "from emoji.core import emoji_count"
      ],
      "metadata": {
        "id": "yPNK2WnhNmNU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0285c231-c0e0-43e8-a177-4388f9b3d900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     || 125 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vaderSentiment) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vaderSentiment) (2.10)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[K     || 88 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Collecting pynndescent>=0.5\n",
            "  Downloading pynndescent-0.5.6.tar.gz (1.1 MB)\n",
            "\u001b[K     || 1.1 MB 39.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.64.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82829 sha256=85cb67efb83f5183461777965aeb7d4eafd76114c6f17b49f784cfa2ebbd988d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/52/a5/1fd9e3e76a7ab34f134c07469cd6f16e27ef3a37aeff1fe821\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.6-py3-none-any.whl size=53943 sha256=67b89615071564d3278f7e56ea79a558a8378ab37ba4e17522358401643c7ad5\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f1/56/f80d72741e400345b5a5b50ec3d929aca581bf45e0225d5c50\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.6 umap-learn-0.5.3\n",
            "Collecting awessome\n",
            "  Downloading awessome-0.0.14-py2.py3-none-any.whl (34 kB)\n",
            "Collecting joblib==0.17.0\n",
            "  Downloading joblib-0.17.0-py3-none-any.whl (301 kB)\n",
            "\u001b[K     || 301 kB 8.0 MB/s \n",
            "\u001b[?25hCollecting urllib3==1.25.11\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     || 127 kB 47.3 MB/s \n",
            "\u001b[?25hCollecting tqdm==4.51.0\n",
            "  Downloading tqdm-4.51.0-py2.py3-none-any.whl (70 kB)\n",
            "\u001b[K     || 70 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting dataclasses==0.6\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Collecting regex==2020.10.23\n",
            "  Downloading regex-2020.10.23-cp37-cp37m-manylinux2010_x86_64.whl (662 kB)\n",
            "\u001b[K     || 662 kB 53.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: click==7.1.2 in /usr/local/lib/python3.7/dist-packages (from awessome) (7.1.2)\n",
            "Collecting packaging==20.4\n",
            "  Downloading packaging-20.4-py2.py3-none-any.whl (37 kB)\n",
            "Collecting nltk==3.5\n",
            "  Downloading nltk-3.5.zip (1.4 MB)\n",
            "\u001b[K     || 1.4 MB 37.3 MB/s \n",
            "\u001b[?25hCollecting torch==1.7.0\n",
            "  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "\u001b[K     || 776.7 MB 4.2 kB/s \n",
            "\u001b[?25hCollecting filelock==3.0.12\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Collecting typing-extensions==3.7.4.3\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting threadpoolctl==2.1.0\n",
            "  Downloading threadpoolctl-2.1.0-py3-none-any.whl (12 kB)\n",
            "Collecting scikit-learn==0.23.2\n",
            "  Downloading scikit_learn-0.23.2-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     || 6.8 MB 40.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     || 3.0 MB 47.0 MB/s \n",
            "\u001b[?25hCollecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp37-cp37m-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[K     || 14.5 MB 23.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.7/dist-packages (from awessome) (3.0.4)\n",
            "Collecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     || 67 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting scipy==1.5.3\n",
            "  Downloading scipy-1.5.3-cp37-cp37m-manylinux1_x86_64.whl (25.9 MB)\n",
            "\u001b[K     || 25.9 MB 54.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from awessome) (2.10)\n",
            "Collecting sentence-transformers==0.3.8\n",
            "  Downloading sentence-transformers-0.3.8.tar.gz (66 kB)\n",
            "\u001b[K     || 66 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting certifi==2020.6.20\n",
            "  Downloading certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     || 156 kB 68.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six==1.15.0 in /usr/local/lib/python3.7/dist-packages (from awessome) (1.15.0)\n",
            "Collecting transformers==3.3.1\n",
            "  Downloading transformers-3.3.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[K     || 1.1 MB 36.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.94\n",
            "  Downloading sentencepiece-0.1.94-cp37-cp37m-manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     || 1.1 MB 43.6 MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.43\n",
            "  Downloading sacremoses-0.0.43.tar.gz (883 kB)\n",
            "\u001b[K     || 883 kB 47.6 MB/s \n",
            "\u001b[?25hCollecting requests==2.24.0\n",
            "  Downloading requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     || 61 kB 307 kB/s \n",
            "\u001b[?25hCollecting future==0.18.2\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     || 829 kB 73.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: future, nltk, sacremoses, sentence-transformers\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=5db5653a69cdbd7517ac1f1a7e43a497a1d872930fea435a2abaafc06c105e6b\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434693 sha256=77829347da6169dc08f3d1bd934bb25aa2bbd3462f8603edba1fc0c9a7ce67da\n",
            "  Stored in directory: /root/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-py3-none-any.whl size=893249 sha256=c8f0123126ef3528ac8665d5edd34b8ba97e66b8b9534e67a62684873dfdeebf\n",
            "  Stored in directory: /root/.cache/pip/wheels/69/09/d1/bf058f7d6fa0ecba2ce7c66be3b8d012beb4bf61a6e0c101c0\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-0.3.8-py3-none-any.whl size=101994 sha256=0441a3bbe506d44d07d106b8eabc2505b630128922729901c489d580ebefb3fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/43/65/fe0f3ea9327623e749a79eb5dfad85a809c84064b1cc4682c1\n",
            "Successfully built future nltk sacremoses sentence-transformers\n",
            "Installing collected packages: urllib3, tqdm, regex, pyparsing, numpy, joblib, certifi, typing-extensions, tokenizers, threadpoolctl, sentencepiece, scipy, sacremoses, requests, packaging, future, filelock, dataclasses, transformers, torch, scikit-learn, nltk, sentence-transformers, awessome\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.64.0\n",
            "    Uninstalling tqdm-4.64.0:\n",
            "      Successfully uninstalled tqdm-4.64.0\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.8\n",
            "    Uninstalling pyparsing-3.0.8:\n",
            "      Successfully uninstalled pyparsing-3.0.8\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2021.10.8\n",
            "    Uninstalling certifi-2021.10.8:\n",
            "      Successfully uninstalled certifi-2021.10.8\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 4.1.1\n",
            "    Uninstalling typing-extensions-4.1.1:\n",
            "      Successfully uninstalled typing-extensions-4.1.1\n",
            "  Attempting uninstall: threadpoolctl\n",
            "    Found existing installation: threadpoolctl 3.1.0\n",
            "    Uninstalling threadpoolctl-3.1.0:\n",
            "      Successfully uninstalled threadpoolctl-3.1.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 21.3\n",
            "    Uninstalling packaging-21.3:\n",
            "      Successfully uninstalled packaging-21.3\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.6.0\n",
            "    Uninstalling filelock-3.6.0:\n",
            "      Successfully uninstalled filelock-3.6.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.0.2\n",
            "    Uninstalling scikit-learn-1.0.2:\n",
            "      Successfully uninstalled scikit-learn-1.0.2\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "torchvision 0.11.1+cu111 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\n",
            "tensorflow 2.8.0 requires numpy>=1.20, but you have numpy 1.19.2 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.23.2 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.24.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed awessome-0.0.14 certifi-2020.6.20 dataclasses-0.6 filelock-3.0.12 future-0.18.2 joblib-0.17.0 nltk-3.5 numpy-1.19.2 packaging-20.4 pyparsing-2.4.7 regex-2020.10.23 requests-2.24.0 sacremoses-0.0.43 scikit-learn-0.23.2 scipy-1.5.3 sentence-transformers-0.3.8 sentencepiece-0.1.94 threadpoolctl-2.1.0 tokenizers-0.8.1rc2 torch-1.7.0 tqdm-4.51.0 transformers-3.3.1 typing-extensions-3.7.4.3 urllib3-1.25.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "dataclasses",
                  "joblib",
                  "nltk",
                  "numpy",
                  "packaging",
                  "pyparsing",
                  "regex",
                  "requests",
                  "scipy",
                  "sklearn",
                  "tqdm",
                  "typing_extensions",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 405M/405M [00:18<00:00, 21.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emosent-py\n",
            "  Downloading emosent-py-0.1.6.tar.gz (28 kB)\n",
            "Building wheels for collected packages: emosent-py\n",
            "  Building wheel for emosent-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emosent-py: filename=emosent_py-0.1.6-py3-none-any.whl size=28502 sha256=a20134b3ca9bb1c5d2db86c7c80fd3db04f37acffe89a103cee37f78c74ec141\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/37/bd/b4e67490f36c4beb85a1047d6cd13a356ffecbfa854eaf4688\n",
            "Successfully built emosent-py\n",
            "Installing collected packages: emosent-py\n",
            "Successfully installed emosent-py-0.1.6\n",
            "Collecting emoji\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[K     || 175 kB 5.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: emoji\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171046 sha256=85a447441bb63caa79751ab3a87147813c0dd0338b7cce7c2ed81a9b81af8d21\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/4e/b6/57b01db010d17ef6ea9b40300af725ef3e210cb1acfb7ac8b6\n",
            "Successfully built emoji\n",
            "Installing collected packages: emoji\n",
            "Successfully installed emoji-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Funciones de ejecuci贸n pipelines para BoW, Tf-Idf y Embeddings preentrenados"
      ],
      "metadata": {
        "id": "QMhaoJ9Klv1G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resample(X, y, param_over = False, param_under = False):\n",
        "\n",
        "    '''\n",
        "    Funci贸n que recibe dataframes de entrada y los devuelve resampleados.\n",
        "    X: dataframe de entrada (X_train)\n",
        "    y: dataframe de entrada (y_train)\n",
        "    param_over: factor por el que se har谩 oversampling\n",
        "    param_under: factor por el que se har谩 subsampling\n",
        "    '''\n",
        "\n",
        "    # Oversampling\n",
        "    if param_over:\n",
        "      count_values = y.value_counts()\n",
        "      dict_over = {key: int(count_values[key] * (1 + param_over)) for key in count_values.keys()[1:]}\n",
        "\n",
        "      if X.ndim == 1:\n",
        "        X = np.array(X).reshape(-1, 1)\n",
        "      oversampler = RandomOverSampler(sampling_strategy = dict_over, random_state = 3380)\n",
        "      X, y = oversampler.fit_resample(X, y)\n",
        "      X = X.reshape(-1, )\n",
        "\n",
        "    # Subsampling\n",
        "    if param_under:\n",
        "      count_values = y.value_counts()\n",
        "      dict_under = {count_values.keys()[0]: int(count_values[0] * param_under)}\n",
        "      if X.ndim == 1:\n",
        "        X = np.array(X).reshape(-1, 1)\n",
        "\n",
        "      undersampler = RandomUnderSampler(sampling_strategy = dict_under, random_state = 3380)\n",
        "      X, y = undersampler.fit_resample(X, y)\n",
        "      X = X.reshape(-1, )\n",
        "\n",
        "    return X, y"
      ],
      "metadata": {
        "id": "GN6v4HHyN6VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(dataset, dataset_name, pipeline, param_over = False, param_under = False):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test, a煤n no se transforma de Strings a valores num茅ricos.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataset.tweet,\n",
        "        dataset.sentiment_intensity,\n",
        "        shuffle=True,\n",
        "        test_size=0.33,\n",
        "        random_state = 3380)\n",
        "\n",
        "    #Balanceo de clases\n",
        "    X_train, y_train = resample(X_train, y_train, param_over, param_under)\n",
        "\n",
        "    print(f'# Datos de entrenamiento en dataset {dataset_name}: {len(X_train)}')\n",
        "    print(f'# Datos de testing en dataset {dataset_name}: {len(X_test)}')\n",
        "\n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline). \n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
        "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "    \n",
        "    # Evaluamos:\n",
        "    scores = evaluate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
        "    return pipeline, learned_labels, scores"
      ],
      "metadata": {
        "id": "teX64Gg4nzIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(pipeline, param_over = False, param_under = False):\n",
        "  classifiers = []\n",
        "  learned_labels_array = []\n",
        "  scores_array = []\n",
        "\n",
        "  # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "  for dataset_name, dataset in train.items():\n",
        "    \n",
        "    # creamos el pipeline\n",
        "    pipeline = pipeline\n",
        "    \n",
        "    # ejecutamos el pipeline sobre el dataset\n",
        "    classifier, learned_labels, scores = run(dataset, dataset_name, pipeline, param_over, param_under)\n",
        "\n",
        "    # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
        "    classifiers.append(classifier)\n",
        "\n",
        "    # guardamos las labels aprendidas por el clasificador\n",
        "    learned_labels_array.append(learned_labels)\n",
        "\n",
        "    # guardamos los scores obtenidos\n",
        "    scores_array.append(scores)\n",
        "\n",
        "  # print avg scores\n",
        "  print(\n",
        "    \"Average scores:\\n\\n\",\n",
        "    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
        "    .format(*np.array(scores_array).mean(axis=0)))\n",
        "  \n",
        "  return classifiers, learned_labels_array, scores_array"
      ],
      "metadata": {
        "id": "Yjw1WaO1oFYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FevBPus0zvIs"
      },
      "source": [
        "#### Definir m茅todos de evaluaci贸n (**NO tocar este c贸digo**)\n",
        "\n",
        "Estas funciones est谩n a cargo de evaluar los resultados de la tarea. No deber铆an cambiarlas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.604066Z",
          "start_time": "2020-04-07T15:44:20.589106Z"
        },
        "id": "9wlllV7PzvIs"
      },
      "source": [
        "def auc_score(test_set, predicted_set):\n",
        "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
        "    medium_predicted = np.array(\n",
        "        [prediction[1] for prediction in predicted_set])\n",
        "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
        "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
        "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
        "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
        "    auc_high = roc_auc_score(high_test, high_predicted)\n",
        "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
        "    auc_low = roc_auc_score(low_test, low_predicted)\n",
        "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
        "             high_test.sum() * auc_high) / (\n",
        "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
        "    return auc_w\n",
        "\n",
        "\n",
        "def evaluate(predicted_probabilities, y_test, labels, dataset_name):\n",
        "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
        "    # entregar el arreglo de clases aprendido por el clasificador.\n",
        "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
        "    predicted_labels = [\n",
        "        labels[np.argmax(item)] for item in predicted_probabilities\n",
        "    ]\n",
        "\n",
        "    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n",
        "    print(\n",
        "        confusion_matrix(y_test,\n",
        "                         predicted_labels,\n",
        "                         labels=['low', 'medium', 'high']))\n",
        "\n",
        "    print('\\nClassification Report:\\n')\n",
        "    print(\n",
        "        classification_report(y_test,\n",
        "                              predicted_labels,\n",
        "                              labels=['low', 'medium', 'high']))\n",
        "    # Reorder predicted probabilities array.\n",
        "    labels = labels.tolist()\n",
        "    \n",
        "    predicted_probabilities = predicted_probabilities[:, [\n",
        "        labels.index('low'),\n",
        "        labels.index('medium'),\n",
        "        labels.index('high')\n",
        "    ]]\n",
        "    \n",
        "    \n",
        "    auc = round(auc_score(y_test, predicted_probabilities), 3)\n",
        "    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n",
        "    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n",
        "    print(\"Kappa:\", kappa, end='\\t')\n",
        "    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print('------------------------------------------------------\\n')\n",
        "    return np.array([auc, kappa, accuracy])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkOP6ugwzvIt"
      },
      "source": [
        "#### Datos\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.068137Z",
          "start_time": "2020-04-07T15:44:20.606061Z"
        },
        "id": "D1XhFPhrzvIt"
      },
      "source": [
        "# Datasets de entrenamiento.\n",
        "train = {\n",
        "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
        "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
        "}\n",
        "# Datasets que deber谩n predecir para la competencia.\n",
        "target = {\n",
        "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
        "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.088707Z",
          "start_time": "2020-04-07T15:44:21.069757Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "flg2Zw2mzvIt",
        "outputId": "8acd4a57-7c38-4507-cd1e-a2620df5eda9"
      },
      "source": [
        "# Ejemplo de algunas filas aleatorias del dataset etiquetado:\n",
        "train['anger'].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                              tweet  class  \\\n",
              "829  10829  follow my girl tiff she only got 3 followers...  anger   \n",
              "249  10249  You will never find someone who loved you like...  anger   \n",
              "280  10280  the outrage has passed and has been replaced b...  anger   \n",
              "166  10166  @SkyUK what a joke!! Cut our internet off earl...  anger   \n",
              "552  10552  Realizing that holding a grudge for long is im...  anger   \n",
              "\n",
              "    sentiment_intensity  \n",
              "829                 low  \n",
              "249              medium  \n",
              "280              medium  \n",
              "166              medium  \n",
              "552              medium  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ffbb2d0e-534f-4aa9-8bfe-0dd736b4c7be\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>class</th>\n",
              "      <th>sentiment_intensity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>829</th>\n",
              "      <td>10829</td>\n",
              "      <td>follow my girl tiff she only got 3 followers...</td>\n",
              "      <td>anger</td>\n",
              "      <td>low</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>10249</td>\n",
              "      <td>You will never find someone who loved you like...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>280</th>\n",
              "      <td>10280</td>\n",
              "      <td>the outrage has passed and has been replaced b...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>10166</td>\n",
              "      <td>@SkyUK what a joke!! Cut our internet off earl...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552</th>\n",
              "      <td>10552</td>\n",
              "      <td>Realizing that holding a grudge for long is im...</td>\n",
              "      <td>anger</td>\n",
              "      <td>medium</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ffbb2d0e-534f-4aa9-8bfe-0dd736b4c7be')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ffbb2d0e-534f-4aa9-8bfe-0dd736b4c7be button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ffbb2d0e-534f-4aa9-8bfe-0dd736b4c7be');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XB7hb7KH2DFK",
        "outputId": "32127426-6292-4809-eeed-1e23c985cce9"
      },
      "source": [
        "# Ejemplo de algunas filas aleatorias del dataset no etiquetado\n",
        "target['anger'].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id                                              tweet  class  \\\n",
              "528  11469  @SMalloy_LWOS that's not easy to blow up the L...  anger   \n",
              "424  11365  There are adults commiting serious crimes in t...  anger   \n",
              "37   10978  @TheRevAl please tell us why 'protesting' inju...  anger   \n",
              "297  11238  #GBBO is such a homely pure piece of tv gold. ...  anger   \n",
              "10   10951       @shae_caitlin ur road rage gives me anxiety.  anger   \n",
              "\n",
              "     sentiment_intensity  \n",
              "528                  NaN  \n",
              "424                  NaN  \n",
              "37                   NaN  \n",
              "297                  NaN  \n",
              "10                   NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f3a137b5-187c-47e7-9ca2-8a8fe10559fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>tweet</th>\n",
              "      <th>class</th>\n",
              "      <th>sentiment_intensity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>528</th>\n",
              "      <td>11469</td>\n",
              "      <td>@SMalloy_LWOS that's not easy to blow up the L...</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>424</th>\n",
              "      <td>11365</td>\n",
              "      <td>There are adults commiting serious crimes in t...</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>10978</td>\n",
              "      <td>@TheRevAl please tell us why 'protesting' inju...</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>297</th>\n",
              "      <td>11238</td>\n",
              "      <td>#GBBO is such a homely pure piece of tv gold. ...</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10951</td>\n",
              "      <td>@shae_caitlin ur road rage gives me anxiety.</td>\n",
              "      <td>anger</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f3a137b5-187c-47e7-9ca2-8a8fe10559fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f3a137b5-187c-47e7-9ca2-8a8fe10559fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f3a137b5-187c-47e7-9ca2-8a8fe10559fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Objeto FeaturesTransform para Tf-Idf y Embeddings preentrenados"
      ],
      "metadata": {
        "id": "VCnGm_Xkw4MD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este objeto preprocesa los tweets y extrae diversas caracter铆sticas. Esta ser谩 utilizada en Tf-Idf y Embeddings preentrenados."
      ],
      "metadata": {
        "id": "lq_hLFpT01op"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload() #para cargar archivo \"Emoji Sentiment Ranking 1.0.txt\"  (est谩 en el drive, podr铆amos cargarlo desde ah铆 pero no s茅 q tan c贸modo sea pa revisar... c analiza)"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "4tvs7Qxn0roD",
        "outputId": "86ac7d25-116a-4ed4-8d6c-521e0c855203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-334370d1-ef2c-4862-976b-90c0d5331b41\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-334370d1-ef2c-4862-976b-90c0d5331b41\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Emoji Sentiment Ranking 1.0.txt to Emoji Sentiment Ranking 1.0.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lee archivo y normaliza rankings\n",
        "listaemojis = pd.read_csv(\"Emoji Sentiment Ranking 1.0.txt\")\n",
        "listaemojis=listaemojis[listaemojis.Occurrences>5]\n",
        "listaemojis.Negative=listaemojis.Negative/listaemojis.Occurrences\n",
        "listaemojis.Positive=listaemojis.Positive/listaemojis.Occurrences\n",
        "listaemojis.Neutral=listaemojis.Neutral/listaemojis.Occurrences\n",
        "listaemojis.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "4F5b0Li-00qZ",
        "outputId": "7f0e79b1-e057-4b94-9ec7-0a477405c790"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  Emoji Unicode codepoint  Occurrences  Position  Negative   Neutral  \\\n",
              "0                0x1f602        14622  0.805101  0.247162  0.284708   \n",
              "1                 0x2764         8050  0.746943  0.044099  0.165714   \n",
              "2                 0x2665         7144  0.753806  0.035274  0.271837   \n",
              "3                0x1f60d         6359  0.765292  0.051738  0.218588   \n",
              "4                0x1f62d         5526  0.803352  0.436482  0.220413   \n",
              "\n",
              "   Positive                         Unicode name          Unicode block  \n",
              "0  0.468130               FACE WITH TEARS OF JOY              Emoticons  \n",
              "1  0.790186                    HEAVY BLACK HEART               Dingbats  \n",
              "2  0.692889                     BLACK HEART SUIT  Miscellaneous Symbols  \n",
              "3  0.729674  SMILING FACE WITH HEART-SHAPED EYES              Emoticons  \n",
              "4  0.343105                   LOUDLY CRYING FACE              Emoticons  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ff4fd901-3f2c-44e9-b067-62c1512d0798\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Emoji</th>\n",
              "      <th>Unicode codepoint</th>\n",
              "      <th>Occurrences</th>\n",
              "      <th>Position</th>\n",
              "      <th>Negative</th>\n",
              "      <th>Neutral</th>\n",
              "      <th>Positive</th>\n",
              "      <th>Unicode name</th>\n",
              "      <th>Unicode block</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>0x1f602</td>\n",
              "      <td>14622</td>\n",
              "      <td>0.805101</td>\n",
              "      <td>0.247162</td>\n",
              "      <td>0.284708</td>\n",
              "      <td>0.468130</td>\n",
              "      <td>FACE WITH TEARS OF JOY</td>\n",
              "      <td>Emoticons</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td></td>\n",
              "      <td>0x2764</td>\n",
              "      <td>8050</td>\n",
              "      <td>0.746943</td>\n",
              "      <td>0.044099</td>\n",
              "      <td>0.165714</td>\n",
              "      <td>0.790186</td>\n",
              "      <td>HEAVY BLACK HEART</td>\n",
              "      <td>Dingbats</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td></td>\n",
              "      <td>0x2665</td>\n",
              "      <td>7144</td>\n",
              "      <td>0.753806</td>\n",
              "      <td>0.035274</td>\n",
              "      <td>0.271837</td>\n",
              "      <td>0.692889</td>\n",
              "      <td>BLACK HEART SUIT</td>\n",
              "      <td>Miscellaneous Symbols</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td></td>\n",
              "      <td>0x1f60d</td>\n",
              "      <td>6359</td>\n",
              "      <td>0.765292</td>\n",
              "      <td>0.051738</td>\n",
              "      <td>0.218588</td>\n",
              "      <td>0.729674</td>\n",
              "      <td>SMILING FACE WITH HEART-SHAPED EYES</td>\n",
              "      <td>Emoticons</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td></td>\n",
              "      <td>0x1f62d</td>\n",
              "      <td>5526</td>\n",
              "      <td>0.803352</td>\n",
              "      <td>0.436482</td>\n",
              "      <td>0.220413</td>\n",
              "      <td>0.343105</td>\n",
              "      <td>LOUDLY CRYING FACE</td>\n",
              "      <td>Emoticons</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff4fd901-3f2c-44e9-b067-62c1512d0798')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff4fd901-3f2c-44e9-b067-62c1512d0798 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff4fd901-3f2c-44e9-b067-62c1512d0798');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El framework `awessome` es una herramienta creada por [Amal Htait](http://amalhtait.com) la cual utiliza BERT en conjunto con los lexicones `vader` y `labmt` para generar un score de intensidad de sentimiento para una frase. Como esta feature se adecua mucho al problema, se implementa al listado de features."
      ],
      "metadata": {
        "id": "eCiybUpDHapy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# awessome model\n",
        "builder = SentimentIntensityScorerBuilder('avg', 'bert-base-nli-mean-tokens', 'cosine', '600', True)\n",
        "scorer_vader = builder.build_scorer_from_prebuilt_lexicon('vader')\n",
        "scorer_labmt = builder.build_scorer_from_prebuilt_lexicon('labmt')"
      ],
      "metadata": {
        "id": "JXysHIWo1Iuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeaturesTransformer(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.pos_set, self.neg_set = set(opinion_lexicon.positive()), set(opinion_lexicon.negative()) #lexicon\n",
        "        self.tokenizer = TweetTokenizer(preserve_case = False) #tokenizador a usar, NO preservamos las mayusculas\n",
        "        self.NEG = r\"(?:^(?:no|not)$)|n't\" #reconoce palabras negativas\n",
        "        self.analyzer = SentimentIntensityAnalyzer() # lexicon vader\n",
        "        self.scorer_vader, self.scorer_labmt = scorer_vader, scorer_labmt #awessome models\n",
        "\n",
        "    def liu_score(self, tokenized_sent: list):\n",
        "\n",
        "        '''\n",
        "        Funci贸n que retorna la cantidad de tokens positivos y negativos dado un lexicon\n",
        "        tokenized_sent: lista con los tokens\n",
        "        '''\n",
        "\n",
        "        pos_words, neg_words = 0, 0\n",
        "        for word in tokenized_sent:\n",
        "            if word.lower() in self.pos_set:\n",
        "                pos_words += 1\n",
        "            elif word.lower() in self.neg_set:\n",
        "                neg_words += 1\n",
        "        return pos_words, neg_words\n",
        "\n",
        "    def count_sentiment_emoji(self, tweet: str):\n",
        "\n",
        "        '''\n",
        "        Funci贸n que retorna la cantidad de emojis positivos y negativos en un string (tweet).\n",
        "        tweet: string contenedor de palabras y emojis.\n",
        "        '''\n",
        "\n",
        "        count_emoji_neg = 0\n",
        "        count_emoji_pos = 0\n",
        "        list_emoji = emoji.distinct_emoji_list(tweet)\n",
        "        for i in range(len(list_emoji)):\n",
        "          emoticon = list_emoji[i]\n",
        "          if emoticon in list(listaemojis[\"Emoji\"]):\n",
        "            neg = listaemojis[listaemojis['Emoji']==emoticon].iloc[0, 4]\n",
        "            pos = listaemojis[listaemojis['Emoji']==emoticon].iloc[0, 6]\n",
        "            if neg > pos:\n",
        "              count_emoji_neg = count_emoji_neg + 1\n",
        "            else:\n",
        "              count_emoji_pos = count_emoji_pos + 1  \n",
        "              \n",
        "        return count_emoji_neg, count_emoji_pos \n",
        "\n",
        "    def pre_processing(self, tweet: str):\n",
        "\n",
        "        '''\n",
        "        Funci贸n que realiza un pre procesamiento al texto.\n",
        "        tweet: string con el texto a pre procesar\n",
        "        '''\n",
        "\n",
        "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet) #elimina urls\n",
        "        tweet = re.sub('(@[^\\s]+)|(w/)','', tweet) #elimina menciones\n",
        "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #le quita el # a los hashtags\n",
        "        tweet = re.sub('\\n','', tweet) #elimina saltos de l铆nea\n",
        "        return tweet\n",
        "\n",
        "    def get_relevant_chars(self, tweet: str):\n",
        "\n",
        "        '''\n",
        "        Funci贸n que, a partir de un documento, devuelve una serie de features a partir de sus car谩cteres y sus tokens\n",
        "        tweet: documento en formato de string\n",
        "        '''\n",
        "\n",
        "        #features de car谩cteres\n",
        "        num_hashtags = tweet.count('#') # cantidad de #\n",
        "        num_exclamations = tweet.count('!') # cantidad de !\n",
        "        num_interrogations = tweet.count('?') # cantidad de ?\n",
        "        num_at = tweet.count('@') #cantidad de @\n",
        "        num_asterisk = tweet.count('*')\n",
        "        num_capital = sum(1 for c in tweet if c.isupper()) # cantidad de caracteres en may煤scula\n",
        "        num_emoji = emoji_count(tweet) # cantidad de emojis en el tweet\n",
        "        num_emoji_neg, num_emoji_pos = self.count_sentiment_emoji(tweet) #cantidad de emojis positivos y negativos en el tweet\n",
        "\n",
        "        tweet = self.pre_processing(tweet)\n",
        "        vader_comp, vader_neg, vader_neu, vader_pos = self.analyzer.polarity_scores(tweet).values() # scores de lexicon vader\n",
        "        awessome_vader, awessome_labmt = self.scorer_vader.score_sentence(tweet), self.scorer_labmt.score_sentence(tweet) #scores generados por framework awessome\n",
        "\n",
        "        # features de tokens\n",
        "\n",
        "        tokens = self.tokenizer.tokenize(tweet) #tokenizamos tweet\n",
        "        pos_count, neg_count = self.liu_score(tokens) #cantidad de tokens positivas y negativas\n",
        "        num_upper = sum(map(str.isupper, tokens)) #cantidad de palabras en may煤scula\n",
        "        num_elongated = sum([1 for token in tokens if re.compile(r\"(.)\\1{2}\").search(token)]) # cantidad de palabras elongadas\n",
        "        num_sequence = sum([1 for token in tokens if re.compile(r\"(.+?)[\\.?!]{2,}\").search(token)]) #cantidad de secuencias de ., ? y ! con largo 2 o superior\n",
        "        num_negwords = sum([1 for token in tokens if re.compile(self.NEG, re.VERBOSE).search(token)]) #cantidad de negaciones\n",
        "\n",
        "        # output\n",
        "        features = [num_hashtags, num_exclamations, num_interrogations, num_at, num_asterisk, num_capital, pos_count, num_emoji, num_emoji_pos, num_emoji_neg,\n",
        "                    neg_count, num_upper, num_elongated, num_sequence, num_negwords, vader_comp, vader_neg, vader_neu, vader_pos,\n",
        "                    awessome_vader, awessome_labmt]\n",
        "        return features\n",
        "\n",
        "    def transform(self, X, y = None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(self.get_relevant_chars(tweet))\n",
        "\n",
        "        return np.array(chars)\n",
        "\n",
        "    def fit(self, X, y = None):\n",
        "        return self"
      ],
      "metadata": {
        "id": "td4R6poCxCRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Veamos que sucede si ejecutamos el transformer\n",
        "sample = train['sadness'].sample(69, random_state = 50).tweet\n",
        "sample_features = FeaturesTransformer().transform(sample)\n",
        "\n",
        "# Se puede verificar que el conteo de s铆mbolos es consistente con el transformer creado.\n",
        "print(f'Tweet original: {sample.iloc[0]}')\n",
        "print(f'Features creados: {sample_features[0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6_robZG1et9",
        "outputId": "b963b54f-68b0-4f49-a9c7-f7a8cefa1a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tweet original: @harowe this sad truth!\n",
            "Features creados: [ 0.          1.          0.          1.          0.          0.\n",
            "  0.          0.          0.          0.          1.          0.\n",
            "  0.          0.          0.          0.507       0.149       0.344\n",
            " -0.2714     -0.31166625 -0.16009769]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Bag of Words**"
      ],
      "metadata": {
        "id": "RHjyaxf0MlPX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Objetos"
      ],
      "metadata": {
        "id": "or1dgyvijaVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clase tokenizador con Stemming, usado en BOW (CountVectorizer)\n",
        "class StemmerTokenizer:\n",
        "    def __init__(self):\n",
        "        self.ps = PorterStemmer()\n",
        "    def __call__(self, doc):\n",
        "        doc_tok = word_tokenize(doc)\n",
        "        doc_tok = [t for t in doc_tok if t not in stop_words]\n",
        "        return [self.ps.stem(t) for t in doc_tok]"
      ],
      "metadata": {
        "id": "plMEhifao-l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Esta Clase permite contar la cantidad de palabras positivas y negatvas contenidas en el tweet para luego incorporarse como features\n",
        "class LiuFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, tokenizer=TweetTokenizer(preserve_case=False, reduce_len=True)):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.pos_set = set(opinion_lexicon.positive())\n",
        "        self.neg_set = set(opinion_lexicon.negative())\n",
        "\n",
        "    def __liu_score(self,sentence):\n",
        "        tokenized_sent = self.tokenizer.tokenize(sentence)\n",
        "        pos_words, neg_words = 0, 0\n",
        "        for word in tokenized_sent:\n",
        "            if word in self.pos_set:\n",
        "                pos_words += 1\n",
        "            elif word in self.neg_set:\n",
        "                neg_words += 1\n",
        "        return [pos_words,neg_words]\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        values = []\n",
        "        for tweet in X:\n",
        "            values.append(self.__liu_score(tweet))\n",
        "\n",
        "        return(np.array(values))\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ],
      "metadata": {
        "id": "_ACLFzJQpQvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n",
        "    def get_relevant_chars(self, tweet):\n",
        "        num_hashtags = tweet.count('#')\n",
        "        num_exclamations = tweet.count('!')\n",
        "        num_interrogations = tweet.count('?')\n",
        "        num_at = tweet.count('@')\n",
        "\n",
        "        # cuenta la cantidad de emojis que aparece para uno de los emojis aprendido del conjunto de entrenamiento\n",
        "        # Para luego obtener una proporci贸n de emojis aparecido en el tweet\n",
        "        dic={} # diccioario que tiene como llave cada emoji del train\n",
        "        for i in self.lista_emoji: # Para cada emoji se cuentan cuantos hay en el tweet\n",
        "            dic[i]=tweet.count(i)\n",
        "        list_final= list(dic.values())\n",
        "        num_emoji=emoji_count(tweet) # n煤mero de emojis en el tweet\n",
        "        if num_emoji==0: # si no habia ningun emoji se redefine como 1 para no dividir por cero\n",
        "            num_emoji=1\n",
        "        \n",
        "        # Finalmente se agrega la tasa de emojis y el n煤mero de emojis como features adicionales\n",
        "        return [num_hashtags, num_exclamations, num_interrogations, num_at]+list(map(lambda x: x /num_emoji, list_final))+[num_emoji]\n",
        "        \n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        chars = []\n",
        "        for tweet in X:\n",
        "            chars.append(self.get_relevant_chars(tweet))\n",
        "\n",
        "        return np.array(chars)\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Lista de emojis aprendidos en train \n",
        "        self.lista_emoji=X.apply(lambda x: emoji.distinct_emoji_list(x)).sum()\n",
        "        return self"
      ],
      "metadata": {
        "id": "yxuW7JrKM0JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experimentos"
      ],
      "metadata": {
        "id": "QOIPXithjj-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exp 1: BOW, 1,2-gramas con selecci贸n de variables del 5% con un XGboost\n",
        "def get_experiment_1_pipeline():\n",
        "    model= xgb.XGBClassifier(seed=1,eval_metric='mlogloss',learning_rate=0.01,gamma=1.3,n_estimators=700,max_depth=10)\n",
        "\n",
        "    return Pipeline(\n",
        "    [\n",
        "        (\"preprocesamiento\", FeatureUnion([('bow',CountVectorizer(tokenizer= StemmerTokenizer(),\n",
        "    ngram_range=(1,2))),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ])),\n",
        "                (\"Selection\", SelectPercentile(f_classif, percentile=5)), #mutual_info_regression\n",
        "     (\"clf\", model)]\n",
        ")"
      ],
      "metadata": {
        "id": "Uk-2ehJRnxfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_1_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUhg9gzqoYjA",
        "outputId": "bcf0efd9-aebc-4555-fa7d-e174cbdcb74c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  2  55   1]\n",
            " [  5 198   8]\n",
            " [  1  24  17]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.25      0.03      0.06        58\n",
            "      medium       0.71      0.94      0.81       211\n",
            "        high       0.65      0.40      0.50        42\n",
            "\n",
            "    accuracy                           0.70       311\n",
            "   macro avg       0.54      0.46      0.46       311\n",
            "weighted avg       0.62      0.70      0.63       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.673\tKappa: 0.204\tAccuracy: 0.698\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 21  66   0]\n",
            " [  8 213  25]\n",
            " [  1  51  30]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.70      0.24      0.36        87\n",
            "      medium       0.65      0.87      0.74       246\n",
            "        high       0.55      0.37      0.44        82\n",
            "\n",
            "    accuracy                           0.64       415\n",
            "   macro avg       0.63      0.49      0.51       415\n",
            "weighted avg       0.64      0.64      0.60       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.691\tKappa: 0.253\tAccuracy: 0.636\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 11  55   6]\n",
            " [  7 147  19]\n",
            " [  0  37  16]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.61      0.15      0.24        72\n",
            "      medium       0.62      0.85      0.71       173\n",
            "        high       0.39      0.30      0.34        53\n",
            "\n",
            "    accuracy                           0.58       298\n",
            "   macro avg       0.54      0.43      0.43       298\n",
            "weighted avg       0.57      0.58      0.53       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.678\tKappa: 0.16\tAccuracy: 0.584\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  7  64   2]\n",
            " [  5 130  25]\n",
            " [  1  31  19]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.54      0.10      0.16        73\n",
            "      medium       0.58      0.81      0.68       160\n",
            "        high       0.41      0.37      0.39        51\n",
            "\n",
            "    accuracy                           0.55       284\n",
            "   macro avg       0.51      0.43      0.41       284\n",
            "weighted avg       0.54      0.55      0.49       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.659\tKappa: 0.121\tAccuracy: 0.549\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.675\t Average Kappa: 0.184\t Average Accuracy: 0.617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exp 2: BOW 1,2-gramas  con PCA n=60 con XGboost\n",
        "def get_experiment_2_pipeline():\n",
        "    model2= xgb.XGBClassifier(seed=1,eval_metric='mlogloss',learning_rate=0.1,gamma=1.3,n_estimators=700,max_depth=10)\n",
        "    return  Pipeline(\n",
        "    [\n",
        "        (\"preprocesamiento\", FeatureUnion([('bow',CountVectorizer(tokenizer= StemmerTokenizer(),\n",
        "    ngram_range=(1,2))),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ])),\n",
        "                (\"PCA\",TruncatedSVD(n_components=60, n_iter=7, random_state=42)), \n",
        "     (\"clf\", model2)]\n",
        ")"
      ],
      "metadata": {
        "id": "U2ODxP4Kn3Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_2_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcK_SJMmu8K4",
        "outputId": "fddebb5a-db05-4217-be3a-c84684b16f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  4  54   0]\n",
            " [  8 200   3]\n",
            " [  2  31   9]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.29      0.07      0.11        58\n",
            "      medium       0.70      0.95      0.81       211\n",
            "        high       0.75      0.21      0.33        42\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.58      0.41      0.42       311\n",
            "weighted avg       0.63      0.68      0.61       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.583\tKappa: 0.136\tAccuracy: 0.685\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 20  65   2]\n",
            " [ 26 199  21]\n",
            " [  6  51  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.38      0.23      0.29        87\n",
            "      medium       0.63      0.81      0.71       246\n",
            "        high       0.52      0.30      0.38        82\n",
            "\n",
            "    accuracy                           0.59       415\n",
            "   macro avg       0.51      0.45      0.46       415\n",
            "weighted avg       0.56      0.59      0.56       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.635\tKappa: 0.177\tAccuracy: 0.588\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 18  52   2]\n",
            " [ 22 139  12]\n",
            " [  2  36  15]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.43      0.25      0.32        72\n",
            "      medium       0.61      0.80      0.69       173\n",
            "        high       0.52      0.28      0.37        53\n",
            "\n",
            "    accuracy                           0.58       298\n",
            "   macro avg       0.52      0.45      0.46       298\n",
            "weighted avg       0.55      0.58      0.54       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.658\tKappa: 0.165\tAccuracy: 0.577\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 21  47   5]\n",
            " [ 13 114  33]\n",
            " [  1  27  23]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.60      0.29      0.39        73\n",
            "      medium       0.61      0.71      0.66       160\n",
            "        high       0.38      0.45      0.41        51\n",
            "\n",
            "    accuracy                           0.56       284\n",
            "   macro avg       0.53      0.48      0.48       284\n",
            "weighted avg       0.56      0.56      0.54       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.629\tKappa: 0.203\tAccuracy: 0.556\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.626\t Average Kappa: 0.17\t Average Accuracy: 0.602\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exp 3 BOW 1,2-gramas, con PCAn=60 y RN de 3 capas\n",
        "def get_experiment_3_pipeline():\n",
        "    model3 = MLPClassifier(hidden_layer_sizes=(3,3,10), max_iter=1000,random_state=1)\n",
        "    return  Pipeline(\n",
        "    [\n",
        "        (\"preprocesamiento\", FeatureUnion([('bow',CountVectorizer(tokenizer= StemmerTokenizer(),\n",
        "    ngram_range=(1,2))),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ])),\n",
        "                (\"PCA\",TruncatedSVD(n_components=60, n_iter=7, random_state=42)), \n",
        "     (\"clf\", model3)]\n",
        ")"
      ],
      "metadata": {
        "id": "KTw6mRimn4PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_3_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ATBisTzu-DL",
        "outputId": "192c8f3f-3462-4f8e-b659-28f46e69aefb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  3  54   1]\n",
            " [  8 195   8]\n",
            " [  1  30  11]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.25      0.05      0.09        58\n",
            "      medium       0.70      0.92      0.80       211\n",
            "        high       0.55      0.26      0.35        42\n",
            "\n",
            "    accuracy                           0.67       311\n",
            "   macro avg       0.50      0.41      0.41       311\n",
            "weighted avg       0.60      0.67      0.60       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.569\tKappa: 0.127\tAccuracy: 0.672\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[  6  75   6]\n",
            " [  3 196  47]\n",
            " [  3  46  33]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.50      0.07      0.12        87\n",
            "      medium       0.62      0.80      0.70       246\n",
            "        high       0.38      0.40      0.39        82\n",
            "\n",
            "    accuracy                           0.57       415\n",
            "   macro avg       0.50      0.42      0.40       415\n",
            "weighted avg       0.55      0.57      0.52       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.608\tKappa: 0.133\tAccuracy: 0.566\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[  0  70   2]\n",
            " [  0 148  25]\n",
            " [  0  35  18]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.00      0.00      0.00        72\n",
            "      medium       0.58      0.86      0.69       173\n",
            "        high       0.40      0.34      0.37        53\n",
            "\n",
            "    accuracy                           0.56       298\n",
            "   macro avg       0.33      0.40      0.35       298\n",
            "weighted avg       0.41      0.56      0.47       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.624\tKappa: 0.078\tAccuracy: 0.557\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  8  64   1]\n",
            " [ 13 121  26]\n",
            " [  3  22  26]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.33      0.11      0.16        73\n",
            "      medium       0.58      0.76      0.66       160\n",
            "        high       0.49      0.51      0.50        51\n",
            "\n",
            "    accuracy                           0.55       284\n",
            "   macro avg       0.47      0.46      0.44       284\n",
            "weighted avg       0.50      0.55      0.50       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.606\tKappa: 0.15\tAccuracy: 0.546\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.602\t Average Kappa: 0.122\t Average Accuracy: 0.585\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
            "  activations.append(np.empty((X.shape[0],\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exp 4 BOW 1,2-gramas, PCA n=60 con SVM lineal\n",
        "def get_experiment_4_pipeline():\n",
        "    model4= svm.SVC(kernel='linear', probability=True)\n",
        "    return  Pipeline(\n",
        "    [\n",
        "        (\"preprocesamiento\", FeatureUnion([('bow',CountVectorizer(tokenizer= StemmerTokenizer(),\n",
        "    ngram_range=(1,2))),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ])),\n",
        "                (\"PCA\",TruncatedSVD(n_components=60, n_iter=7, random_state=42)), \n",
        "     (\"clf\", model4)]\n",
        ")"
      ],
      "metadata": {
        "id": "ULeQtTDbn66d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_4_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--YuveYWvBVx",
        "outputId": "4a59bff5-503e-4edf-8120-b7bf479aa426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  0  58   0]\n",
            " [  0 211   0]\n",
            " [  0  42   0]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.00      0.00      0.00        58\n",
            "      medium       0.68      1.00      0.81       211\n",
            "        high       0.00      0.00      0.00        42\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.23      0.33      0.27       311\n",
            "weighted avg       0.46      0.68      0.55       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.586\tKappa: 0.0\tAccuracy: 0.678\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for fear:\n",
            "\n",
            "[[  0  87   0]\n",
            " [  0 243   3]\n",
            " [  0  77   5]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.00      0.00      0.00        87\n",
            "      medium       0.60      0.99      0.74       246\n",
            "        high       0.62      0.06      0.11        82\n",
            "\n",
            "    accuracy                           0.60       415\n",
            "   macro avg       0.41      0.35      0.29       415\n",
            "weighted avg       0.48      0.60      0.46       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.614\tKappa: 0.03\tAccuracy: 0.598\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for joy:\n",
            "\n",
            "[[  0  72   0]\n",
            " [  0 169   4]\n",
            " [  0  44   9]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.00      0.00      0.00        72\n",
            "      medium       0.59      0.98      0.74       173\n",
            "        high       0.69      0.17      0.27        53\n",
            "\n",
            "    accuracy                           0.60       298\n",
            "   macro avg       0.43      0.38      0.34       298\n",
            "weighted avg       0.47      0.60      0.48       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.653\tKappa: 0.079\tAccuracy: 0.597\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  3  70   0]\n",
            " [  1 148  11]\n",
            " [  0  42   9]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.75      0.04      0.08        73\n",
            "      medium       0.57      0.93      0.70       160\n",
            "        high       0.45      0.18      0.25        51\n",
            "\n",
            "    accuracy                           0.56       284\n",
            "   macro avg       0.59      0.38      0.35       284\n",
            "weighted avg       0.59      0.56      0.46       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.665\tKappa: 0.067\tAccuracy: 0.563\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.629\t Average Kappa: 0.044\t Average Accuracy: 0.609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exp 5 BOW 1,2-gramas, PCA n=60 con SVM rbf\n",
        "def get_experiment_5_pipeline():\n",
        "    model5= svm.SVC(kernel='rbf',probability=True)\n",
        "    return  Pipeline(\n",
        "    [\n",
        "        (\"preprocesamiento\", FeatureUnion([('bow',CountVectorizer(tokenizer= StemmerTokenizer(),\n",
        "    ngram_range=(1,2))),\n",
        "                                    ('chars_count', CharsCountTransformer()),\n",
        "                                    ])),\n",
        "                (\"PCA\",TruncatedSVD(n_components=60, n_iter=7, random_state=42)), \n",
        "     (\"clf\", model5)]\n",
        ")"
      ],
      "metadata": {
        "id": "6F-99lobn9Kh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_5_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9bzGgUWvDWT",
        "outputId": "e12669a8-d272-49d3-8194-315f695f38d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  0  58   0]\n",
            " [  0 211   0]\n",
            " [  0  39   3]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.00      0.00      0.00        58\n",
            "      medium       0.69      1.00      0.81       211\n",
            "        high       1.00      0.07      0.13        42\n",
            "\n",
            "    accuracy                           0.69       311\n",
            "   macro avg       0.56      0.36      0.32       311\n",
            "weighted avg       0.60      0.69      0.57       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.583\tKappa: 0.046\tAccuracy: 0.688\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  labels : list, optional\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix for fear:\n",
            "\n",
            "[[  1  82   4]\n",
            " [  2 231  13]\n",
            " [  0  68  14]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.33      0.01      0.02        87\n",
            "      medium       0.61      0.94      0.74       246\n",
            "        high       0.45      0.17      0.25        82\n",
            "\n",
            "    accuracy                           0.59       415\n",
            "   macro avg       0.46      0.37      0.34       415\n",
            "weighted avg       0.52      0.59      0.49       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.611\tKappa: 0.073\tAccuracy: 0.593\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[  2  68   2]\n",
            " [  2 157  14]\n",
            " [  0  29  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.50      0.03      0.05        72\n",
            "      medium       0.62      0.91      0.74       173\n",
            "        high       0.60      0.45      0.52        53\n",
            "\n",
            "    accuracy                           0.61       298\n",
            "   macro avg       0.57      0.46      0.43       298\n",
            "weighted avg       0.59      0.61      0.53       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.654\tKappa: 0.193\tAccuracy: 0.614\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  1  68   4]\n",
            " [  2 135  23]\n",
            " [  0  29  22]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.33      0.01      0.03        73\n",
            "      medium       0.58      0.84      0.69       160\n",
            "        high       0.45      0.43      0.44        51\n",
            "\n",
            "    accuracy                           0.56       284\n",
            "   macro avg       0.45      0.43      0.39       284\n",
            "weighted avg       0.49      0.56      0.47       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.632\tKappa: 0.123\tAccuracy: 0.556\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.62\t Average Kappa: 0.109\t Average Accuracy: 0.613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**TF-IDF**"
      ],
      "metadata": {
        "id": "ShfKYaxCMnSg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pipelines con los distintos experimentos ejecutados con Tf-Idf"
      ],
      "metadata": {
        "id": "dMnUqTjq2e1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_experiment_0_pipeline():\n",
        "\n",
        "  return Pipeline([('features', FeatureUnion([('td_idf', TfidfVectorizer()), \n",
        "                                ('extractor', FeaturesTransformer())\n",
        "                                ])), \n",
        "                    ('clf', RandomForestClassifier())])\n",
        "\n",
        "def get_experiment_1_pipeline():\n",
        "\n",
        "  return Pipeline([('features', FeatureUnion([('td_idf', TfidfVectorizer()), \n",
        "                                ('extractor', FeaturesTransformer())\n",
        "                                ])), \n",
        "                    ('clf', RandomForestClassifier(random_state = 3380))])\n",
        "  \n",
        "def get_experiment_2_pipeline():\n",
        "\n",
        "  return Pipeline([('features', FeatureUnion([('td_idf', TfidfVectorizer()), \n",
        "                                ('extractor', FeaturesTransformer())\n",
        "                                ])), \n",
        "                    ('clf', RandomForestClassifier(n_estimators=1000))]) \n",
        "  \n",
        "def get_experiment_3_pipeline():\n",
        "\n",
        "  return Pipeline([('features', FeatureUnion([('td_idf', TfidfVectorizer()), \n",
        "                                ('extractor', FeaturesTransformer())\n",
        "                                ])), \n",
        "                    ('clf', XGBClassifier())])\n",
        "  \n",
        "def get_experiment_4_pipeline():\n",
        "\n",
        "  return Pipeline([('features', FeatureUnion([('td_idf', TfidfVectorizer()), \n",
        "                                ('extractor', FeaturesTransformer())\n",
        "                                ])), \n",
        "                    ('clf', SVC(kernel='linear', probability=True))])\n",
        "  \n",
        "def get_experiment_5_pipeline():\n",
        "\n",
        "  return Pipeline([('features', FeatureUnion([('td_idf', TfidfVectorizer()), \n",
        "                                ('extractor', FeaturesTransformer())\n",
        "                                ])), \n",
        "                    ('clf', SVC(kernel='rbf', C=250, probability=True))])\n",
        "\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "def get_experiment_6_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('features', FeatureUnion([('tdidf_pca', Pipeline([('td_idf', TfidfVectorizer()),\n",
        "                                                            (\"PCA\",TruncatedSVD(n_components=60, n_iter=7, random_state=42)),\n",
        "                                                            ])), \n",
        "                                  ('extractor', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "def get_experiment_7_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('features', FeatureUnion([('tdidf_pca', Pipeline([('td_idf', TfidfVectorizer()),\n",
        "                                                            (\"PCA\",TruncatedSVD(n_components=10)),\n",
        "                                                            ])), \n",
        "                                  ('extractor', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(n_estimators=1000,random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "def get_experiment_8_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('features', FeatureUnion([('tdidf_pca', Pipeline([('td_idf', TfidfVectorizer()),\n",
        "                                                            (\"PCA\",TruncatedSVD(n_components=10)),\n",
        "                                                            ])), \n",
        "                                  ('extractor', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', SVC(kernel='linear', probability=True)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "q8BRh7YP2hcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experimentos"
      ],
      "metadata": {
        "id": "8jLD8ChO3h2k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comentario: demora 20min cada experimento :( ahora solo ejecut茅 el primero. para tenerlo en cuenta y definamos bien el tema de los expermientos."
      ],
      "metadata": {
        "id": "-FJ4x2mH8CKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 0: TF_IDF + EXTRACTOR + RF0\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_0_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VowWtwMA2PSX",
        "outputId": "da1d7bf1-2bf2-4780-869e-4ed8225e8b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 11  47   0]\n",
            " [  3 206   2]\n",
            " [  0  31  11]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.79      0.19      0.31        58\n",
            "      medium       0.73      0.98      0.83       211\n",
            "        high       0.85      0.26      0.40        42\n",
            "\n",
            "    accuracy                           0.73       311\n",
            "   macro avg       0.79      0.48      0.51       311\n",
            "weighted avg       0.75      0.73      0.68       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.733\tKappa: 0.272\tAccuracy: 0.733\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 37  48   2]\n",
            " [ 18 214  14]\n",
            " [  0  55  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.67      0.43      0.52        87\n",
            "      medium       0.68      0.87      0.76       246\n",
            "        high       0.63      0.33      0.43        82\n",
            "\n",
            "    accuracy                           0.67       415\n",
            "   macro avg       0.66      0.54      0.57       415\n",
            "weighted avg       0.67      0.67      0.65       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.727\tKappa: 0.338\tAccuracy: 0.67\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 31  41   0]\n",
            " [ 15 149   9]\n",
            " [  0  28  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.67      0.43      0.53        72\n",
            "      medium       0.68      0.86      0.76       173\n",
            "        high       0.74      0.47      0.57        53\n",
            "\n",
            "    accuracy                           0.69       298\n",
            "   macro avg       0.70      0.59      0.62       298\n",
            "weighted avg       0.69      0.69      0.67       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.763\tKappa: 0.397\tAccuracy: 0.688\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 30  43   0]\n",
            " [ 16 128  16]\n",
            " [  0  29  22]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.65      0.41      0.50        73\n",
            "      medium       0.64      0.80      0.71       160\n",
            "        high       0.58      0.43      0.49        51\n",
            "\n",
            "    accuracy                           0.63       284\n",
            "   macro avg       0.62      0.55      0.57       284\n",
            "weighted avg       0.63      0.63      0.62       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.701\tKappa: 0.319\tAccuracy: 0.634\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.731\t Average Kappa: 0.332\t Average Accuracy: 0.681\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 1: TF_IDF + EXTRACTOR + RF1\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_1_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b4b7a9a-1be8-4022-9dd6-8538ef930305",
        "id": "-RpSJYIi35Gz"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  7  51   0]\n",
            " [  3 206   2]\n",
            " [  0  30  12]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.70      0.12      0.21        58\n",
            "      medium       0.72      0.98      0.83       211\n",
            "        high       0.86      0.29      0.43        42\n",
            "\n",
            "    accuracy                           0.72       311\n",
            "   macro avg       0.76      0.46      0.49       311\n",
            "weighted avg       0.73      0.72      0.66       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.705\tKappa: 0.236\tAccuracy: 0.723\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 40  46   1]\n",
            " [ 17 212  17]\n",
            " [  0  55  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.70      0.46      0.56        87\n",
            "      medium       0.68      0.86      0.76       246\n",
            "        high       0.60      0.33      0.43        82\n",
            "\n",
            "    accuracy                           0.67       415\n",
            "   macro avg       0.66      0.55      0.58       415\n",
            "weighted avg       0.67      0.67      0.65       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.719\tKappa: 0.348\tAccuracy: 0.672\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 27  45   0]\n",
            " [ 13 145  15]\n",
            " [  0  28  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.68      0.38      0.48        72\n",
            "      medium       0.67      0.84      0.74       173\n",
            "        high       0.62      0.47      0.54        53\n",
            "\n",
            "    accuracy                           0.66       298\n",
            "   macro avg       0.66      0.56      0.59       298\n",
            "weighted avg       0.66      0.66      0.64       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.748\tKappa: 0.347\tAccuracy: 0.661\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 31  42   0]\n",
            " [ 16 127  17]\n",
            " [  0  34  17]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.66      0.42      0.52        73\n",
            "      medium       0.63      0.79      0.70       160\n",
            "        high       0.50      0.33      0.40        51\n",
            "\n",
            "    accuracy                           0.62       284\n",
            "   macro avg       0.60      0.52      0.54       284\n",
            "weighted avg       0.61      0.62      0.60       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.695\tKappa: 0.28\tAccuracy: 0.616\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.717\t Average Kappa: 0.303\t Average Accuracy: 0.668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 2: TF_IDF + EXTRACTOR + RF2\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_2_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed1edfbd-58d7-497b-cfc4-3a67ee43d3ce",
        "id": "02vG1OGU4AU_"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  8  50   0]\n",
            " [  3 208   0]\n",
            " [  0  31  11]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.73      0.14      0.23        58\n",
            "      medium       0.72      0.99      0.83       211\n",
            "        high       1.00      0.26      0.42        42\n",
            "\n",
            "    accuracy                           0.73       311\n",
            "   macro avg       0.82      0.46      0.49       311\n",
            "weighted avg       0.76      0.73      0.66       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.733\tKappa: 0.246\tAccuracy: 0.73\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 39  47   1]\n",
            " [ 16 215  15]\n",
            " [  0  55  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.71      0.45      0.55        87\n",
            "      medium       0.68      0.87      0.76       246\n",
            "        high       0.63      0.33      0.43        82\n",
            "\n",
            "    accuracy                           0.68       415\n",
            "   macro avg       0.67      0.55      0.58       415\n",
            "weighted avg       0.67      0.68      0.65       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.729\tKappa: 0.353\tAccuracy: 0.677\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 31  41   0]\n",
            " [ 11 151  11]\n",
            " [  0  27  26]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.74      0.43      0.54        72\n",
            "      medium       0.69      0.87      0.77       173\n",
            "        high       0.70      0.49      0.58        53\n",
            "\n",
            "    accuracy                           0.70       298\n",
            "   macro avg       0.71      0.60      0.63       298\n",
            "weighted avg       0.70      0.70      0.68       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.758\tKappa: 0.416\tAccuracy: 0.698\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 30  43   0]\n",
            " [ 14 130  16]\n",
            " [  0  29  22]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.68      0.41      0.51        73\n",
            "      medium       0.64      0.81      0.72       160\n",
            "        high       0.58      0.43      0.49        51\n",
            "\n",
            "    accuracy                           0.64       284\n",
            "   macro avg       0.63      0.55      0.58       284\n",
            "weighted avg       0.64      0.64      0.63       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.703\tKappa: 0.329\tAccuracy: 0.641\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.731\t Average Kappa: 0.336\t Average Accuracy: 0.686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 3: TF_IDF + EXTRACTOR + XGboost + Oversampling\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_3_pipeline(), param_over = 0.8, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJRAh8os4gWt",
        "outputId": "eb328d33-3d22-4c61-f17f-456adbfed966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset sadness: 801\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[45 27  1]\n",
            " [34 69 57]\n",
            " [ 0 14 37]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.57      0.62      0.59        73\n",
            "      medium       0.63      0.43      0.51       160\n",
            "        high       0.39      0.73      0.51        51\n",
            "\n",
            "    accuracy                           0.53       284\n",
            "   macro avg       0.53      0.59      0.54       284\n",
            "weighted avg       0.57      0.53      0.53       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.692\tKappa: 0.28\tAccuracy: 0.532\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.692\t Average Kappa: 0.28\t Average Accuracy: 0.532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 4: TF_IDF + EXTRACTOR + SVM lineal\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_4_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZuBCIZn4oZ7",
        "outputId": "3adc1f94-d2a1-472e-db89-ceb259e91737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 25  45   3]\n",
            " [ 14 117  29]\n",
            " [  0  27  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.64      0.34      0.45        73\n",
            "      medium       0.62      0.73      0.67       160\n",
            "        high       0.43      0.47      0.45        51\n",
            "\n",
            "    accuracy                           0.58       284\n",
            "   macro avg       0.56      0.51      0.52       284\n",
            "weighted avg       0.59      0.58      0.57       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.711\tKappa: 0.251\tAccuracy: 0.585\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.711\t Average Kappa: 0.251\t Average Accuracy: 0.585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 5: TF_IDF + EXTRACTOR + SVM rbf + Subsampling\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_5_pipeline(), param_over = False, param_under = 0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW04-b6g5arO",
        "outputId": "edcc5940-a5b0-4f5d-c9be-fe6b49896fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset sadness: 546\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 19  51   3]\n",
            " [ 13 121  26]\n",
            " [  1  31  19]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.58      0.26      0.36        73\n",
            "      medium       0.60      0.76      0.67       160\n",
            "        high       0.40      0.37      0.38        51\n",
            "\n",
            "    accuracy                           0.56       284\n",
            "   macro avg       0.52      0.46      0.47       284\n",
            "weighted avg       0.55      0.56      0.54       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.636\tKappa: 0.18\tAccuracy: 0.56\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.636\t Average Kappa: 0.18\t Average Accuracy: 0.56\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 6: TF_IDF + EXTRACTOR + PCA + RF1\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_6_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvUgNeCa5r7Z",
        "outputId": "2cacb24b-9187-438d-a240-9d9593e74d15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 33  40   0]\n",
            " [ 17 129  14]\n",
            " [  0  34  17]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.66      0.45      0.54        73\n",
            "      medium       0.64      0.81      0.71       160\n",
            "        high       0.55      0.33      0.41        51\n",
            "\n",
            "    accuracy                           0.63       284\n",
            "   macro avg       0.61      0.53      0.55       284\n",
            "weighted avg       0.63      0.63      0.61       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.709\tKappa: 0.306\tAccuracy: 0.63\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.709\t Average Kappa: 0.306\t Average Accuracy: 0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 7: TF_IDF + EXTRACTOR + PCA + RF2\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_7_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74dca53-1933-40de-f853-94d312a5acb0",
        "id": "9E8oaQK14P60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 12  46   0]\n",
            " [  5 203   3]\n",
            " [  0  32  10]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.71      0.21      0.32        58\n",
            "      medium       0.72      0.96      0.83       211\n",
            "        high       0.77      0.24      0.36        42\n",
            "\n",
            "    accuracy                           0.72       311\n",
            "   macro avg       0.73      0.47      0.50       311\n",
            "weighted avg       0.73      0.72      0.67       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.732\tKappa: 0.255\tAccuracy: 0.723\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 38  46   3]\n",
            " [ 22 204  20]\n",
            " [  3  55  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.60      0.44      0.51        87\n",
            "      medium       0.67      0.83      0.74       246\n",
            "        high       0.51      0.29      0.37        82\n",
            "\n",
            "    accuracy                           0.64       415\n",
            "   macro avg       0.59      0.52      0.54       415\n",
            "weighted avg       0.62      0.64      0.62       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.702\tKappa: 0.296\tAccuracy: 0.641\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 35  37   0]\n",
            " [ 28 128  17]\n",
            " [  1  24  28]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.55      0.49      0.51        72\n",
            "      medium       0.68      0.74      0.71       173\n",
            "        high       0.62      0.53      0.57        53\n",
            "\n",
            "    accuracy                           0.64       298\n",
            "   macro avg       0.62      0.58      0.60       298\n",
            "weighted avg       0.64      0.64      0.64       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.748\tKappa: 0.351\tAccuracy: 0.641\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 32  41   0]\n",
            " [ 19 120  21]\n",
            " [  0  24  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.63      0.44      0.52        73\n",
            "      medium       0.65      0.75      0.70       160\n",
            "        high       0.56      0.53      0.55        51\n",
            "\n",
            "    accuracy                           0.63       284\n",
            "   macro avg       0.61      0.57      0.59       284\n",
            "weighted avg       0.63      0.63      0.62       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.727\tKappa: 0.336\tAccuracy: 0.63\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.727\t Average Kappa: 0.309\t Average Accuracy: 0.659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Exp 8: TF_IDF + EXTRACTOR + PCA + SVM lineal\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_experiment_8_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5183xvs50AK",
        "outputId": "5a8454eb-0b1a-49d0-9574-4b27ae734e95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 30  41   2]\n",
            " [ 14 123  23]\n",
            " [  0  30  21]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.68      0.41      0.51        73\n",
            "      medium       0.63      0.77      0.69       160\n",
            "        high       0.46      0.41      0.43        51\n",
            "\n",
            "    accuracy                           0.61       284\n",
            "   macro avg       0.59      0.53      0.55       284\n",
            "weighted avg       0.61      0.61      0.60       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.713\tKappa: 0.291\tAccuracy: 0.613\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.713\t Average Kappa: 0.291\t Average Accuracy: 0.613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Embeddings preentrenados**"
      ],
      "metadata": {
        "id": "KEzZjbV3Mp-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lo largo de los experimentos, notamos que la inclusi贸n de features es siempre mejor a generar predicciones solo con los embeddings. Adem谩s, la inclusi贸n de resampleo logra mejores resultados para configuraciones espec铆ficas. Finalmente y dada la dimensionalidad de los datos, fue 煤til implementar `PCA` reducir la cantidad de columnas, evitar la maldici贸n de la dimensionalidad y mejorar el poder de predicci贸n de los clasificadores.\n",
        "\n",
        "Mejores resultados:\n",
        "\n",
        "- Anger: 0.756\n",
        "- Fear: 0.677\n",
        "- Joy: 0.695\n",
        "- Sadness: 0.648"
      ],
      "metadata": {
        "id": "IM37873YfIya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Objetos y funciones"
      ],
      "metadata": {
        "id": "23GC7e6XSQqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Doc2Vector(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = pretrained_model\n",
        "        self.vocab = list(pretrained_model.vocab.keys())\n",
        "\n",
        "    def pre_processing(self, tweet: str):\n",
        "        tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet) #elimina urls\n",
        "        tweet = re.sub('(@[^\\s]+)|(w/)','', tweet) #elimina menciones\n",
        "        tweet = re.sub(r'#([^\\s]+)', r'\\1', tweet) #le quita el # a los hashtags\n",
        "        tweet = re.sub('\\n','', tweet) #elimina saltos de l铆nea\n",
        "        return tweet\n",
        "\n",
        "    def to_vector(self, sentence):\n",
        "        \n",
        "        sentence = self.pre_processing(sentence)\n",
        "\n",
        "        tokenized_sent = self.tokenizer.tokenize(sentence)\n",
        "        doc_vector = np.empty(self.model.vector_size, dtype=float)\n",
        "        \n",
        "        for word in tokenized_sent:\n",
        "            if word in self.vocab:\n",
        "                word_vector = self.model[word]\n",
        "                doc_vector  = np.vstack((doc_vector, word_vector))\n",
        "         \n",
        "        try:\n",
        "            doc_vector = doc_vector[1:].mean(axis=0)\n",
        "            return list(doc_vector)\n",
        "        except:\n",
        "            #print('Retornando vector de 0s!')\n",
        "            return list(np.array(np.zeros(self.model.vector_size)))\n",
        "            #return None\n",
        "        \n",
        "    def transform(self, X, y=None):\n",
        "        values = []\n",
        "        for tweet in X:\n",
        "            vector = self.to_vector(tweet)\n",
        "            values.append(vector)\n",
        "                \n",
        "        final_array = np.squeeze(np.array(values))\n",
        "        dataframe = pd.DataFrame(final_array)\n",
        "        return dataframe\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ],
      "metadata": {
        "id": "XM4hsImNOCzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizador= TweetTokenizer(preserve_case=False, reduce_len=True)\n",
        "\n",
        "# best models\n",
        "best_models = {sentiment: (0, 0) for sentiment in train.keys()}\n",
        "best_models\n",
        "\n",
        "FeaturesTransformer().transform(['I am  #depression  BUT I love YOU!  What* estoy sad :( love www.hola.cl @ @hola']) #probamos transformer"
      ],
      "metadata": {
        "id": "Vf3AtHJdOOoN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcce5259-5d8e-4fea-da31-0df43a89326f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.        , 1.        , 0.        , 2.        , 1.        ,\n",
              "        9.        , 2.        , 4.        , 3.        , 1.        ,\n",
              "        2.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "        0.276     , 0.318     , 0.406     , 0.8733    , 0.07504688,\n",
              "        0.02131401]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Prueba de diferentes embeddings preentrenados"
      ],
      "metadata": {
        "id": "-Rj4x0oEOrYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizador= TweetTokenizer(preserve_case=False, reduce_len=True)"
      ],
      "metadata": {
        "id": "GdpYARkvM0qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline():\n",
        "  pipe = Pipeline([\n",
        "     ('embedding', Doc2Vector(tokenizer = tokenizador)), \n",
        "     ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "     ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "nOnkjokTOVwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 25d"
      ],
      "metadata": {
        "id": "7XXDCEbdO5pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = gensim.downloader.load('glove-twitter-25')\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline())"
      ],
      "metadata": {
        "id": "o-9xgjhWPT_v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94ce2d83-f319-4160-f275-c9a0bac91868"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  6  52   0]\n",
            " [  4 201   6]\n",
            " [  0  35   7]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.60      0.10      0.18        58\n",
            "      medium       0.70      0.95      0.81       211\n",
            "        high       0.54      0.17      0.25        42\n",
            "\n",
            "    accuracy                           0.69       311\n",
            "   macro avg       0.61      0.41      0.41       311\n",
            "weighted avg       0.66      0.69      0.61       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.623\tKappa: 0.134\tAccuracy: 0.688\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 22  63   2]\n",
            " [ 26 195  25]\n",
            " [  2  59  21]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.44      0.25      0.32        87\n",
            "      medium       0.62      0.79      0.69       246\n",
            "        high       0.44      0.26      0.32        82\n",
            "\n",
            "    accuracy                           0.57       415\n",
            "   macro avg       0.50      0.43      0.45       415\n",
            "weighted avg       0.54      0.57      0.54       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.669\tKappa: 0.145\tAccuracy: 0.573\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 22  50   0]\n",
            " [ 18 144  11]\n",
            " [  0  29  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.55      0.31      0.39        72\n",
            "      medium       0.65      0.83      0.73       173\n",
            "        high       0.69      0.45      0.55        53\n",
            "\n",
            "    accuracy                           0.64       298\n",
            "   macro avg       0.63      0.53      0.56       298\n",
            "weighted avg       0.63      0.64      0.61       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.717\tKappa: 0.293\tAccuracy: 0.638\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 18  47   8]\n",
            " [ 14 126  20]\n",
            " [  2  34  15]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.53      0.25      0.34        73\n",
            "      medium       0.61      0.79      0.69       160\n",
            "        high       0.35      0.29      0.32        51\n",
            "\n",
            "    accuracy                           0.56       284\n",
            "   macro avg       0.50      0.44      0.45       284\n",
            "weighted avg       0.54      0.56      0.53       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.659\tKappa: 0.172\tAccuracy: 0.56\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.667\t Average Kappa: 0.186\t Average Accuracy: 0.615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 200d"
      ],
      "metadata": {
        "id": "ujKIW2n6Pdzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = gensim.downloader.load('glove-twitter-200')\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline())"
      ],
      "metadata": {
        "id": "i7awR7GuPfyo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c329112a-d9dc-4961-de1a-3503f2c275c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 758.5/758.5MB downloaded\n",
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  5  53   0]\n",
            " [  2 200   9]\n",
            " [  0  35   7]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.71      0.09      0.15        58\n",
            "      medium       0.69      0.95      0.80       211\n",
            "        high       0.44      0.17      0.24        42\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.62      0.40      0.40       311\n",
            "weighted avg       0.66      0.68      0.61       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.661\tKappa: 0.117\tAccuracy: 0.682\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 17  65   5]\n",
            " [ 15 214  17]\n",
            " [  1  62  19]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.52      0.20      0.28        87\n",
            "      medium       0.63      0.87      0.73       246\n",
            "        high       0.46      0.23      0.31        82\n",
            "\n",
            "    accuracy                           0.60       415\n",
            "   macro avg       0.54      0.43      0.44       415\n",
            "weighted avg       0.57      0.60      0.55       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.683\tKappa: 0.166\tAccuracy: 0.602\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 18  54   0]\n",
            " [ 13 147  13]\n",
            " [  0  30  23]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.58      0.25      0.35        72\n",
            "      medium       0.64      0.85      0.73       173\n",
            "        high       0.64      0.43      0.52        53\n",
            "\n",
            "    accuracy                           0.63       298\n",
            "   macro avg       0.62      0.51      0.53       298\n",
            "weighted avg       0.62      0.63      0.60       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.713\tKappa: 0.267\tAccuracy: 0.631\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 17  52   4]\n",
            " [  8 131  21]\n",
            " [  1  33  17]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.65      0.23      0.34        73\n",
            "      medium       0.61      0.82      0.70       160\n",
            "        high       0.40      0.33      0.37        51\n",
            "\n",
            "    accuracy                           0.58       284\n",
            "   macro avg       0.56      0.46      0.47       284\n",
            "weighted avg       0.58      0.58      0.55       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.659\tKappa: 0.196\tAccuracy: 0.581\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.679\t Average Kappa: 0.186\t Average Accuracy: 0.624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Embedding + features"
      ],
      "metadata": {
        "id": "sGf5lnrOPrWW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci贸n, se probar谩 incorporar al modelo las features creadas a mano por el objeto `FeaturesTransformer`.\n",
        "\n",
        "**Conclusi贸n**: la incorporaci贸n de las features tiene un gran impacto en el poder de predicci贸n del modelo."
      ],
      "metadata": {
        "id": "vOsQL57DPvri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = gensim.downloader.load('glove-twitter-200')"
      ],
      "metadata": {
        "id": "M0Dzg9XvQFeh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding', Doc2Vector(tokenizer = tokenizador)), \n",
        "                                    ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "4-_wTxHSQJIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline())"
      ],
      "metadata": {
        "id": "JzDnQCsYQPGN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c093fcfd-57f9-4a6a-f7ad-0b17654bea77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  6  52   0]\n",
            " [  2 204   5]\n",
            " [  0  32  10]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.75      0.10      0.18        58\n",
            "      medium       0.71      0.97      0.82       211\n",
            "        high       0.67      0.24      0.35        42\n",
            "\n",
            "    accuracy                           0.71       311\n",
            "   macro avg       0.71      0.44      0.45       311\n",
            "weighted avg       0.71      0.71      0.64       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.69\tKappa: 0.188\tAccuracy: 0.707\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 35  51   1]\n",
            " [ 13 221  12]\n",
            " [  1  59  22]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.71      0.40      0.51        87\n",
            "      medium       0.67      0.90      0.77       246\n",
            "        high       0.63      0.27      0.38        82\n",
            "\n",
            "    accuracy                           0.67       415\n",
            "   macro avg       0.67      0.52      0.55       415\n",
            "weighted avg       0.67      0.67      0.64       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.722\tKappa: 0.32\tAccuracy: 0.67\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 25  47   0]\n",
            " [ 13 149  11]\n",
            " [  0  28  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.66      0.35      0.45        72\n",
            "      medium       0.67      0.86      0.75       173\n",
            "        high       0.69      0.47      0.56        53\n",
            "\n",
            "    accuracy                           0.67       298\n",
            "   macro avg       0.67      0.56      0.59       298\n",
            "weighted avg       0.67      0.67      0.65       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.746\tKappa: 0.35\tAccuracy: 0.668\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 31  40   2]\n",
            " [  8 128  24]\n",
            " [  0  34  17]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.79      0.42      0.55        73\n",
            "      medium       0.63      0.80      0.71       160\n",
            "        high       0.40      0.33      0.36        51\n",
            "\n",
            "    accuracy                           0.62       284\n",
            "   macro avg       0.61      0.52      0.54       284\n",
            "weighted avg       0.63      0.62      0.61       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.729\tKappa: 0.292\tAccuracy: 0.62\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.722\t Average Kappa: 0.287\t Average Accuracy: 0.666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embedding + features + balanceo de clases"
      ],
      "metadata": {
        "id": "vaQ1fXOvQUOz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci贸n se presentan los experimentos sobre diferentes configuraciones del tama帽o de muestra. Para esto, nos apoyaremos de la funci贸n `resample`, donde iremos variando los par谩metros `param_under` y `param_over` para probar diferentes configuraciones de undersampling y oversampling.\n",
        "\n",
        "**Conclusi贸n**: La mejor combinaci贸n de `param_under` y `param_over` se di贸 cuando `param_under` = 0.9 y `param_over` = 0.1. "
      ],
      "metadata": {
        "id": "RkM77KjcQkoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = gensim.downloader.load('glove-twitter-200')"
      ],
      "metadata": {
        "id": "-4dmndTDQZIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding', Doc2Vector(tokenizer = tokenizador)), \n",
        "                                    ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "4txTPDWzQb2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### param_under = 0.9"
      ],
      "metadata": {
        "id": "Sbc6cJtHQ112"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = False, param_under = 0.9)"
      ],
      "metadata": {
        "id": "8rbAledkQd_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6250b7c2-6ac5-4f8a-8e08-1aa54ade9e68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 589\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  7  51   0]\n",
            " [  2 204   5]\n",
            " [  0  34   8]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.78      0.12      0.21        58\n",
            "      medium       0.71      0.97      0.82       211\n",
            "        high       0.62      0.19      0.29        42\n",
            "\n",
            "    accuracy                           0.70       311\n",
            "   macro avg       0.70      0.43      0.44       311\n",
            "weighted avg       0.71      0.70      0.63       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.702\tKappa: 0.175\tAccuracy: 0.704\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 796\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 36  50   1]\n",
            " [ 21 208  17]\n",
            " [  3  55  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.60      0.41      0.49        87\n",
            "      medium       0.66      0.85      0.74       246\n",
            "        high       0.57      0.29      0.39        82\n",
            "\n",
            "    accuracy                           0.65       415\n",
            "   macro avg       0.61      0.52      0.54       415\n",
            "weighted avg       0.63      0.65      0.62       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.722\tKappa: 0.295\tAccuracy: 0.646\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 572\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 34  38   0]\n",
            " [ 25 135  13]\n",
            " [  0  30  23]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.58      0.47      0.52        72\n",
            "      medium       0.67      0.78      0.72       173\n",
            "        high       0.64      0.43      0.52        53\n",
            "\n",
            "    accuracy                           0.64       298\n",
            "   macro avg       0.63      0.56      0.58       298\n",
            "weighted avg       0.64      0.64      0.63       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.752\tKappa: 0.335\tAccuracy: 0.644\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 546\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 27  44   2]\n",
            " [ 10 122  28]\n",
            " [  0  29  22]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.73      0.37      0.49        73\n",
            "      medium       0.63      0.76      0.69       160\n",
            "        high       0.42      0.43      0.43        51\n",
            "\n",
            "    accuracy                           0.60       284\n",
            "   macro avg       0.59      0.52      0.54       284\n",
            "weighted avg       0.62      0.60      0.59       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.727\tKappa: 0.272\tAccuracy: 0.602\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.726\t Average Kappa: 0.269\t Average Accuracy: 0.649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### param_over = 0.1"
      ],
      "metadata": {
        "id": "gHiPAFQuQ6pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = 0.1, param_under = False)"
      ],
      "metadata": {
        "id": "_gP0QjYVQ9sf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e800a69-268b-4932-e910-d4df50c2b247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 652\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  5  52   1]\n",
            " [  2 205   4]\n",
            " [  0  31  11]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.71      0.09      0.15        58\n",
            "      medium       0.71      0.97      0.82       211\n",
            "        high       0.69      0.26      0.38        42\n",
            "\n",
            "    accuracy                           0.71       311\n",
            "   macro avg       0.70      0.44      0.45       311\n",
            "weighted avg       0.71      0.71      0.64       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.689\tKappa: 0.197\tAccuracy: 0.711\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 880\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 40  46   1]\n",
            " [ 17 219  10]\n",
            " [  2  60  20]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.68      0.46      0.55        87\n",
            "      medium       0.67      0.89      0.77       246\n",
            "        high       0.65      0.24      0.35        82\n",
            "\n",
            "    accuracy                           0.67       415\n",
            "   macro avg       0.67      0.53      0.56       415\n",
            "weighted avg       0.67      0.67      0.64       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.734\tKappa: 0.333\tAccuracy: 0.672\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 632\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 24  48   0]\n",
            " [ 14 145  14]\n",
            " [  0  28  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.63      0.33      0.44        72\n",
            "      medium       0.66      0.84      0.74       173\n",
            "        high       0.64      0.47      0.54        53\n",
            "\n",
            "    accuracy                           0.65       298\n",
            "   macro avg       0.64      0.55      0.57       298\n",
            "weighted avg       0.65      0.65      0.63       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.764\tKappa: 0.323\tAccuracy: 0.651\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 603\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 33  36   4]\n",
            " [  9 132  19]\n",
            " [  0  32  19]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.79      0.45      0.57        73\n",
            "      medium       0.66      0.82      0.73       160\n",
            "        high       0.45      0.37      0.41        51\n",
            "\n",
            "    accuracy                           0.65       284\n",
            "   macro avg       0.63      0.55      0.57       284\n",
            "weighted avg       0.66      0.65      0.63       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.726\tKappa: 0.346\tAccuracy: 0.648\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.728\t Average Kappa: 0.3\t Average Accuracy: 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### param_over = 0.1, param_under = 0.9"
      ],
      "metadata": {
        "id": "ed5I5PynRAEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = 0.1, param_under = 0.9)"
      ],
      "metadata": {
        "id": "MCiLA9R1RCqE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a50230a1-95eb-426a-c21b-b6f1eea96090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 611\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 11  47   0]\n",
            " [  1 208   2]\n",
            " [  0  33   9]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.92      0.19      0.31        58\n",
            "      medium       0.72      0.99      0.83       211\n",
            "        high       0.82      0.21      0.34        42\n",
            "\n",
            "    accuracy                           0.73       311\n",
            "   macro avg       0.82      0.46      0.50       311\n",
            "weighted avg       0.77      0.73      0.67       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.692\tKappa: 0.258\tAccuracy: 0.733\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 834\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 39  47   1]\n",
            " [ 23 206  17]\n",
            " [  1  57  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.62      0.45      0.52        87\n",
            "      medium       0.66      0.84      0.74       246\n",
            "        high       0.57      0.29      0.39        82\n",
            "\n",
            "    accuracy                           0.65       415\n",
            "   macro avg       0.62      0.53      0.55       415\n",
            "weighted avg       0.64      0.65      0.62       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.717\tKappa: 0.304\tAccuracy: 0.648\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 600\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 39  33   0]\n",
            " [ 25 133  15]\n",
            " [  0  28  25]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.61      0.54      0.57        72\n",
            "      medium       0.69      0.77      0.72       173\n",
            "        high       0.62      0.47      0.54        53\n",
            "\n",
            "    accuracy                           0.66       298\n",
            "   macro avg       0.64      0.59      0.61       298\n",
            "weighted avg       0.66      0.66      0.65       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.765\tKappa: 0.38\tAccuracy: 0.661\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 573\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 28  42   3]\n",
            " [  8 128  24]\n",
            " [  1  24  26]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.76      0.38      0.51        73\n",
            "      medium       0.66      0.80      0.72       160\n",
            "        high       0.49      0.51      0.50        51\n",
            "\n",
            "    accuracy                           0.64       284\n",
            "   macro avg       0.64      0.56      0.58       284\n",
            "weighted avg       0.65      0.64      0.63       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.738\tKappa: 0.345\tAccuracy: 0.641\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.728\t Average Kappa: 0.322\t Average Accuracy: 0.671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Embeddings + features + balanceo de clases + PCA"
      ],
      "metadata": {
        "id": "gzCUbD2ZRdJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta secci贸n se presentan los experimentos de haber probado con diferentes combinaciones del par谩metro `n_components`."
      ],
      "metadata": {
        "id": "vRKOwS1eR35A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### n_components = 20"
      ],
      "metadata": {
        "id": "9cghH8soTQWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding_pca', Pipeline([('embedding', Doc2Vector(tokenizer = tokenizador)),\n",
        "                                                            ('pca', PCA(n_components = 20)),\n",
        "                                                            ])), \n",
        "                                  ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "VIG0_BvDTY9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = False, param_under = False)"
      ],
      "metadata": {
        "id": "2iEw9Sn7TcDX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca37720a-f58a-46e5-f5be-4324acde3020"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 11  47   0]\n",
            " [  3 202   6]\n",
            " [  0  25  17]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.79      0.19      0.31        58\n",
            "      medium       0.74      0.96      0.83       211\n",
            "        high       0.74      0.40      0.52        42\n",
            "\n",
            "    accuracy                           0.74       311\n",
            "   macro avg       0.75      0.52      0.55       311\n",
            "weighted avg       0.75      0.74      0.69       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.727\tKappa: 0.322\tAccuracy: 0.74\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 45  42   0]\n",
            " [ 19 206  21]\n",
            " [  1  57  24]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.69      0.52      0.59        87\n",
            "      medium       0.68      0.84      0.75       246\n",
            "        high       0.53      0.29      0.38        82\n",
            "\n",
            "    accuracy                           0.66       415\n",
            "   macro avg       0.63      0.55      0.57       415\n",
            "weighted avg       0.65      0.66      0.64       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.731\tKappa: 0.339\tAccuracy: 0.663\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 37  35   0]\n",
            " [ 17 142  14]\n",
            " [  0  20  33]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.69      0.51      0.59        72\n",
            "      medium       0.72      0.82      0.77       173\n",
            "        high       0.70      0.62      0.66        53\n",
            "\n",
            "    accuracy                           0.71       298\n",
            "   macro avg       0.70      0.65      0.67       298\n",
            "weighted avg       0.71      0.71      0.70       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.774\tKappa: 0.47\tAccuracy: 0.711\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 38  33   2]\n",
            " [ 15 120  25]\n",
            " [  1  31  19]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.70      0.52      0.60        73\n",
            "      medium       0.65      0.75      0.70       160\n",
            "        high       0.41      0.37      0.39        51\n",
            "\n",
            "    accuracy                           0.62       284\n",
            "   macro avg       0.59      0.55      0.56       284\n",
            "weighted avg       0.62      0.62      0.62       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.736\tKappa: 0.324\tAccuracy: 0.623\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.742\t Average Kappa: 0.364\t Average Accuracy: 0.684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = 0.1, param_under = 0.9)"
      ],
      "metadata": {
        "id": "7nlBlfKnThuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad69c235-3543-47df-f8a6-cfe2ba712bd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 611\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 16  42   0]\n",
            " [  3 201   7]\n",
            " [  0  27  15]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.84      0.28      0.42        58\n",
            "      medium       0.74      0.95      0.84       211\n",
            "        high       0.68      0.36      0.47        42\n",
            "\n",
            "    accuracy                           0.75       311\n",
            "   macro avg       0.76      0.53      0.57       311\n",
            "weighted avg       0.75      0.75      0.71       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.732\tKappa: 0.349\tAccuracy: 0.746\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 834\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 47  39   1]\n",
            " [ 29 191  26]\n",
            " [  2  47  33]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.60      0.54      0.57        87\n",
            "      medium       0.69      0.78      0.73       246\n",
            "        high       0.55      0.40      0.46        82\n",
            "\n",
            "    accuracy                           0.65       415\n",
            "   macro avg       0.61      0.57      0.59       415\n",
            "weighted avg       0.64      0.65      0.64       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.748\tKappa: 0.353\tAccuracy: 0.653\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 600\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 43  29   0]\n",
            " [ 31 125  17]\n",
            " [  0  21  32]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.58      0.60      0.59        72\n",
            "      medium       0.71      0.72      0.72       173\n",
            "        high       0.65      0.60      0.63        53\n",
            "\n",
            "    accuracy                           0.67       298\n",
            "   macro avg       0.65      0.64      0.64       298\n",
            "weighted avg       0.67      0.67      0.67       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.766\tKappa: 0.423\tAccuracy: 0.671\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 573\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 41  31   1]\n",
            " [ 21 111  28]\n",
            " [  2  22  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.64      0.56      0.60        73\n",
            "      medium       0.68      0.69      0.69       160\n",
            "        high       0.48      0.53      0.50        51\n",
            "\n",
            "    accuracy                           0.63       284\n",
            "   macro avg       0.60      0.59      0.60       284\n",
            "weighted avg       0.63      0.63      0.63       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.74\tKappa: 0.364\tAccuracy: 0.63\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.746\t Average Kappa: 0.372\t Average Accuracy: 0.675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####n_components = 15"
      ],
      "metadata": {
        "id": "UUg-raAETT2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding_pca', Pipeline([('embedding', Doc2Vector(tokenizer = tokenizador)),\n",
        "                                                            ('pca', PCA(n_components = 15)),\n",
        "                                                            ])), \n",
        "                                  ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "CsflxiujTTea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = 0.1, param_under = 0.9)"
      ],
      "metadata": {
        "id": "xOyfYJJSTq2k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "218053cc-f5c7-45d6-822a-2a94ccb78118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 611\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 15  43   0]\n",
            " [  5 200   6]\n",
            " [  0  23  19]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.75      0.26      0.38        58\n",
            "      medium       0.75      0.95      0.84       211\n",
            "        high       0.76      0.45      0.57        42\n",
            "\n",
            "    accuracy                           0.75       311\n",
            "   macro avg       0.75      0.55      0.60       311\n",
            "weighted avg       0.75      0.75      0.72       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.727\tKappa: 0.376\tAccuracy: 0.752\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 834\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 48  38   1]\n",
            " [ 29 194  23]\n",
            " [  5  46  31]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.59      0.55      0.57        87\n",
            "      medium       0.70      0.79      0.74       246\n",
            "        high       0.56      0.38      0.45        82\n",
            "\n",
            "    accuracy                           0.66       415\n",
            "   macro avg       0.62      0.57      0.59       415\n",
            "weighted avg       0.65      0.66      0.65       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.746\tKappa: 0.361\tAccuracy: 0.658\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 600\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 44  28   0]\n",
            " [ 29 128  16]\n",
            " [  1  21  31]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.59      0.61      0.60        72\n",
            "      medium       0.72      0.74      0.73       173\n",
            "        high       0.66      0.58      0.62        53\n",
            "\n",
            "    accuracy                           0.68       298\n",
            "   macro avg       0.66      0.65      0.65       298\n",
            "weighted avg       0.68      0.68      0.68       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.79\tKappa: 0.438\tAccuracy: 0.681\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 573\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 38  34   1]\n",
            " [ 21 108  31]\n",
            " [  1  19  31]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.63      0.52      0.57        73\n",
            "      medium       0.67      0.68      0.67       160\n",
            "        high       0.49      0.61      0.54        51\n",
            "\n",
            "    accuracy                           0.62       284\n",
            "   macro avg       0.60      0.60      0.60       284\n",
            "weighted avg       0.63      0.62      0.62       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.747\tKappa: 0.358\tAccuracy: 0.623\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.752\t Average Kappa: 0.383\t Average Accuracy: 0.679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####n_components = 10"
      ],
      "metadata": {
        "id": "8YVWK3_1TVtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding_pca', Pipeline([('embedding', Doc2Vector(tokenizer = tokenizador)),\n",
        "                                                            ('pca', PCA(n_components = 15)),\n",
        "                                                            ])), \n",
        "                                  ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "3-AxgX4lTYl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(get_pipeline(), param_over = 0.1, param_under = 0.9)"
      ],
      "metadata": {
        "id": "9W3oIYmGT8jK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "324c0ca6-9693-49c3-9fca-28ff3a67b550"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 611\n",
            "# Datos de testing en dataset anger: 311\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[ 16  42   0]\n",
            " [  5 201   5]\n",
            " [  0  24  18]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.76      0.28      0.41        58\n",
            "      medium       0.75      0.95      0.84       211\n",
            "        high       0.78      0.43      0.55        42\n",
            "\n",
            "    accuracy                           0.76       311\n",
            "   macro avg       0.77      0.55      0.60       311\n",
            "weighted avg       0.76      0.76      0.72       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.729\tKappa: 0.381\tAccuracy: 0.756\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 834\n",
            "# Datos de testing en dataset fear: 415\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 47  38   2]\n",
            " [ 27 194  25]\n",
            " [  3  49  30]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.61      0.54      0.57        87\n",
            "      medium       0.69      0.79      0.74       246\n",
            "        high       0.53      0.37      0.43        82\n",
            "\n",
            "    accuracy                           0.65       415\n",
            "   macro avg       0.61      0.56      0.58       415\n",
            "weighted avg       0.64      0.65      0.64       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.738\tKappa: 0.348\tAccuracy: 0.653\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 600\n",
            "# Datos de testing en dataset joy: 298\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 43  29   0]\n",
            " [ 35 120  18]\n",
            " [  1  20  32]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.54      0.60      0.57        72\n",
            "      medium       0.71      0.69      0.70       173\n",
            "        high       0.64      0.60      0.62        53\n",
            "\n",
            "    accuracy                           0.65       298\n",
            "   macro avg       0.63      0.63      0.63       298\n",
            "weighted avg       0.66      0.65      0.66       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.778\tKappa: 0.401\tAccuracy: 0.654\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 573\n",
            "# Datos de testing en dataset sadness: 284\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[ 40  32   1]\n",
            " [ 19 109  32]\n",
            " [  0  24  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.68      0.55      0.61        73\n",
            "      medium       0.66      0.68      0.67       160\n",
            "        high       0.45      0.53      0.49        51\n",
            "\n",
            "    accuracy                           0.62       284\n",
            "   macro avg       0.60      0.59      0.59       284\n",
            "weighted avg       0.63      0.62      0.62       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.746\tKappa: 0.346\tAccuracy: 0.62\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.748\t Average Kappa: 0.369\t Average Accuracy: 0.671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Embeddings entrenados con la data disponible**"
      ],
      "metadata": {
        "id": "nosZAw48L7re"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este apartado se utilizar谩n embeddings entrenados con los mismos corpus de los sentimientos. La idea que hay por detr谩s es generar representaciones de tokens dentro de un mismo sentimiento porque al tener todas un contexto parecido\n",
        "har谩 que el vector que represente cada palabra sea m谩s parecido a las palabras que ve constantemente en su ventana *k* dimensional, pero en el contexto del sentimiento, por lo que ser谩 una representaci贸n m谩s a la medida de la palabra, dado su contexto (sentimiento). Esto se contrasta con los embeddings de glove, los cuales fueron entrenados con corpus gigantescos, lo cual genera representaciones de palabras mucho m谩s \"robustas\" ya que contienen informaci贸n de muchos m谩s contextos en los cuales se han visto esas palabras juntas, lo que genera una vector mucho m谩s \"general\" o \"sem谩ntico\" de la palabra.\n",
        "La utilizaci贸n de embeddings por corpus se justifica debido a que se asume que la competencia es clara en que el desaf铆o consiste en clasificar la intensidad de un tweet mediante las clases \"low\", \"medium\" y \"high\", **sabiendo** la categor铆a o sentimiento al cual corresponde el documento a clasificar, y que por lo tanto, tiene sentido generar representaciones de palabras dentro de un mismo sentimiento. **Es cierto que los embedding tienen un sesgo**, pero es un sesgo que se genera a prop贸sito en este caso, porque tenemos informaci贸n sobre el sentimiento al cual pertenece el documento que vamos a predecir y al final a la task de clasificar, que es la que nos compete, no le interesa de donde se consiguieron los embeddings si es que estoy teniendo buenos resultados con ellos.\n",
        "\n",
        "Partiremos experimentando s贸lo con los embeddings, y luego le iremos agregando transformaciones vistas anteriormente. En el siguiente c贸digo se generan los embeddings para cada sentimiento, para esto ser谩 necesario crear una clase que transforme un documento en un solo vector de embedding."
      ],
      "metadata": {
        "id": "VducZwpwMkTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseFeature(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "class Doc2VecTransformer(BaseFeature):\n",
        "    \"\"\" Transforma tweets a representaciones vectoriales usando alg煤n modelo de Word Embeddings.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, aggregation_func):\n",
        "        # extraemos los embeddings desde el objeto contenedor. ojo con esta parte.\n",
        "        self.model = model.wv \n",
        "        \n",
        "        # indicamos la funci贸n de agregaci贸n (np.min, np.max, np.mean, np.sum, ...)\n",
        "        self.aggregation_func = aggregation_func\n",
        "\n",
        "    def simple_tokenizer(self, doc, lower=False):\n",
        "        \"\"\"Tokenizador. Elimina signos de puntuaci贸n, lleva las letras a min煤scula(opcional) y \n",
        "           separa el tweet por espacios.\n",
        "        \"\"\"\n",
        "        return tokenizador.tokenize(doc)\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \n",
        "        doc_embeddings = []\n",
        "        \n",
        "        for doc in X:\n",
        "            tokens = self.simple_tokenizer(doc, lower = True) \n",
        "            \n",
        "            selected_wv = []\n",
        "            for token in tokens:\n",
        "                if token in self.model.vocab:\n",
        "                    selected_wv.append(self.model[token])\n",
        "                    \n",
        "            if len(selected_wv) > 0:\n",
        "                doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n",
        "                doc_embeddings.append(doc_embedding)\n",
        "            else: \n",
        "                print('No pude encontrar ning煤n embedding en el tweet: {}. Agregando vector de ceros.'.format(doc))\n",
        "                doc_embeddings.append(np.zeros(self.model.vector_size)) # la dimension del modelo \n",
        "\n",
        "        return np.array(doc_embeddings)"
      ],
      "metadata": {
        "id": "9HG16hiNiVIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se generaliza la funci贸n *get_pipeline* para que acepte como argumento el objeto Doc2Vector de cada sentimiento y a continuaci贸n se generan los embeddings para cada uno de ellos."
      ],
      "metadata": {
        "id": "gsOskZ49jlZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline(doc2vect):\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([\n",
        "                                   ('doc2vect', doc2vect)\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe"
      ],
      "metadata": {
        "id": "L6PID3yTYPVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generaci贸n de los embeddings por cada sentimiento.\n",
        "embeddingsSentiment = {}\n",
        "pipelines = {}\n",
        "for sentimiento, datos in train.items():\n",
        "  content = train[sentimiento]['tweet']\n",
        "  #se tokeniza el documento\n",
        "  cleaned_content = [TweetTokenizer(preserve_case=False, reduce_len=True).tokenize(documento) for documento in content]\n",
        "  #se buscan bigramas\n",
        "  phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) \n",
        "  #se tokenizan bigramas\n",
        "  bigram = Phraser(phrases)\n",
        "  sentences = bigram[cleaned_content]\n",
        "  #definir el modelo\n",
        "  biobio_w2v = Word2Vec(min_count=10,\n",
        "                        window=4,\n",
        "                        size=200,\n",
        "                        sample=6e-5,\n",
        "                        alpha=0.03,\n",
        "                        min_alpha=0.0007,\n",
        "                        negative=20,\n",
        "                        workers=multiprocessing.cpu_count())\n",
        "  #construir el vocabulario\n",
        "  biobio_w2v.build_vocab(sentences, progress_per=10000)\n",
        "  #entrenar el modelo\n",
        "  t = time()\n",
        "  biobio_w2v.train(sentences, total_examples=biobio_w2v.corpus_count, epochs=15, report_delay=10)\n",
        "  print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "  #se indicamos que termin贸 el entrenamiento\n",
        "  biobio_w2v.init_sims(replace=True)\n",
        "  embeddingsSentiment[sentimiento] = Doc2VecTransformer(biobio_w2v, np.mean)\n",
        "  pipelines[sentimiento] = get_pipeline(embeddingsSentiment[sentimiento])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmXV8L2Gc0_8",
        "outputId": "247df0cb-3726-4f06-840a-498d8dce1998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to train the model: 0.02 mins\n",
            "Time to train the model: 0.02 mins\n",
            "Time to train the model: 0.01 mins\n",
            "Time to train the model: 0.02 mins\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora se entrenar谩 un RandomForest con estos embeddings generador. Debido a como fue creada la funci贸n auxiliar *evaluate_model*, en donde recibe un s贸lo pipe, necesitaremos extenderla para que sea capaz de recibir un dictionario de pipes y los vaya eligiendo a medida que van pasando los sentimientos."
      ],
      "metadata": {
        "id": "vbtphMatdjms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  n_resample = dataset['category'].value_counts().mean()\n",
        "  NUM_SAMPLES = int(n_resample)\n",
        "  g = dataset.groupby('category')\n",
        "  dataset = pd.DataFrame(\n",
        "    g.apply(lambda x: x.sample(NUM_SAMPLES, replace=True).reset_index(drop=True))\n",
        "  ).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "ehYVMf8qoOco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train['anger']"
      ],
      "metadata": {
        "id": "4jKgOIbJErOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([train['anger']['tweet'], train['anger']['sentiment_intensity']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxTXJC-GFAlC",
        "outputId": "3f50c79f-1a7b-4736-a5f7-36a9ddbc47cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      How the fu*k! Who the heck! moved my fridge!.....\n",
              "1      So my Indian Uber driver just called someone t...\n",
              "2      @DPD_UK I asked for my parcel to be delivered ...\n",
              "3      so ef whichever butt wipe pulled the fire alar...\n",
              "4      Don't join @BTCare they put the phone down on ...\n",
              "                             ...                        \n",
              "936                                               medium\n",
              "937                                               medium\n",
              "938                                                  low\n",
              "939                                                  low\n",
              "940                                               medium\n",
              "Length: 1882, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([train['anger']['tweet'], train['anger']['sentiment_intensity']], axis = 1).groupby('sentiment_intensity')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEkdRw3dEaV3",
        "outputId": "c99e640e-dfae-43f5-936a-f6b604080f0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x7fca7dffacd0>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run(dataset, dataset_name, pipeline, param_over = False, param_under = False):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test, a煤n no se transforma de Strings a valores num茅ricos.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        dataset.tweet,\n",
        "        dataset.sentiment_intensity,\n",
        "        shuffle=True,\n",
        "        test_size=0.33,\n",
        "        random_state = 3380)\n",
        "\n",
        "    #Balanceo de clases\n",
        "    # X_train, y_train = resample(X_train, y_train, param_over, param_under)\n",
        "    n_resample = y_train.value_counts().mean()\n",
        "    NUM_SAMPLES = int(n_resample)\n",
        "    g = pd.concat([X_train, y_train], axis = 1).groupby('sentiment_intensity')\n",
        "    datasetResampleado = pd.DataFrame(\n",
        "      g.apply(lambda x: x.sample(NUM_SAMPLES, replace=True).reset_index(drop=True))\n",
        "    ).reset_index(drop=True)\n",
        "    X_train = datasetResampleado['tweet']\n",
        "    y_train = datasetResampleado['sentiment_intensity']\n",
        "    \n",
        "    print(f'# Datos de entrenamiento en dataset {dataset_name}: {len(X_train)}')\n",
        "    print(f'# Datos de testing en dataset {dataset_name}: {len(X_test)}')\n",
        "\n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline). \n",
        "    # En este caso el Bag of Words es el encargado de transformar de Strings a vectores num茅ricos.\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n",
        "    predicted_probabilities = pipeline.predict_proba(X_test)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "    \n",
        "    # Evaluamos:\n",
        "    scores = evaluate(predicted_probabilities, y_test, learned_labels, dataset_name)\n",
        "    return pipeline, learned_labels, scores"
      ],
      "metadata": {
        "id": "gxL7SxJUDJL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(pipeline, param_over = False, param_under = False):\n",
        "  classifiers = []\n",
        "  learned_labels_array = []\n",
        "  scores_array = []\n",
        "\n",
        "  # Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\n",
        "  for dataset_name, dataset in train.items():\n",
        "    #resampleado de dataset para balancear clases.\n",
        "    # n_resample = dataset['sentiment_intensity'].value_counts().mean()\n",
        "    # NUM_SAMPLES = int(n_resample)\n",
        "    # g = dataset.groupby('sentiment_intensity')\n",
        "    # datasetResampleado = pd.DataFrame(\n",
        "    #   g.apply(lambda x: x.sample(NUM_SAMPLES, replace=True).reset_index(drop=True))\n",
        "    # ).reset_index(drop=True)\n",
        "    # creamos el pipeline\n",
        "    if isinstance(pipeline, dict):\n",
        "      pipeline = pipeline[dataset_name]\n",
        "    else:\n",
        "      pipeline = pipeline\n",
        "    \n",
        "    # ejecutamos el pipeline sobre el dataset\n",
        "    classifier, learned_labels, scores = run(dataset, dataset_name, pipeline, param_over, param_under)\n",
        "    accuracy = scores[2]\n",
        "    if accuracy > best_models[dataset_name][1]: #si accuracy del modelo es mejor que el mejor accuracy logrado\n",
        "      best_models.update({dataset_name: (classifier, accuracy)})\n",
        "\n",
        "    # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n",
        "    classifiers.append(classifier)\n",
        "\n",
        "    # guardamos las labels aprendidas por el clasificador\n",
        "    learned_labels_array.append(learned_labels)\n",
        "\n",
        "    # guardamos los scores obtenidos\n",
        "    scores_array.append(scores)\n",
        "\n",
        "  # print avg scores\n",
        "  print(\n",
        "    \"Average scores:\\n\\n\",\n",
        "    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n",
        "    .format(*np.array(scores_array).mean(axis=0)))\n",
        "  return classifiers, learned_labels_array, scores_array"
      ],
      "metadata": {
        "id": "3CRy55Sigvo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(pipelines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omQGhAm3kY86",
        "outputId": "af55358e-5eb3-4449-c396-2980a6035b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @TzumiXIV huff puff. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Bloody parking ticket  . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: #Hudcomedy #AdamRowe  #insult Slutfaceshlongnugget. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  2  56   0]\n",
            " [  5 201   5]\n",
            " [  0  33   9]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.29      0.03      0.06        58\n",
            "      medium       0.69      0.95      0.80       211\n",
            "        high       0.64      0.21      0.32        42\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.54      0.40      0.40       311\n",
            "weighted avg       0.61      0.68      0.60       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.585\tKappa: 0.109\tAccuracy: 0.682\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "No pude encontrar ning煤n embedding en el tweet: @lukeshawtime terrible. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: dread pitt. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Rooney shocking attempted cross. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @CSTrey4 thanks brotein shake . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @ImJim_YoureNot  cyber bully. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Penny dreadful 3 temporada. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: #smackdev #ptp #start word. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: shake fries. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @casillasbreanna awe thanks girl . Agregando vector de ceros.\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 12  72   3]\n",
            " [ 18 213  15]\n",
            " [  0  66  16]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.40      0.14      0.21        87\n",
            "      medium       0.61      0.87      0.71       246\n",
            "        high       0.47      0.20      0.28        82\n",
            "\n",
            "    accuracy                           0.58       415\n",
            "   macro avg       0.49      0.40      0.40       415\n",
            "weighted avg       0.54      0.58      0.52       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.548\tKappa: 0.103\tAccuracy: 0.581\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "No pude encontrar ning煤n embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @weebtard sparkling water wyd. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: 4-2 Canada final tomorrow #WCH #Predictions #optimism #Canadian . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @jimadair3 Guitar shop owners everywhere rejoice. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LeBatardShow #heyday Race war 2016. Agregando vector de ceros.\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[ 14  57   1]\n",
            " [ 11 150  12]\n",
            " [  2  36  15]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.52      0.19      0.28        72\n",
            "      medium       0.62      0.87      0.72       173\n",
            "        high       0.54      0.28      0.37        53\n",
            "\n",
            "    accuracy                           0.60       298\n",
            "   macro avg       0.56      0.45      0.46       298\n",
            "weighted avg       0.58      0.60      0.55       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.657\tKappa: 0.182\tAccuracy: 0.601\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "No pude encontrar ning煤n embedding en el tweet: Ffs dreadful defending. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RossKemp great programme tonight #sad #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RossKemp great programme tonight  #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @Hails_Berry8 @sajedhariri23 varsity pine riding. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither  #unwoke. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Baaarissshhhhh + sad song =  prefect night  feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @KatelynKolsrud thanks mucho kate #sober. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RyhenMessedUp goodbye despair. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: untypical kinda Friday #dull. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither #lost #unwoke. Agregando vector de ceros.\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  7  58   8]\n",
            " [  4 135  21]\n",
            " [  0  39  12]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.64      0.10      0.17        73\n",
            "      medium       0.58      0.84      0.69       160\n",
            "        high       0.29      0.24      0.26        51\n",
            "\n",
            "    accuracy                           0.54       284\n",
            "   macro avg       0.50      0.39      0.37       284\n",
            "weighted avg       0.54      0.54      0.48       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.593\tKappa: 0.092\tAccuracy: 0.542\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.596\t Average Kappa: 0.121\t Average Accuracy: 0.601\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# con resampleado\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(pipelines)"
      ],
      "metadata": {
        "id": "vWE5GJxQo47Z",
        "outputId": "ff93e28d-01db-4476-8cdc-53a1ee8db181",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 629\n",
            "# Datos de testing en dataset anger: 310\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: #Hudcomedy #AdamRowe  #insult Slutfaceshlongnugget. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[84 25  4]\n",
            " [ 8 83 11]\n",
            " [ 1 13 81]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.90      0.74      0.82       113\n",
            "      medium       0.69      0.81      0.74       102\n",
            "        high       0.84      0.85      0.85        95\n",
            "\n",
            "    accuracy                           0.80       310\n",
            "   macro avg       0.81      0.80      0.80       310\n",
            "weighted avg       0.81      0.80      0.80       310\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.937\tKappa: 0.7\tAccuracy: 0.8\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "No pude encontrar ning煤n embedding en el tweet: Penny dreadful 3 temporada. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 99  38   9]\n",
            " [ 23  83  20]\n",
            " [  4  20 119]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.79      0.68      0.73       146\n",
            "      medium       0.59      0.66      0.62       126\n",
            "        high       0.80      0.83      0.82       143\n",
            "\n",
            "    accuracy                           0.73       415\n",
            "   macro avg       0.73      0.72      0.72       415\n",
            "weighted avg       0.73      0.73      0.73       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.896\tKappa: 0.588\tAccuracy: 0.725\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 603\n",
            "# Datos de testing en dataset joy: 297\n",
            "No pude encontrar ning煤n embedding en el tweet: @LeBatardShow #heyday Race war 2016. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @weebtard sparkling water wyd. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @weebtard sparkling water wyd. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[75 19 11]\n",
            " [18 64 19]\n",
            " [ 4 14 73]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.77      0.71      0.74       105\n",
            "      medium       0.66      0.63      0.65       101\n",
            "        high       0.71      0.80      0.75        91\n",
            "\n",
            "    accuracy                           0.71       297\n",
            "   macro avg       0.71      0.72      0.71       297\n",
            "weighted avg       0.71      0.71      0.71       297\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.898\tKappa: 0.571\tAccuracy: 0.714\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 574\n",
            "# Datos de testing en dataset sadness: 284\n",
            "No pude encontrar ning煤n embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Baaarissshhhhh + sad song =  prefect night  feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Ffs dreadful defending. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Baaarissshhhhh + sad song =  prefect night  feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @Hails_Berry8 @sajedhariri23 varsity pine riding. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @Hails_Berry8 @sajedhariri23 varsity pine riding. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: untypical kinda Friday #dull. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither #lost #unwoke. Agregando vector de ceros.\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[69 27 10]\n",
            " [11 67 14]\n",
            " [ 2 11 73]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.84      0.65      0.73       106\n",
            "      medium       0.64      0.73      0.68        92\n",
            "        high       0.75      0.85      0.80        86\n",
            "\n",
            "    accuracy                           0.74       284\n",
            "   macro avg       0.74      0.74      0.74       284\n",
            "weighted avg       0.75      0.74      0.74       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.89\tKappa: 0.605\tAccuracy: 0.736\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.905\t Average Kappa: 0.616\t Average Accuracy: 0.744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# con con over_param y under_param\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(pipelines, param_over = 0.5, param_under = 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tgDtX5NVCEpb",
        "outputId": "0170ce99-74e4-45a9-9428-6177a38a080d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 538\n",
            "# Datos de testing en dataset anger: 311\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Bloody parking ticket  . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: #Hudcomedy #AdamRowe  #insult Slutfaceshlongnugget. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  7  45   6]\n",
            " [ 15 170  26]\n",
            " [  2  20  20]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.29      0.12      0.17        58\n",
            "      medium       0.72      0.81      0.76       211\n",
            "        high       0.38      0.48      0.43        42\n",
            "\n",
            "    accuracy                           0.63       311\n",
            "   macro avg       0.47      0.47      0.45       311\n",
            "weighted avg       0.60      0.63      0.61       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.636\tKappa: 0.186\tAccuracy: 0.633\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 809\n",
            "# Datos de testing en dataset fear: 415\n",
            "No pude encontrar ning煤n embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @CSTrey4 thanks brotein shake . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: dread pitt. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Rooney shocking attempted cross. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: #smackdev #ptp #start word. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: shake fries. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @casillasbreanna awe thanks girl . Agregando vector de ceros.\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[33 32 22]\n",
            " [91 94 61]\n",
            " [20 30 32]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.23      0.38      0.29        87\n",
            "      medium       0.60      0.38      0.47       246\n",
            "        high       0.28      0.39      0.32        82\n",
            "\n",
            "    accuracy                           0.38       415\n",
            "   macro avg       0.37      0.38      0.36       415\n",
            "weighted avg       0.46      0.38      0.40       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.557\tKappa: 0.051\tAccuracy: 0.383\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 590\n",
            "# Datos de testing en dataset joy: 298\n",
            "No pude encontrar ning煤n embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @weebtard sparkling water wyd. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: 4-2 Canada final tomorrow #WCH #Predictions #optimism #Canadian . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @jimadair3 Guitar shop owners everywhere rejoice. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LeBatardShow #heyday Race war 2016. Agregando vector de ceros.\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[50 16  6]\n",
            " [69 62 42]\n",
            " [15 12 26]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.37      0.69      0.49        72\n",
            "      medium       0.69      0.36      0.47       173\n",
            "        high       0.35      0.49      0.41        53\n",
            "\n",
            "    accuracy                           0.46       298\n",
            "   macro avg       0.47      0.51      0.46       298\n",
            "weighted avg       0.55      0.46      0.46       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.628\tKappa: 0.201\tAccuracy: 0.463\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 570\n",
            "# Datos de testing en dataset sadness: 284\n",
            "No pude encontrar ning煤n embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Baaarissshhhhh + sad song =  prefect night  feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @Hails_Berry8 @sajedhariri23 varsity pine riding. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither  #unwoke. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RossKemp great programme tonight #sad #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @KatelynKolsrud thanks mucho kate #sober. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RyhenMessedUp goodbye despair. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: untypical kinda Friday #dull. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither #lost #unwoke. Agregando vector de ceros.\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[26 15 32]\n",
            " [50 43 67]\n",
            " [18 13 20]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.28      0.36      0.31        73\n",
            "      medium       0.61      0.27      0.37       160\n",
            "        high       0.17      0.39      0.24        51\n",
            "\n",
            "    accuracy                           0.31       284\n",
            "   macro avg       0.35      0.34      0.31       284\n",
            "weighted avg       0.44      0.31      0.33       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.549\tKappa: 0.017\tAccuracy: 0.313\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.593\t Average Kappa: 0.114\t Average Accuracy: 0.448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# con con over_param y under_param\n",
        "output_classifiers, output_labels, output_scores = evaluate_model(pipelines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gANNTn7vEG0i",
        "outputId": "afa9c03a-4632-4c80-ef8f-81a0abaacce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Bloody parking ticket  . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: #Hudcomedy #AdamRowe  #insult Slutfaceshlongnugget. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  9  46   3]\n",
            " [ 20 158  33]\n",
            " [  1  21  20]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.30      0.16      0.20        58\n",
            "      medium       0.70      0.75      0.72       211\n",
            "        high       0.36      0.48      0.41        42\n",
            "\n",
            "    accuracy                           0.60       311\n",
            "   macro avg       0.45      0.46      0.45       311\n",
            "weighted avg       0.58      0.60      0.58       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.575\tKappa: 0.146\tAccuracy: 0.601\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 840\n",
            "# Datos de testing en dataset fear: 415\n",
            "No pude encontrar ning煤n embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @CSTrey4 thanks brotein shake . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @CSTrey4 thanks brotein shake . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Penny dreadful 3 temporada. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: #smackdev #ptp #start word. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: shake fries. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @casillasbreanna awe thanks girl . Agregando vector de ceros.\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 22  46  19]\n",
            " [ 45 151  50]\n",
            " [  7  48  27]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.30      0.25      0.27        87\n",
            "      medium       0.62      0.61      0.62       246\n",
            "        high       0.28      0.33      0.30        82\n",
            "\n",
            "    accuracy                           0.48       415\n",
            "   macro avg       0.40      0.40      0.40       415\n",
            "weighted avg       0.48      0.48      0.48       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.558\tKappa: 0.086\tAccuracy: 0.482\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 603\n",
            "# Datos de testing en dataset joy: 298\n",
            "No pude encontrar ning煤n embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: 4-2 Canada final tomorrow #WCH #Predictions #optimism #Canadian . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @jimadair3 Guitar shop owners everywhere rejoice. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LeBatardShow #heyday Race war 2016. Agregando vector de ceros.\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[36 33  3]\n",
            " [49 95 29]\n",
            " [ 9 26 18]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.38      0.50      0.43        72\n",
            "      medium       0.62      0.55      0.58       173\n",
            "        high       0.36      0.34      0.35        53\n",
            "\n",
            "    accuracy                           0.50       298\n",
            "   macro avg       0.45      0.46      0.45       298\n",
            "weighted avg       0.51      0.50      0.50       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.614\tKappa: 0.158\tAccuracy: 0.5\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "No pude encontrar ning煤n embedding en el tweet: Baaarissshhhhh + sad song =  prefect night  feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Baaarissshhhhh + sad song =  prefect night  feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Baaarissshhhhh + sad song =  prefect night  feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Ffs dreadful defending. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RossKemp great programme tonight #sad #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither  #unwoke. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RossKemp great programme tonight  #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @KatelynKolsrud thanks mucho kate #sober. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RyhenMessedUp goodbye despair. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: untypical kinda Friday #dull. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither #lost #unwoke. Agregando vector de ceros.\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[15 37 21]\n",
            " [35 75 50]\n",
            " [ 9 21 21]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.25      0.21      0.23        73\n",
            "      medium       0.56      0.47      0.51       160\n",
            "        high       0.23      0.41      0.29        51\n",
            "\n",
            "    accuracy                           0.39       284\n",
            "   macro avg       0.35      0.36      0.34       284\n",
            "weighted avg       0.42      0.39      0.40       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.54\tKappa: 0.025\tAccuracy: 0.391\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.572\t Average Kappa: 0.104\t Average Accuracy: 0.493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se observa que los resultados no son los mejores, si se comparan con los obtenidos en los experimentos anteriores. Se probar谩 agregando el bag of words para ver c贸mo responde el modelo."
      ],
      "metadata": {
        "id": "oD9BfI_RnMWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline(doc2vect):\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([\n",
        "                                   ('bow', CountVectorizer(analyzer='word', ngram_range=(1, 2))),\n",
        "                                   ('doc2vect', doc2vect)\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "for sentimiento, pipe in pipelines.items():\n",
        "  pipelines[sentimiento] = get_pipeline(embeddingsSentiment[sentimiento])"
      ],
      "metadata": {
        "id": "yACdZ-bqMS2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(pipelines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkaQ38fkoG5y",
        "outputId": "ceb6c19d-c98b-40cd-a850-0c4f09f9a886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @TzumiXIV huff puff. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Bloody parking ticket  . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: #Hudcomedy #AdamRowe  #insult Slutfaceshlongnugget. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  2  56   0]\n",
            " [  4 202   5]\n",
            " [  0  35   7]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.33      0.03      0.06        58\n",
            "      medium       0.69      0.96      0.80       211\n",
            "        high       0.58      0.17      0.26        42\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.54      0.39      0.37       311\n",
            "weighted avg       0.61      0.68      0.59       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.615\tKappa: 0.087\tAccuracy: 0.678\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "No pude encontrar ning煤n embedding en el tweet: @lukeshawtime terrible. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: dread pitt. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Rooney shocking attempted cross. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @CSTrey4 thanks brotein shake . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @ImJim_YoureNot  cyber bully. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Penny dreadful 3 temporada. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: #smackdev #ptp #start word. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: shake fries. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @casillasbreanna awe thanks girl . Agregando vector de ceros.\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 10  73   4]\n",
            " [ 14 218  14]\n",
            " [  2  67  13]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.38      0.11      0.18        87\n",
            "      medium       0.61      0.89      0.72       246\n",
            "        high       0.42      0.16      0.23        82\n",
            "\n",
            "    accuracy                           0.58       415\n",
            "   macro avg       0.47      0.39      0.38       415\n",
            "weighted avg       0.52      0.58      0.51       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.564\tKappa: 0.09\tAccuracy: 0.581\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "No pude encontrar ning煤n embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @weebtard sparkling water wyd. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: 4-2 Canada final tomorrow #WCH #Predictions #optimism #Canadian . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @jimadair3 Guitar shop owners everywhere rejoice. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LeBatardShow #heyday Race war 2016. Agregando vector de ceros.\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[  9  63   0]\n",
            " [  8 157   8]\n",
            " [  1  41  11]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.50      0.12      0.20        72\n",
            "      medium       0.60      0.91      0.72       173\n",
            "        high       0.58      0.21      0.31        53\n",
            "\n",
            "    accuracy                           0.59       298\n",
            "   macro avg       0.56      0.41      0.41       298\n",
            "weighted avg       0.57      0.59      0.52       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.641\tKappa: 0.128\tAccuracy: 0.594\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "No pude encontrar ning煤n embedding en el tweet: Ffs dreadful defending. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RossKemp great programme tonight #sad #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RossKemp great programme tonight  #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @Hails_Berry8 @sajedhariri23 varsity pine riding. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither  #unwoke. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Baaarissshhhhh + sad song =  prefect night  feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @KatelynKolsrud thanks mucho kate #sober. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RyhenMessedUp goodbye despair. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: untypical kinda Friday #dull. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither #lost #unwoke. Agregando vector de ceros.\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  4  66   3]\n",
            " [  6 141  13]\n",
            " [  0  44   7]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.40      0.05      0.10        73\n",
            "      medium       0.56      0.88      0.69       160\n",
            "        high       0.30      0.14      0.19        51\n",
            "\n",
            "    accuracy                           0.54       284\n",
            "   macro avg       0.42      0.36      0.32       284\n",
            "weighted avg       0.47      0.54      0.45       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.599\tKappa: 0.029\tAccuracy: 0.535\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.605\t Average Kappa: 0.0835\t Average Accuracy: 0.597\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se observa como mejoraron los resultados al agregar atributos que provienen del BoW. A continuaci贸n se experimentar谩 agregando las features creadas por el clase FeatureTransformer creada anteriormente."
      ],
      "metadata": {
        "id": "w-iyIAs8oMZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline(doc2vect):\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([\n",
        "                                   ('bow', CountVectorizer(analyzer='word', ngram_range=(1, 2))),\n",
        "                                   ('doc2vect', doc2vect),\n",
        "                                   ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "for sentimiento, pipe in pipelines.items():\n",
        "  pipelines[sentimiento] = get_pipeline(embeddingsSentiment[sentimiento])"
      ],
      "metadata": {
        "id": "2-FToeN9oawC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_classifiers, output_labels, output_scores = evaluate_model(pipelines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGpwyLUjokQ-",
        "outputId": "94d9cb73-1fc5-4b70-f980-8bf9d9027d81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Datos de entrenamiento en dataset anger: 630\n",
            "# Datos de testing en dataset anger: 311\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @TzumiXIV huff puff. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Bloody parking ticket  . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: #Hudcomedy #AdamRowe  #insult Slutfaceshlongnugget. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "Confusion Matrix for anger:\n",
            "\n",
            "[[  2  56   0]\n",
            " [  5 204   2]\n",
            " [  0  35   7]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.29      0.03      0.06        58\n",
            "      medium       0.69      0.97      0.81       211\n",
            "        high       0.78      0.17      0.27        42\n",
            "\n",
            "    accuracy                           0.68       311\n",
            "   macro avg       0.59      0.39      0.38       311\n",
            "weighted avg       0.63      0.68      0.60       311\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.648\tKappa: 0.095\tAccuracy: 0.685\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset fear: 842\n",
            "# Datos de testing en dataset fear: 415\n",
            "No pude encontrar ning煤n embedding en el tweet: @lukeshawtime terrible. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: dread pitt. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Rooney shocking attempted cross. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: T5ylw ansh a79l shy 7lw mn wayed nas fe whatsapp. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @CSTrey4 thanks brotein shake . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @ImJim_YoureNot  cyber bully. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Penny dreadful 3 temporada. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: #smackdev #ptp #start word. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: shake fries. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @casillasbreanna awe thanks girl . Agregando vector de ceros.\n",
            "Confusion Matrix for fear:\n",
            "\n",
            "[[ 15  69   3]\n",
            " [ 11 221  14]\n",
            " [  1  68  13]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.56      0.17      0.26        87\n",
            "      medium       0.62      0.90      0.73       246\n",
            "        high       0.43      0.16      0.23        82\n",
            "\n",
            "    accuracy                           0.60       415\n",
            "   macro avg       0.54      0.41      0.41       415\n",
            "weighted avg       0.57      0.60      0.53       415\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.613\tKappa: 0.132\tAccuracy: 0.6\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset joy: 604\n",
            "# Datos de testing en dataset joy: 298\n",
            "No pude encontrar ning煤n embedding en el tweet: f breezy. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LaurenBrierley2 sparkling water = death. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Happy Birthday @Brooke56_56  #cheerchick #jeep #jeepgirl #IDriveAJeep #jeepjeep #Cheer. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @weebtard sparkling water wyd. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: 4-2 Canada final tomorrow #WCH #Predictions #optimism #Canadian . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @jimadair3 Guitar shop owners everywhere rejoice. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @LeBatardShow #heyday Race war 2016. Agregando vector de ceros.\n",
            "Confusion Matrix for joy:\n",
            "\n",
            "[[  9  62   1]\n",
            " [ 10 155   8]\n",
            " [  1  37  15]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.45      0.12      0.20        72\n",
            "      medium       0.61      0.90      0.73       173\n",
            "        high       0.62      0.28      0.39        53\n",
            "\n",
            "    accuracy                           0.60       298\n",
            "   macro avg       0.56      0.43      0.44       298\n",
            "weighted avg       0.57      0.60      0.54       298\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.704\tKappa: 0.159\tAccuracy: 0.601\n",
            "------------------------------------------------------\n",
            "\n",
            "# Datos de entrenamiento en dataset sadness: 576\n",
            "# Datos de testing en dataset sadness: 284\n",
            "No pude encontrar ning煤n embedding en el tweet: Ffs dreadful defending. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: blues bar. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RossKemp great programme tonight #sad #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RossKemp great programme tonight  #upsetting #extremeworld. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @Hails_Berry8 @sajedhariri23 varsity pine riding. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither  #unwoke. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Baaarissshhhhh + sad song =  prefect night  feeling alone. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @Samkingftw nahh thats grim. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @khloe_speaks sad music. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @KatelynKolsrud thanks mucho kate #sober. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @RyhenMessedUp goodbye despair. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: untypical kinda Friday #dull. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: @raylewis name change Uncle Remus Lewis #foh #theydontlikeyoueither #lost #unwoke. Agregando vector de ceros.\n",
            "Confusion Matrix for sadness:\n",
            "\n",
            "[[  8  61   4]\n",
            " [  3 138  19]\n",
            " [  0  43   8]]\n",
            "\n",
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         low       0.73      0.11      0.19        73\n",
            "      medium       0.57      0.86      0.69       160\n",
            "        high       0.26      0.16      0.20        51\n",
            "\n",
            "    accuracy                           0.54       284\n",
            "   macro avg       0.52      0.38      0.36       284\n",
            "weighted avg       0.55      0.54      0.47       284\n",
            "\n",
            "Scores:\n",
            "\n",
            "AUC:  0.663\tKappa: 0.067\tAccuracy: 0.542\n",
            "------------------------------------------------------\n",
            "\n",
            "Average scores:\n",
            "\n",
            " Average AUC: 0.657\t Average Kappa: 0.113\t Average Accuracy: 0.607\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#variables n_resample, sentimiento, agg_func\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "clasificadores = {'anger': [],\n",
        "                  'fear': [],\n",
        "                  'joy': [],\n",
        "                  'sadness': []}\n",
        "\n",
        "bog_ = True\n",
        "for sentimiento in train.keys():\n",
        "  content = train[sentimiento]['tweet']\n",
        "  #se tokeniza el documento\n",
        "  cleaned_content = [tokenizador.tokenize(documento) for documento in content]\n",
        "  #se buscan bigramas\n",
        "  phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) \n",
        "  #se tokenizan bigramas\n",
        "  bigram = Phraser(phrases)\n",
        "  sentences = bigram[cleaned_content]\n",
        "  #definir el modelo\n",
        "  biobio_w2v = Word2Vec(min_count=10,\n",
        "                        window=4,\n",
        "                        size=200,\n",
        "                        sample=6e-5,\n",
        "                        alpha=0.03,\n",
        "                        min_alpha=0.0007,\n",
        "                        negative=20,\n",
        "                        workers=multiprocessing.cpu_count())\n",
        "  #construir el vocabulario\n",
        "  biobio_w2v.build_vocab(sentences, progress_per=10000)\n",
        "  #entrenar el modelo\n",
        "  t = time()\n",
        "  biobio_w2v.train(sentences, total_examples=biobio_w2v.corpus_count, epochs=15, report_delay=10)\n",
        "  print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))\n",
        "  #se indicamos que termin贸 el entrenamiento\n",
        "  biobio_w2v.init_sims(replace=True)\n",
        "\n",
        "  #problema de clasificaci贸n\n",
        "  #generaci贸n dataset\n",
        "  dataset = pd.DataFrame({'content': train[sentimiento]['tweet'], 'category': train[sentimiento]['sentiment_intensity']})\n",
        "  # dataset['category'].value_counts()\n",
        "  #resampleo\n",
        "  n_resample = dataset['category'].value_counts().mean()\n",
        "  NUM_SAMPLES = int(n_resample)\n",
        "  g = dataset.groupby('category')\n",
        "  dataset = pd.DataFrame(\n",
        "    g.apply(lambda x: x.sample(NUM_SAMPLES, replace=True).reset_index(drop=True))\n",
        "  ).reset_index(drop=True)\n",
        "  #train y test\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "  from google.colab import output\n",
        "  output.disable_custom_widget_manager()\n",
        "  X_train, X_test, y_train, y_test = train_test_split(dataset.content,\n",
        "                                                      dataset.category,\n",
        "                                                      test_size=0.33,\n",
        "                                                      random_state=3380, \n",
        "                                                      stratify = dataset.category)\n",
        "\n",
        "  #pipeline\n",
        "  clf = RandomForestClassifier(random_state = 3380)\n",
        "  # clf = XGBClassifier(n_estimators = 2000)\n",
        "  for agg_func in [('mean', np.mean)]:\n",
        "                  #  ('max', np.max), ('sum', np.sum)]:\n",
        "    doc2vec_func = Doc2VecTransformer(biobio_w2v, agg_func[1])\n",
        "    if bog_:\n",
        "      pipeline = Pipeline([('features',\n",
        "                        FeatureUnion([\n",
        "                                      # ('tf-idf', TfidfVectorizer()),\n",
        "                                      # ('bow', CountVectorizer(analyzer='word', ngram_range=(1, 2))),\n",
        "                                      ('doc2vec', doc2vec_func),\n",
        "                                      # ('features', FeaturesTransformer())\n",
        "                                      ])), \n",
        "                            ('clf', clf)])\n",
        "    else:\n",
        "      pipeline = Pipeline([('doc2vec', doc2vec_func), ('clf', clf)])\n",
        "\n",
        "    #fit\n",
        "    pipeline.fit(X_train, y_train)\n",
        "    y_pred = pipeline.predict(X_test)\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    print(f'\\nResultados sentimiento: {sentimiento} - Funci贸n de agregaci贸n {agg_func[0]}')\n",
        "    print(conf_matrix)\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    clasificadores[sentimiento].append(pipeline)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj2S2WVKZC1r",
        "outputId": "140b2a5b-02cc-45b0-ee99-12f7091b59c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time to train the model: 0.02 mins\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Drop Snapchat names #bored  #swap #pics. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: testing . Agregando vector de ceros.\n",
            "\n",
            "Resultados sentimiento: anger - Funci贸n de agregaci贸n mean\n",
            "[[89  0 14]\n",
            " [ 5 76 23]\n",
            " [13  8 82]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.83      0.86      0.85       103\n",
            "         low       0.90      0.73      0.81       104\n",
            "      medium       0.69      0.80      0.74       103\n",
            "\n",
            "    accuracy                           0.80       310\n",
            "   macro avg       0.81      0.80      0.80       310\n",
            "weighted avg       0.81      0.80      0.80       310\n",
            "\n",
            "Time to train the model: 0.02 mins\n",
            "\n",
            "Resultados sentimiento: fear - Funci贸n de agregaci贸n mean\n",
            "[[101  12  25]\n",
            " [ 10 105  24]\n",
            " [ 23  26  89]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.75      0.73      0.74       138\n",
            "         low       0.73      0.76      0.74       139\n",
            "      medium       0.64      0.64      0.64       138\n",
            "\n",
            "    accuracy                           0.71       415\n",
            "   macro avg       0.71      0.71      0.71       415\n",
            "weighted avg       0.71      0.71      0.71       415\n",
            "\n",
            "Time to train the model: 0.02 mins\n",
            "No pude encontrar ning煤n embedding en el tweet: Riggs dumb ass hell lolol  #LethalWeapon. Agregando vector de ceros.\n",
            "\n",
            "Resultados sentimiento: joy - Funci贸n de agregaci贸n mean\n",
            "[[81  3 15]\n",
            " [ 5 72 22]\n",
            " [ 7 18 74]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.87      0.82      0.84        99\n",
            "         low       0.77      0.73      0.75        99\n",
            "      medium       0.67      0.75      0.70        99\n",
            "\n",
            "    accuracy                           0.76       297\n",
            "   macro avg       0.77      0.76      0.77       297\n",
            "weighted avg       0.77      0.76      0.77       297\n",
            "\n",
            "Time to train the model: 0.02 mins\n",
            "No pude encontrar ning煤n embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "No pude encontrar ning煤n embedding en el tweet: Extreme sadness. Agregando vector de ceros.\n",
            "\n",
            "Resultados sentimiento: sadness - Funci贸n de agregaci贸n mean\n",
            "[[79  1 14]\n",
            " [ 4 70 21]\n",
            " [17  8 70]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        high       0.79      0.84      0.81        94\n",
            "         low       0.89      0.74      0.80        95\n",
            "      medium       0.67      0.74      0.70        95\n",
            "\n",
            "    accuracy                           0.77       284\n",
            "   macro avg       0.78      0.77      0.77       284\n",
            "weighted avg       0.78      0.77      0.77       284\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **B煤squeda de hiperpar谩metros**"
      ],
      "metadata": {
        "id": "bCd5lN52M4yZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta secci贸n realizaremos una b煤squeda de hiperpar谩metros a trav茅s de `HalvingGridSearchCV`. Para esto, usaremos como input la configuraciones 贸ptimas encontradas en la secci贸n anterior. Adem谩s, nos fijaremos en 3 modelos a ocupar: `RandomForestClassifier`, `XGBClassifier` y `SVC`."
      ],
      "metadata": {
        "id": "M1HupaA2NLRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = gensim.downloader.load('glove-twitter-200')\n",
        "pipeline = Pipeline([\n",
        "                     ('clf', RandomForestClassifier())\n",
        "                     ])"
      ],
      "metadata": {
        "id": "PHOvH4esz9lm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = [\n",
        "              # Random Forest\n",
        "              {'clf': [RandomForestClassifier(random_state = 3380)],\n",
        "               'clf__n_estimators': [100, 500, 1000],\n",
        "               'clf__bootstrap': [True],\n",
        "               'clf__max_depth': [80, 90, 100, 110],\n",
        "               'clf__max_features': [2, 3],\n",
        "               'clf__min_samples_leaf': [3, 4, 5],\n",
        "               'clf__min_samples_split': [8, 10, 12],\n",
        "               },\n",
        "              # XGBoost\n",
        "              {'clf': [XGBClassifier(random_state = 3380)],\n",
        "               \"clf__max_depth\": [3, 4, 5, 7],\n",
        "               \"clf__learning_rate\": [0.1, 0.01, 0.05],\n",
        "               \"clf__gamma\": [0, 0.25, 1],\n",
        "               \"clf__reg_lambda\": [0, 1, 10],\n",
        "               \"clf__scale_pos_weight\": [1, 3, 5],\n",
        "               \"clf__subsample\": [0.8],\n",
        "               \"clf__colsample_bytree\": [0.5],\n",
        "               },\n",
        "              # SVM\n",
        "              {'clf': [SVC(probability = True, random_state = 3380)],\n",
        "               'clf__C': [1, 10, 100, 1000],\n",
        "               'clf__kernel': ['linear', 'rbf', 'poly'],\n",
        "               'clf__gamma': [0.001, 0.0001]}\n",
        "]"
      ],
      "metadata": {
        "id": "gGPJ-Tta0scZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd0Bfonr1Ajp"
      },
      "source": [
        "#### **Anger**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn3XqDF_1Ajq"
      },
      "outputs": [],
      "source": [
        "grid = HalvingGridSearchCV(pipeline, param_grid, cv = 5, verbose = 1, scoring = 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXBbuu5W1Ajq"
      },
      "outputs": [],
      "source": [
        "X, y = resample(train['anger']['tweet'], train['anger']['sentiment_intensity'], param_over = False, param_under = False)\n",
        "embedding_anger = Doc2Vector(tokenizer = tokenizador).transform(X)\n",
        "embedding_anger = PCA(n_components = 15).fit_transform(embedding_anger)\n",
        "features_anger = FeaturesTransformer().transform(X)\n",
        "data_anger = np.hstack([embedding_anger, features_anger])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "704f1272-775c-4958-b431-53e7076c5eb2",
        "id": "AFTJVMGP1Ajr"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 4\n",
            "n_required_iterations: 6\n",
            "n_possible_iterations: 4\n",
            "min_resources_: 30\n",
            "max_resources_: 941\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 564\n",
            "n_resources: 30\n",
            "Fitting 5 folds for each of 564 candidates, totalling 2820 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 188\n",
            "n_resources: 90\n",
            "Fitting 5 folds for each of 188 candidates, totalling 940 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 63\n",
            "n_resources: 270\n",
            "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 21\n",
            "n_resources: 810\n",
            "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
          ]
        }
      ],
      "source": [
        "anger_model = grid.fit(data_anger, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "ZvVldMXt4QaU",
        "outputId": "5c3a2574-6961-4bd1-f04f-17ea7f2988f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
              "751     1           90       0.049146      0.004337         0.001262   \n",
              "748     1           90       0.038550      0.000296         0.001127   \n",
              "747     1           90       0.042893      0.004091         0.001242   \n",
              "746     1           90       0.041899      0.000848         0.001184   \n",
              "745     1           90       0.043049      0.001973         0.001140   \n",
              "..    ...          ...            ...           ...              ...   \n",
              "555     0           30       0.002431      0.000421         0.000556   \n",
              "552     0           30       0.002780      0.000586         0.000604   \n",
              "549     0           30       0.002454      0.000499         0.000533   \n",
              "546     0           30       0.002420      0.000420         0.000544   \n",
              "559     0           30       0.002140      0.000072         0.000534   \n",
              "\n",
              "     std_score_time                                          param_clf  \\\n",
              "751        0.000253  XGBClassifier(colsample_bytree=0.5, random_sta...   \n",
              "748        0.000044  XGBClassifier(colsample_bytree=0.5, random_sta...   \n",
              "747        0.000191  XGBClassifier(colsample_bytree=0.5, random_sta...   \n",
              "746        0.000069  XGBClassifier(colsample_bytree=0.5, random_sta...   \n",
              "745        0.000013  XGBClassifier(colsample_bytree=0.5, random_sta...   \n",
              "..              ...                                                ...   \n",
              "555        0.000018           SVC(probability=True, random_state=3380)   \n",
              "552        0.000133           SVC(probability=True, random_state=3380)   \n",
              "549        0.000016           SVC(probability=True, random_state=3380)   \n",
              "546        0.000009           SVC(probability=True, random_state=3380)   \n",
              "559        0.000018           SVC(probability=True, random_state=3380)   \n",
              "\n",
              "    param_clf__bootstrap param_clf__max_depth param_clf__max_features  ...  \\\n",
              "751                  NaN                    7                     NaN  ...   \n",
              "748                  NaN                    3                     NaN  ...   \n",
              "747                  NaN                    4                     NaN  ...   \n",
              "746                  NaN                    4                     NaN  ...   \n",
              "745                  NaN                    4                     NaN  ...   \n",
              "..                   ...                  ...                     ...  ...   \n",
              "555                  NaN                  NaN                     NaN  ...   \n",
              "552                  NaN                  NaN                     NaN  ...   \n",
              "549                  NaN                  NaN                     NaN  ...   \n",
              "546                  NaN                  NaN                     NaN  ...   \n",
              "559                  NaN                  NaN                     NaN  ...   \n",
              "\n",
              "    mean_test_score std_test_score rank_test_score split0_train_score  \\\n",
              "751        0.700654       0.138488               1                1.0   \n",
              "748        0.700654       0.138488               1                1.0   \n",
              "747        0.700654       0.138488               1                1.0   \n",
              "746        0.700654       0.138488               1                1.0   \n",
              "745        0.700654       0.138488               1                1.0   \n",
              "..              ...            ...             ...                ...   \n",
              "555        0.566667       0.240370             830                1.0   \n",
              "552        0.566667       0.240370             830                1.0   \n",
              "549        0.566667       0.240370             830                1.0   \n",
              "546        0.566667       0.240370             830                1.0   \n",
              "559        0.460000       0.149666             836                1.0   \n",
              "\n",
              "    split1_train_score split2_train_score split3_train_score  \\\n",
              "751                1.0                1.0           1.000000   \n",
              "748                1.0                1.0           1.000000   \n",
              "747                1.0                1.0           1.000000   \n",
              "746                1.0                1.0           1.000000   \n",
              "745                1.0                1.0           1.000000   \n",
              "..                 ...                ...                ...   \n",
              "555                1.0                1.0           1.000000   \n",
              "552                1.0                1.0           1.000000   \n",
              "549                1.0                1.0           1.000000   \n",
              "546                1.0                1.0           1.000000   \n",
              "559                1.0                1.0           0.958333   \n",
              "\n",
              "    split4_train_score mean_train_score std_train_score  \n",
              "751                1.0         1.000000        0.000000  \n",
              "748                1.0         1.000000        0.000000  \n",
              "747                1.0         1.000000        0.000000  \n",
              "746                1.0         1.000000        0.000000  \n",
              "745                1.0         1.000000        0.000000  \n",
              "..                 ...              ...             ...  \n",
              "555                1.0         1.000000        0.000000  \n",
              "552                1.0         1.000000        0.000000  \n",
              "549                1.0         1.000000        0.000000  \n",
              "546                1.0         1.000000        0.000000  \n",
              "559                1.0         0.991667        0.016667  \n",
              "\n",
              "[836 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-31e2ff97-2c6b-4cc5-8bec-639a09b4df12\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iter</th>\n",
              "      <th>n_resources</th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_clf</th>\n",
              "      <th>param_clf__bootstrap</th>\n",
              "      <th>param_clf__max_depth</th>\n",
              "      <th>param_clf__max_features</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>split1_train_score</th>\n",
              "      <th>split2_train_score</th>\n",
              "      <th>split3_train_score</th>\n",
              "      <th>split4_train_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>751</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.049146</td>\n",
              "      <td>0.004337</td>\n",
              "      <td>0.001262</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, random_sta...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.700654</td>\n",
              "      <td>0.138488</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>748</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.038550</td>\n",
              "      <td>0.000296</td>\n",
              "      <td>0.001127</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, random_sta...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.700654</td>\n",
              "      <td>0.138488</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>747</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.042893</td>\n",
              "      <td>0.004091</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, random_sta...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.700654</td>\n",
              "      <td>0.138488</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>746</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.041899</td>\n",
              "      <td>0.000848</td>\n",
              "      <td>0.001184</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, random_sta...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.700654</td>\n",
              "      <td>0.138488</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>745</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.043049</td>\n",
              "      <td>0.001973</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, random_sta...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.700654</td>\n",
              "      <td>0.138488</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002431</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000556</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.240370</td>\n",
              "      <td>830</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002780</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>0.000604</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.240370</td>\n",
              "      <td>830</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>549</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002454</td>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.000533</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.240370</td>\n",
              "      <td>830</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002420</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.240370</td>\n",
              "      <td>830</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>559</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002140</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000534</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.460000</td>\n",
              "      <td>0.149666</td>\n",
              "      <td>836</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.991667</td>\n",
              "      <td>0.016667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>836 rows  37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31e2ff97-2c6b-4cc5-8bec-639a09b4df12')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-31e2ff97-2c6b-4cc5-8bec-639a09b4df12 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-31e2ff97-2c6b-4cc5-8bec-639a09b4df12');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "ranking = pd.DataFrame(anger_model.cv_results_)\n",
        "ranking = ranking.sort_values('rank_test_score')\n",
        "ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UmFL2wU4nlo",
        "outputId": "63f4e1ab-f0ef-4853-e1dc-1b3463d83809"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf': XGBClassifier(colsample_bytree=0.5, random_state=3380, reg_lambda=0,\n",
              "               subsample=0.8),\n",
              " 'clf__colsample_bytree': 0.5,\n",
              " 'clf__gamma': 0,\n",
              " 'clf__learning_rate': 0.1,\n",
              " 'clf__max_depth': 3,\n",
              " 'clf__reg_lambda': 0,\n",
              " 'clf__scale_pos_weight': 1,\n",
              " 'clf__subsample': 0.8}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "anger_model.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xAgO-Qbw7Ev"
      },
      "source": [
        "####**Joy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D32G5chvxChd"
      },
      "outputs": [],
      "source": [
        "grid = HalvingGridSearchCV(pipeline, param_grid, cv = 5, verbose = 1, scoring = 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2nPyf_AeYV7"
      },
      "outputs": [],
      "source": [
        "X, y = resample(train['joy']['tweet'], train['joy']['sentiment_intensity'], param_over = False, param_under = False)\n",
        "embedding_joy = Doc2Vector(tokenizer = tokenizador).transform(X)\n",
        "embedding_joy = PCA(n_components = 15).fit_transform(embedding_joy)\n",
        "features_joy = FeaturesTransformer().transform(X)\n",
        "data_joy = np.hstack([embedding_joy, features_joy])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ri37ZVF3ZXs",
        "outputId": "ef778c88-c94d-49a4-d0e9-4ae9ecf7985b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 4\n",
            "n_required_iterations: 6\n",
            "n_possible_iterations: 4\n",
            "min_resources_: 30\n",
            "max_resources_: 902\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 564\n",
            "n_resources: 30\n",
            "Fitting 5 folds for each of 564 candidates, totalling 2820 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 188\n",
            "n_resources: 90\n",
            "Fitting 5 folds for each of 188 candidates, totalling 940 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 63\n",
            "n_resources: 270\n",
            "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 21\n",
            "n_resources: 810\n",
            "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
          ]
        }
      ],
      "source": [
        "joy_model = grid.fit(data_joy, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "qzBR2Kq0e3LL",
        "outputId": "1cd5b3c6-0985-4a66-ca6a-66b165820cdd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
              "290     0           30       0.023400      0.000644         0.000881   \n",
              "289     0           30       0.024462      0.001580         0.000845   \n",
              "288     0           30       0.023808      0.000622         0.000887   \n",
              "605     1           90       0.047792      0.003131         0.001085   \n",
              "606     1           90       0.046639      0.000366         0.001087   \n",
              "..    ...          ...            ...           ...              ...   \n",
              "552     0           30       0.002005      0.000239         0.000511   \n",
              "549     0           30       0.002375      0.000689         0.000614   \n",
              "546     0           30       0.002440      0.000597         0.000739   \n",
              "555     0           30       0.002872      0.000648         0.000754   \n",
              "558     0           30       0.002721      0.000531         0.000691   \n",
              "\n",
              "     std_score_time                                          param_clf  \\\n",
              "290        0.000053  XGBClassifier(colsample_bytree=0.5, learning_r...   \n",
              "289        0.000055  XGBClassifier(colsample_bytree=0.5, learning_r...   \n",
              "288        0.000035  XGBClassifier(colsample_bytree=0.5, learning_r...   \n",
              "605        0.000019  XGBClassifier(colsample_bytree=0.5, learning_r...   \n",
              "606        0.000025  XGBClassifier(colsample_bytree=0.5, learning_r...   \n",
              "..              ...                                                ...   \n",
              "552        0.000009           SVC(probability=True, random_state=3380)   \n",
              "549        0.000108           SVC(probability=True, random_state=3380)   \n",
              "546        0.000258           SVC(probability=True, random_state=3380)   \n",
              "555        0.000151           SVC(probability=True, random_state=3380)   \n",
              "558        0.000158           SVC(probability=True, random_state=3380)   \n",
              "\n",
              "    param_clf__bootstrap param_clf__max_depth param_clf__max_features  ...  \\\n",
              "290                  NaN                    3                     NaN  ...   \n",
              "289                  NaN                    3                     NaN  ...   \n",
              "288                  NaN                    3                     NaN  ...   \n",
              "605                  NaN                    4                     NaN  ...   \n",
              "606                  NaN                    4                     NaN  ...   \n",
              "..                   ...                  ...                     ...  ...   \n",
              "552                  NaN                  NaN                     NaN  ...   \n",
              "549                  NaN                  NaN                     NaN  ...   \n",
              "546                  NaN                  NaN                     NaN  ...   \n",
              "555                  NaN                  NaN                     NaN  ...   \n",
              "558                  NaN                  NaN                     NaN  ...   \n",
              "\n",
              "    mean_test_score std_test_score rank_test_score split0_train_score  \\\n",
              "290        0.680000       0.220706               1                1.0   \n",
              "289        0.680000       0.220706               1                1.0   \n",
              "288        0.680000       0.220706               1                1.0   \n",
              "605        0.656209       0.055243               4                1.0   \n",
              "606        0.656209       0.055243               4                1.0   \n",
              "..              ...            ...             ...                ...   \n",
              "552        0.513333       0.222711             831                1.0   \n",
              "549        0.513333       0.222711             831                1.0   \n",
              "546        0.513333       0.222711             831                1.0   \n",
              "555        0.513333       0.222711             831                1.0   \n",
              "558        0.513333       0.222711             831                1.0   \n",
              "\n",
              "    split1_train_score split2_train_score split3_train_score  \\\n",
              "290           1.000000                1.0                1.0   \n",
              "289           1.000000                1.0                1.0   \n",
              "288           1.000000                1.0                1.0   \n",
              "605           0.985915                1.0                1.0   \n",
              "606           0.985915                1.0                1.0   \n",
              "..                 ...                ...                ...   \n",
              "552           1.000000                1.0                1.0   \n",
              "549           1.000000                1.0                1.0   \n",
              "546           1.000000                1.0                1.0   \n",
              "555           1.000000                1.0                1.0   \n",
              "558           1.000000                1.0                1.0   \n",
              "\n",
              "    split4_train_score mean_train_score std_train_score  \n",
              "290                1.0         1.000000        0.000000  \n",
              "289                1.0         1.000000        0.000000  \n",
              "288                1.0         1.000000        0.000000  \n",
              "605                1.0         0.997183        0.005634  \n",
              "606                1.0         0.997183        0.005634  \n",
              "..                 ...              ...             ...  \n",
              "552                1.0         1.000000        0.000000  \n",
              "549                1.0         1.000000        0.000000  \n",
              "546                1.0         1.000000        0.000000  \n",
              "555                1.0         1.000000        0.000000  \n",
              "558                1.0         1.000000        0.000000  \n",
              "\n",
              "[836 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a7c1350f-1c60-45d3-a82b-e0673ca6aff8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iter</th>\n",
              "      <th>n_resources</th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_clf</th>\n",
              "      <th>param_clf__bootstrap</th>\n",
              "      <th>param_clf__max_depth</th>\n",
              "      <th>param_clf__max_features</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>split1_train_score</th>\n",
              "      <th>split2_train_score</th>\n",
              "      <th>split3_train_score</th>\n",
              "      <th>split4_train_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>290</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.023400</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>0.000881</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, learning_r...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.220706</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>289</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.024462</td>\n",
              "      <td>0.001580</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, learning_r...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.220706</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>288</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.023808</td>\n",
              "      <td>0.000622</td>\n",
              "      <td>0.000887</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, learning_r...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.680000</td>\n",
              "      <td>0.220706</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>605</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.047792</td>\n",
              "      <td>0.003131</td>\n",
              "      <td>0.001085</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, learning_r...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.656209</td>\n",
              "      <td>0.055243</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.985915</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.997183</td>\n",
              "      <td>0.005634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>606</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.046639</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>0.001087</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, learning_r...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.656209</td>\n",
              "      <td>0.055243</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.985915</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.997183</td>\n",
              "      <td>0.005634</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>552</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002005</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.000511</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.513333</td>\n",
              "      <td>0.222711</td>\n",
              "      <td>831</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>549</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002375</td>\n",
              "      <td>0.000689</td>\n",
              "      <td>0.000614</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.513333</td>\n",
              "      <td>0.222711</td>\n",
              "      <td>831</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002440</td>\n",
              "      <td>0.000597</td>\n",
              "      <td>0.000739</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.513333</td>\n",
              "      <td>0.222711</td>\n",
              "      <td>831</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002872</td>\n",
              "      <td>0.000648</td>\n",
              "      <td>0.000754</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.513333</td>\n",
              "      <td>0.222711</td>\n",
              "      <td>831</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.000691</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.513333</td>\n",
              "      <td>0.222711</td>\n",
              "      <td>831</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>836 rows  37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a7c1350f-1c60-45d3-a82b-e0673ca6aff8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a7c1350f-1c60-45d3-a82b-e0673ca6aff8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a7c1350f-1c60-45d3-a82b-e0673ca6aff8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "ranking = pd.DataFrame(joy_model.cv_results_)\n",
        "ranking = ranking.sort_values('rank_test_score')\n",
        "ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slf24N0Se3zB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88477599-a6fc-4f37-f576-d0dfcffe0954"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf': XGBClassifier(colsample_bytree=0.5, learning_rate=0.05, random_state=3380,\n",
              "               reg_lambda=0, scale_pos_weight=5, subsample=0.8),\n",
              " 'clf__colsample_bytree': 0.5,\n",
              " 'clf__gamma': 0,\n",
              " 'clf__learning_rate': 0.05,\n",
              " 'clf__max_depth': 3,\n",
              " 'clf__reg_lambda': 0,\n",
              " 'clf__scale_pos_weight': 5,\n",
              " 'clf__subsample': 0.8}"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "joy_model.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88EjZhfYw8sO"
      },
      "source": [
        "#### **Fear**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3DPQeYKOxC6j"
      },
      "outputs": [],
      "source": [
        "grid = HalvingGridSearchCV(pipeline, param_grid, cv = 5, verbose = 1, scoring = 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9I_nU-78yl7v"
      },
      "outputs": [],
      "source": [
        "X, y = resample(train['fear']['tweet'], train['fear']['sentiment_intensity'], param_over = False, param_under = False)\n",
        "embedding_fear = Doc2Vector(tokenizer = tokenizador).transform(X)\n",
        "embedding_fear = PCA(n_components = 20).fit_transform(embedding_fear)\n",
        "features_fear = FeaturesTransformer().transform(X)\n",
        "data_fear = np.hstack([embedding_fear, features_fear])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpnRyfSq3b1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eabe4670-3afd-4760-ad7b-b0be6ca023f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 4\n",
            "n_required_iterations: 6\n",
            "n_possible_iterations: 4\n",
            "min_resources_: 30\n",
            "max_resources_: 1257\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 564\n",
            "n_resources: 30\n",
            "Fitting 5 folds for each of 564 candidates, totalling 2820 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 188\n",
            "n_resources: 90\n",
            "Fitting 5 folds for each of 188 candidates, totalling 940 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 63\n",
            "n_resources: 270\n",
            "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 21\n",
            "n_resources: 810\n",
            "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
          ]
        }
      ],
      "source": [
        "fear_model = grid.fit(data_fear, y)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ranking = pd.DataFrame(fear_model.cv_results_)\n",
        "ranking = ranking.sort_values('rank_test_score')\n",
        "ranking"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "Hc-aW6gVbKqp",
        "outputId": "064bcd3b-b237-4381-c2d7-a31b245213fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
              "711     1           90       0.743417      0.005757         0.073839   \n",
              "707     1           90       0.732907      0.009431         0.067356   \n",
              "686     1           90       0.732202      0.006730         0.074616   \n",
              "682     1           90       0.738754      0.006167         0.075639   \n",
              "737     1           90       0.148516      0.006317         0.014789   \n",
              "..    ...          ...            ...           ...              ...   \n",
              "555     0           30       0.002242      0.000228         0.000534   \n",
              "558     0           30       0.002175      0.000144         0.000523   \n",
              "559     0           30       0.003101      0.000727         0.000762   \n",
              "561     0           30       0.002152      0.000183         0.000512   \n",
              "546     0           30       0.002218      0.000069         0.000541   \n",
              "\n",
              "     std_score_time                                  param_clf  \\\n",
              "711        0.010863  RandomForestClassifier(random_state=3380)   \n",
              "707        0.002275  RandomForestClassifier(random_state=3380)   \n",
              "686        0.012616  RandomForestClassifier(random_state=3380)   \n",
              "682        0.010849  RandomForestClassifier(random_state=3380)   \n",
              "737        0.001461  RandomForestClassifier(random_state=3380)   \n",
              "..              ...                                        ...   \n",
              "555        0.000015   SVC(probability=True, random_state=3380)   \n",
              "558        0.000017   SVC(probability=True, random_state=3380)   \n",
              "559        0.000152   SVC(probability=True, random_state=3380)   \n",
              "561        0.000011   SVC(probability=True, random_state=3380)   \n",
              "546        0.000024   SVC(probability=True, random_state=3380)   \n",
              "\n",
              "    param_clf__bootstrap param_clf__max_depth param_clf__max_features  ...  \\\n",
              "711                 True                  110                       3  ...   \n",
              "707                 True                  100                       3  ...   \n",
              "686                 True                   80                       3  ...   \n",
              "682                 True                   90                       3  ...   \n",
              "737                 True                   90                       2  ...   \n",
              "..                   ...                  ...                     ...  ...   \n",
              "555                  NaN                  NaN                     NaN  ...   \n",
              "558                  NaN                  NaN                     NaN  ...   \n",
              "559                  NaN                  NaN                     NaN  ...   \n",
              "561                  NaN                  NaN                     NaN  ...   \n",
              "546                  NaN                  NaN                     NaN  ...   \n",
              "\n",
              "    mean_test_score std_test_score rank_test_score split0_train_score  \\\n",
              "711        0.690196       0.113251               1           0.957746   \n",
              "707        0.690196       0.113251               1           0.957746   \n",
              "686        0.690196       0.113251               1           0.957746   \n",
              "682        0.690196       0.113251               1           0.957746   \n",
              "737        0.689542       0.115077               5           0.816901   \n",
              "..              ...            ...             ...                ...   \n",
              "555        0.326667       0.189033             828           1.000000   \n",
              "558        0.326667       0.189033             828           1.000000   \n",
              "559        0.326667       0.189033             828           1.000000   \n",
              "561        0.326667       0.189033             828           1.000000   \n",
              "546        0.326667       0.189033             828           1.000000   \n",
              "\n",
              "    split1_train_score split2_train_score split3_train_score  \\\n",
              "711           0.887324           0.847222           0.875000   \n",
              "707           0.887324           0.847222           0.875000   \n",
              "686           0.887324           0.847222           0.875000   \n",
              "682           0.887324           0.847222           0.875000   \n",
              "737           0.788732           0.736111           0.722222   \n",
              "..                 ...                ...                ...   \n",
              "555           1.000000           1.000000           1.000000   \n",
              "558           1.000000           1.000000           1.000000   \n",
              "559           1.000000           1.000000           1.000000   \n",
              "561           1.000000           1.000000           1.000000   \n",
              "546           1.000000           1.000000           1.000000   \n",
              "\n",
              "    split4_train_score mean_train_score std_train_score  \n",
              "711           0.833333         0.880125        0.043302  \n",
              "707           0.833333         0.880125        0.043302  \n",
              "686           0.833333         0.880125        0.043302  \n",
              "682           0.833333         0.880125        0.043302  \n",
              "737           0.694444         0.751682        0.044750  \n",
              "..                 ...              ...             ...  \n",
              "555           1.000000         1.000000        0.000000  \n",
              "558           1.000000         1.000000        0.000000  \n",
              "559           1.000000         1.000000        0.000000  \n",
              "561           1.000000         1.000000        0.000000  \n",
              "546           1.000000         1.000000        0.000000  \n",
              "\n",
              "[836 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3d02e73f-64a3-405b-ad3d-237bd9697692\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iter</th>\n",
              "      <th>n_resources</th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_clf</th>\n",
              "      <th>param_clf__bootstrap</th>\n",
              "      <th>param_clf__max_depth</th>\n",
              "      <th>param_clf__max_features</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>split1_train_score</th>\n",
              "      <th>split2_train_score</th>\n",
              "      <th>split3_train_score</th>\n",
              "      <th>split4_train_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>711</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.743417</td>\n",
              "      <td>0.005757</td>\n",
              "      <td>0.073839</td>\n",
              "      <td>0.010863</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>110</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.113251</td>\n",
              "      <td>1</td>\n",
              "      <td>0.957746</td>\n",
              "      <td>0.887324</td>\n",
              "      <td>0.847222</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.880125</td>\n",
              "      <td>0.043302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>707</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.732907</td>\n",
              "      <td>0.009431</td>\n",
              "      <td>0.067356</td>\n",
              "      <td>0.002275</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>100</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.113251</td>\n",
              "      <td>1</td>\n",
              "      <td>0.957746</td>\n",
              "      <td>0.887324</td>\n",
              "      <td>0.847222</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.880125</td>\n",
              "      <td>0.043302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>686</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.732202</td>\n",
              "      <td>0.006730</td>\n",
              "      <td>0.074616</td>\n",
              "      <td>0.012616</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>80</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.113251</td>\n",
              "      <td>1</td>\n",
              "      <td>0.957746</td>\n",
              "      <td>0.887324</td>\n",
              "      <td>0.847222</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.880125</td>\n",
              "      <td>0.043302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>682</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.738754</td>\n",
              "      <td>0.006167</td>\n",
              "      <td>0.075639</td>\n",
              "      <td>0.010849</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.690196</td>\n",
              "      <td>0.113251</td>\n",
              "      <td>1</td>\n",
              "      <td>0.957746</td>\n",
              "      <td>0.887324</td>\n",
              "      <td>0.847222</td>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.880125</td>\n",
              "      <td>0.043302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>737</th>\n",
              "      <td>1</td>\n",
              "      <td>90</td>\n",
              "      <td>0.148516</td>\n",
              "      <td>0.006317</td>\n",
              "      <td>0.014789</td>\n",
              "      <td>0.001461</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>90</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.689542</td>\n",
              "      <td>0.115077</td>\n",
              "      <td>5</td>\n",
              "      <td>0.816901</td>\n",
              "      <td>0.788732</td>\n",
              "      <td>0.736111</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.694444</td>\n",
              "      <td>0.751682</td>\n",
              "      <td>0.044750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>555</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002242</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000534</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.326667</td>\n",
              "      <td>0.189033</td>\n",
              "      <td>828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>558</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002175</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>0.000523</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.326667</td>\n",
              "      <td>0.189033</td>\n",
              "      <td>828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>559</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.003101</td>\n",
              "      <td>0.000727</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.326667</td>\n",
              "      <td>0.189033</td>\n",
              "      <td>828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>561</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002152</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.326667</td>\n",
              "      <td>0.189033</td>\n",
              "      <td>828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>546</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.002218</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000541</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>SVC(probability=True, random_state=3380)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.326667</td>\n",
              "      <td>0.189033</td>\n",
              "      <td>828</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>836 rows  37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3d02e73f-64a3-405b-ad3d-237bd9697692')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3d02e73f-64a3-405b-ad3d-237bd9697692 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3d02e73f-64a3-405b-ad3d-237bd9697692');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fear_model.best_params_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3N6poKXvRFU",
        "outputId": "27598db0-e320-4c15-e11a-f5b046c75248"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf': XGBClassifier(colsample_bytree=0.5, gamma=1, max_depth=4, random_state=3380,\n",
              "               reg_lambda=10, scale_pos_weight=5, subsample=0.8),\n",
              " 'clf__colsample_bytree': 0.5,\n",
              " 'clf__gamma': 1,\n",
              " 'clf__learning_rate': 0.1,\n",
              " 'clf__max_depth': 4,\n",
              " 'clf__reg_lambda': 10,\n",
              " 'clf__scale_pos_weight': 5,\n",
              " 'clf__subsample': 0.8}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E152EsRWw_xK"
      },
      "source": [
        "#### **Sadness**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmHSTYElxDXG"
      },
      "outputs": [],
      "source": [
        "grid = HalvingGridSearchCV(pipeline, param_grid, cv = 5, verbose = 1, scoring = 'accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvZAb2PPymhM"
      },
      "outputs": [],
      "source": [
        "X, y = resample(train['sadness']['tweet'], train['sadness']['sentiment_intensity'], param_over = False, param_under = False) #evitamos resampleo pues no queremos ensuciar los split\n",
        "embedding_sadness = Doc2Vector(tokenizer = tokenizador).transform(X)\n",
        "features_sadness = FeaturesTransformer().transform(X)\n",
        "data_sadness = np.hstack([embedding_sadness, features_sadness])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bE0w9u913f1w",
        "outputId": "b00d62cc-ca4f-42ee-d4bb-b2130bcfffae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n_iterations: 4\n",
            "n_required_iterations: 6\n",
            "n_possible_iterations: 4\n",
            "min_resources_: 30\n",
            "max_resources_: 860\n",
            "aggressive_elimination: False\n",
            "factor: 3\n",
            "----------\n",
            "iter: 0\n",
            "n_candidates: 564\n",
            "n_resources: 30\n",
            "Fitting 5 folds for each of 564 candidates, totalling 2820 fits\n",
            "----------\n",
            "iter: 1\n",
            "n_candidates: 188\n",
            "n_resources: 90\n",
            "Fitting 5 folds for each of 188 candidates, totalling 940 fits\n",
            "----------\n",
            "iter: 2\n",
            "n_candidates: 63\n",
            "n_resources: 270\n",
            "Fitting 5 folds for each of 63 candidates, totalling 315 fits\n",
            "----------\n",
            "iter: 3\n",
            "n_candidates: 21\n",
            "n_resources: 810\n",
            "Fitting 5 folds for each of 21 candidates, totalling 105 fits\n"
          ]
        }
      ],
      "source": [
        "sadness_model = grid.fit(data_sadness, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "cTjy5P-5LDYx",
        "outputId": "0a16808b-8dff-4969-ac94-9cd47333acc9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     iter  n_resources  mean_fit_time  std_fit_time  mean_score_time  \\\n",
              "762     2          270       0.444558      0.004305         0.001821   \n",
              "785     2          270       0.598441      0.003625         0.002032   \n",
              "782     2          270       0.599032      0.005539         0.002057   \n",
              "781     2          270       0.601370      0.006744         0.002036   \n",
              "759     2          270       0.725932      0.007959         0.001965   \n",
              "..    ...          ...            ...           ...              ...   \n",
              "103     0           30       0.727339      0.011571         0.073304   \n",
              "184     0           30       0.727123      0.010713         0.081510   \n",
              "101     0           30       1.464791      0.009670         0.155428   \n",
              "71      0           30       1.450002      0.017268         0.147436   \n",
              "79      0           30       0.716358      0.009638         0.081437   \n",
              "\n",
              "     std_score_time                                          param_clf  \\\n",
              "762        0.000018  XGBClassifier(colsample_bytree=0.5, gamma=1, l...   \n",
              "785        0.000030  XGBClassifier(colsample_bytree=0.5, gamma=1, l...   \n",
              "782        0.000060  XGBClassifier(colsample_bytree=0.5, gamma=1, l...   \n",
              "781        0.000017  XGBClassifier(colsample_bytree=0.5, gamma=1, l...   \n",
              "759        0.000026  XGBClassifier(colsample_bytree=0.5, gamma=1, l...   \n",
              "..              ...                                                ...   \n",
              "103        0.008294          RandomForestClassifier(random_state=3380)   \n",
              "184        0.015027          RandomForestClassifier(random_state=3380)   \n",
              "101        0.014966          RandomForestClassifier(random_state=3380)   \n",
              "71         0.010547          RandomForestClassifier(random_state=3380)   \n",
              "79         0.009578          RandomForestClassifier(random_state=3380)   \n",
              "\n",
              "    param_clf__bootstrap param_clf__max_depth param_clf__max_features  ...  \\\n",
              "762                  NaN                    3                     NaN  ...   \n",
              "785                  NaN                    4                     NaN  ...   \n",
              "782                  NaN                    4                     NaN  ...   \n",
              "781                  NaN                    4                     NaN  ...   \n",
              "759                  NaN                    7                     NaN  ...   \n",
              "..                   ...                  ...                     ...  ...   \n",
              "103                 True                   90                       3  ...   \n",
              "184                 True                  110                       2  ...   \n",
              "101                 True                   90                       3  ...   \n",
              "71                  True                   90                       2  ...   \n",
              "79                  True                   90                       2  ...   \n",
              "\n",
              "    mean_test_score std_test_score rank_test_score split0_train_score  \\\n",
              "762        0.659259       0.027716               1           1.000000   \n",
              "785        0.651852       0.048855               2           1.000000   \n",
              "782        0.651852       0.048855               2           1.000000   \n",
              "781        0.651852       0.048855               2           1.000000   \n",
              "759        0.651852       0.048855               2           1.000000   \n",
              "..              ...            ...             ...                ...   \n",
              "103        0.300000       0.124722             703           0.833333   \n",
              "184        0.300000       0.124722             703           0.750000   \n",
              "101        0.300000       0.124722             703           0.833333   \n",
              "71         0.300000       0.124722             703           0.791667   \n",
              "79         0.300000       0.124722             703           0.625000   \n",
              "\n",
              "    split1_train_score split2_train_score split3_train_score  \\\n",
              "762           1.000000           1.000000           1.000000   \n",
              "785           1.000000           1.000000           1.000000   \n",
              "782           1.000000           1.000000           1.000000   \n",
              "781           1.000000           1.000000           1.000000   \n",
              "759           1.000000           1.000000           1.000000   \n",
              "..                 ...                ...                ...   \n",
              "103           0.541667           0.666667           0.583333   \n",
              "184           0.541667           0.666667           0.583333   \n",
              "101           0.541667           0.666667           0.583333   \n",
              "71            0.541667           0.666667           0.583333   \n",
              "79            0.541667           0.666667           0.583333   \n",
              "\n",
              "    split4_train_score mean_train_score std_train_score  \n",
              "762           1.000000         1.000000        0.000000  \n",
              "785           1.000000         1.000000        0.000000  \n",
              "782           1.000000         1.000000        0.000000  \n",
              "781           1.000000         1.000000        0.000000  \n",
              "759           1.000000         1.000000        0.000000  \n",
              "..                 ...              ...             ...  \n",
              "103           0.583333         0.641667        0.104083  \n",
              "184           0.541667         0.616667        0.080795  \n",
              "101           0.666667         0.658333        0.100000  \n",
              "71            0.541667         0.625000        0.095015  \n",
              "79            0.541667         0.591667        0.048591  \n",
              "\n",
              "[836 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2357b629-7be1-418e-a9e0-44522f7ac2c8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>iter</th>\n",
              "      <th>n_resources</th>\n",
              "      <th>mean_fit_time</th>\n",
              "      <th>std_fit_time</th>\n",
              "      <th>mean_score_time</th>\n",
              "      <th>std_score_time</th>\n",
              "      <th>param_clf</th>\n",
              "      <th>param_clf__bootstrap</th>\n",
              "      <th>param_clf__max_depth</th>\n",
              "      <th>param_clf__max_features</th>\n",
              "      <th>...</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>std_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_train_score</th>\n",
              "      <th>split1_train_score</th>\n",
              "      <th>split2_train_score</th>\n",
              "      <th>split3_train_score</th>\n",
              "      <th>split4_train_score</th>\n",
              "      <th>mean_train_score</th>\n",
              "      <th>std_train_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>762</th>\n",
              "      <td>2</td>\n",
              "      <td>270</td>\n",
              "      <td>0.444558</td>\n",
              "      <td>0.004305</td>\n",
              "      <td>0.001821</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, gamma=1, l...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.659259</td>\n",
              "      <td>0.027716</td>\n",
              "      <td>1</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>785</th>\n",
              "      <td>2</td>\n",
              "      <td>270</td>\n",
              "      <td>0.598441</td>\n",
              "      <td>0.003625</td>\n",
              "      <td>0.002032</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, gamma=1, l...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.651852</td>\n",
              "      <td>0.048855</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>782</th>\n",
              "      <td>2</td>\n",
              "      <td>270</td>\n",
              "      <td>0.599032</td>\n",
              "      <td>0.005539</td>\n",
              "      <td>0.002057</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, gamma=1, l...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.651852</td>\n",
              "      <td>0.048855</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>781</th>\n",
              "      <td>2</td>\n",
              "      <td>270</td>\n",
              "      <td>0.601370</td>\n",
              "      <td>0.006744</td>\n",
              "      <td>0.002036</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, gamma=1, l...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.651852</td>\n",
              "      <td>0.048855</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>2</td>\n",
              "      <td>270</td>\n",
              "      <td>0.725932</td>\n",
              "      <td>0.007959</td>\n",
              "      <td>0.001965</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>XGBClassifier(colsample_bytree=0.5, gamma=1, l...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>0.651852</td>\n",
              "      <td>0.048855</td>\n",
              "      <td>2</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>103</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.727339</td>\n",
              "      <td>0.011571</td>\n",
              "      <td>0.073304</td>\n",
              "      <td>0.008294</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.124722</td>\n",
              "      <td>703</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.104083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>184</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.727123</td>\n",
              "      <td>0.010713</td>\n",
              "      <td>0.081510</td>\n",
              "      <td>0.015027</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>110</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.124722</td>\n",
              "      <td>703</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.616667</td>\n",
              "      <td>0.080795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>1.464791</td>\n",
              "      <td>0.009670</td>\n",
              "      <td>0.155428</td>\n",
              "      <td>0.014966</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>90</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.124722</td>\n",
              "      <td>703</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.658333</td>\n",
              "      <td>0.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>1.450002</td>\n",
              "      <td>0.017268</td>\n",
              "      <td>0.147436</td>\n",
              "      <td>0.010547</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>90</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.124722</td>\n",
              "      <td>703</td>\n",
              "      <td>0.791667</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.095015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0.716358</td>\n",
              "      <td>0.009638</td>\n",
              "      <td>0.081437</td>\n",
              "      <td>0.009578</td>\n",
              "      <td>RandomForestClassifier(random_state=3380)</td>\n",
              "      <td>True</td>\n",
              "      <td>90</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>0.124722</td>\n",
              "      <td>703</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.583333</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.591667</td>\n",
              "      <td>0.048591</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>836 rows  37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2357b629-7be1-418e-a9e0-44522f7ac2c8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2357b629-7be1-418e-a9e0-44522f7ac2c8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2357b629-7be1-418e-a9e0-44522f7ac2c8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "ranking = pd.DataFrame(sadness_model.cv_results_)\n",
        "ranking = ranking.sort_values('rank_test_score')\n",
        "ranking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pi_XaJBK7kx",
        "outputId": "2719b9c9-32b3-4dd0-ca4f-91e3c96c8a79"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf': XGBClassifier(colsample_bytree=0.5, gamma=1, learning_rate=0.05, max_depth=4,\n",
              "               random_state=3380, reg_lambda=0, scale_pos_weight=3,\n",
              "               subsample=0.8),\n",
              " 'clf__colsample_bytree': 0.5,\n",
              " 'clf__gamma': 1,\n",
              " 'clf__learning_rate': 0.05,\n",
              " 'clf__max_depth': 4,\n",
              " 'clf__reg_lambda': 0,\n",
              " 'clf__scale_pos_weight': 3,\n",
              " 'clf__subsample': 0.8}"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "sadness_model.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ltimo entrenamiento**"
      ],
      "metadata": {
        "id": "dHbFdpSj6v4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente y con tal de lograr una mejor predicci贸n, entrenamos nuestros modelos con toda la data disponible para luego generar las predicciones."
      ],
      "metadata": {
        "id": "bOYJYNcqIlvY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_evaluate_model(pipeline, sentiment: str, param_over = False, param_under = False):\n",
        "\n",
        "  dataset = train[sentiment]\n",
        "  \n",
        "  # ejecutamos el pipeline sobre el dataset\n",
        "  classifier, learned_labels = final_run(dataset, sentiment, pipeline, param_over, param_under)\n",
        "  \n",
        "  return classifier, learned_labels"
      ],
      "metadata": {
        "id": "OzGG5ffb6059"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def final_run(dataset, dataset_name, pipeline, param_over = False, param_under = False):\n",
        "    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n",
        "    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n",
        "\n",
        "    # Dividimos el dataset en train y test, a煤n no se transforma de Strings a valores num茅ricos.\n",
        "\n",
        "    X = dataset.tweet\n",
        "    y = dataset.sentiment_intensity\n",
        "\n",
        "    #Balanceo de clases\n",
        "    X, y = resample(X, y, param_over, param_under)\n",
        "\n",
        "    # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline). \n",
        "    # En este caso el Bag of Words es el encargado de transformar de Strings a vectores num茅ricos.\n",
        "    pipeline.fit(X, y)\n",
        "\n",
        "    # Obtenemos el orden de las clases aprendidas.\n",
        "    learned_labels = pipeline.classes_\n",
        "    \n",
        "    # Evaluamos:\n",
        "    return pipeline, learned_labels"
      ],
      "metadata": {
        "id": "Gtg5AIEs6-ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline_anger():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding_pca', Pipeline([('embedding', Doc2Vector(tokenizer = tokenizador)),\n",
        "                                                            ('pca', PCA(n_components = 15)),\n",
        "                                                            ])), \n",
        "                                  ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', XGBClassifier(colsample_bytree = 0.5, gamma = 0, learning_rate = 0.05, max_depth = 3, \n",
        "                            reg_lambda = 10, scale_pos_weight = 5, subsample = 0.8, random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "final_anger, labels_anger = final_evaluate_model(get_pipeline_anger(), 'anger', param_over = False, param_under = False)"
      ],
      "metadata": {
        "id": "rOnCmUsrGAWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline_joy():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding_pca', Pipeline([('embedding', Doc2Vector(tokenizer = tokenizador)),\n",
        "                                                            ('pca', PCA(n_components = 15)),\n",
        "                                                            ])), \n",
        "                                  ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(max_depth = 80, max_features = 3, min_samples_leaf = 3, \n",
        "                                     min_samples_split = 12, n_estimators = 500, random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "final_joy, labels_joy = final_evaluate_model(get_pipeline_joy(), 'joy', param_over = False, param_under = False)"
      ],
      "metadata": {
        "id": "yn4r0JoCF5Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline_fear():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding_pca', Pipeline([('embedding', Doc2Vector(tokenizer = tokenizador)),\n",
        "                                                            ('pca', PCA(n_components = 20)),\n",
        "                                                            ])), \n",
        "                                  ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', RandomForestClassifier(max_depth = 110, max_features = 3, min_samples_leaf = 3, \n",
        "                                     min_samples_split = 12, n_estimators = 1000, random_state = 3380)),\n",
        "      ])\n",
        "  \n",
        "  return pipe\n",
        "\n",
        "final_fear, labels_fear = final_evaluate_model(get_pipeline_fear(), 'fear', param_over = False, param_under = False)"
      ],
      "metadata": {
        "id": "zk0dVXdFEmUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pipeline_sadness():\n",
        "  pipe = Pipeline([\n",
        "      ('vectorizer', FeatureUnion([('embedding', Doc2Vector(tokenizer = tokenizador)), \n",
        "                                    ('features', FeaturesTransformer())\n",
        "                                    ])), \n",
        "      ('clf', XGBClassifier(colsample_bytree = 0.5, max_depth = 7, subsample = 0.8, \n",
        "                                     gamma = 0, learning_rate = 0.1, reg_lambda = 1, scale_pos_weight = 1,\n",
        "                                     random_state = 3380)),\n",
        "      ])                  \n",
        "  return pipe\n",
        "\n",
        "final_sadness, labels_sadness = final_evaluate_model(get_pipeline_sadness(), 'sadness', param_over = 0.1, param_under = False)"
      ],
      "metadata": {
        "id": "3GlrKf24CnV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_models = {'anger': final_anger, 'joy': final_joy, 'sadness': final_sadness, 'fear': final_fear}\n",
        "final_labels = {'anger': labels_anger, 'joy': labels_joy, 'sadness': labels_sadness, 'fear': labels_fear}"
      ],
      "metadata": {
        "id": "CCOyVD7gHahH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUKwcde_zvI0"
      },
      "source": [
        "### **Predecir los target set y crear la submission**\n",
        "\n",
        "Aqu铆 predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWDUoSmbzvI1"
      },
      "outputs": [],
      "source": [
        "def predict_target(dataset, classifier, labels):\n",
        "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
        "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
        "    \n",
        "    # Agregar ids\n",
        "    predicted['id'] = dataset.id.values\n",
        "    # Reordenar las columnas\n",
        "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
        "    return predicted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CJ4PTwZzvI1",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "predicted_target = {}\n",
        "\n",
        "# Crear carpeta ./predictions\n",
        "if (not os.path.exists('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "# por cada target set:\n",
        "for idx, key in enumerate(target):\n",
        "    # Predecirlo\n",
        "    predicted_target[key] = predict_target(target[key], final_models[key],\n",
        "                                           final_labels[key])\n",
        "    # Guardar predicciones en archivos separados. \n",
        "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
        "                                 sep='\\t',\n",
        "                                 header=False,\n",
        "                                 index=False)\n",
        "\n",
        "# Crear archivo zip\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Resultados**\n"
      ],
      "metadata": {
        "id": "EYAfgeyrN2pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Resultados**: Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones.  Pueden mostrar los resultados sobre la partici贸n de validaci贸n en caso que la generen o sobre los resultados del conjunto de testing. Mostrar los resultados en alguna tabla, pueden poner aqu铆 tambi茅n los resultados obtenidos al realizar la submission. (**0.5 puntos**)\n"
      ],
      "metadata": {
        "id": "lm1IO9EHN51j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci贸n, se presenta una muestra de los clasificadores entrenados con sus respectivos puntajes:\n",
        "\n",
        "| No. | Approach                       || Dataset   | AUC   | Kappa | Accuracy |\n",
        "|-----|--------------------------------||-----------|-------|-------|----------|\n",
        "|     | Features        | Clasifier     |           |       |       |          |\n",
        "| 0   | bow+chars_count | MultinomialNB | anger     | 0.622 | 0.163 | 0.688    |\n",
        "|     |                 |               | fear      | 0.597 | 0.091 | 0.559    |\n",
        "|     |                 |               | joy       | 0.728 | 0.251 | 0.601    |\n",
        "|     |                 |               | sadness   | 0.645 | 0.166 | 0.581    |\n",
        "|     |                 |               |**average**| 0.648 | 0.168 | 0.607    |\n",
        "| 1   | pretrained embedding + features+ PCA (15)+ resampling  | RandomForest  | anger     |   0.734    |   0.366    |     0.756     |\n",
        "|     |                 |               | fear      |   0.746    |  0.354     |     0.67     |\n",
        "|     |                 |               | joy       |   0.776    |    0.448   |      0.695    |\n",
        "|     |                 |               | sadness   |    0.728   |   0.249    |   0.588       |\n",
        "|     |                 |               |**average**|    0.746   |    0.354   |    0.677      |\n",
        "| 2   | pretrained embedding + features | RandomForest | anger  | 0.688 | 0.223 | 0.72    |\n",
        "|     |                 |               | fear      | 0.735 | 0.288 | 0.653    |\n",
        "|     |                 |               | joy       | 0.755 | 0.343 | 0.664    |\n",
        "|     |                 |               | sadness   | 0.704 | 0.264 | 0.606    |\n",
        "|     |                 |               |**average**| 0.72 | 0.28 | 0.661    |\n",
        "| 3   | BOW 1,2-gramas  con PCA n=60 | XGboost | anger     | 0.583 | 0.136 | 0.685    |\n",
        "|     |                 |               | fear      | 0.635 | 0.177 | 0.588    |\n",
        "|     |                 |               | joy       | 0.658 | 0.165 | 0.577    |\n",
        "|     |                 |               | sadness   | 0.629 | 0.203 | 0.556    |\n",
        "|     |                 |               |**average**| 0.626 | 0.17 | 0.602    |\n",
        "| 4   | embedding propios + resampling | RandomForest | anger     | 0.636 | 0.186 | 0.633    |\n",
        "|     |                 |               | fear      | 0.557 | 0.051 | 0.383    |\n",
        "|     |                 |               | joy       | 0.628 | 0.201 | 0.463    |\n",
        "|     |                 |               | sadness   | 0.549 | 0.017 | 0.313    |\n",
        "|     |                 |               |**average**| 0.593 | 0.114 | 0.448    |\n",
        "| 5   | bow+embedding propios + features| RandomForest | anger     | 0.648 | 0.095 | 0.685    |\n",
        "|     |                 |               | fear      | 0.613 | 0.132 | 0.6    |\n",
        "|     |                 |               | joy       | 0.704 | 0.159 | 0.601    |\n",
        "|     |                 |               | sadness   | 0.663 | 0.067 | 0.542    |\n",
        "|     |                 |               |**average**| 0.657 | 0.113 | 0.607    |\n",
        "| 6   | TF_IDF + Features Extractor + PCA       | RandomForest | anger     | 0.732 | 0.255 | 0.723    |\n",
        "|     |                 |               | fear      | 0.702 | 0.296 | 0.641    |\n",
        "|     |                 |               | joy       | 0.748 | 0.351 | 0.641    |\n",
        "|     |                 |               | sadness   | 0.727 | 0.336 | 0.63    |\n",
        "|     |                 |               |**average**| 0.727 | 0.309 | 0.659    |"
      ],
      "metadata": {
        "id": "a23by_SJORMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A partir de la tabla se puede concluir que, a pesar de usar `RandomForest` en 5 de los 7 modelos mostrados, existe una amplia diferencia entre cada uno que se explica en el **pre procesamiento** de los datos. En primer lugar, notamos que utilizar embeddings propios no tiene un rendimiento tan bueno, pues este es menor al ofrecido por t茅cnicas como Bag of Words o Td-Idf. Sin embargo, cuando al embedding se le a帽ade tambi茅n las features generadas, el clasificador posee un mayor poder de predicci贸n que Bag of Words o el embedding por s铆 solo. En paralelo, se encuentran resultados sorprendentes con TF-IDF que, a pesar de ser una metodolog铆a de vectorizaci贸n que resulta en representaciones *sparse* logra llegar a buenos resultados, incluso mejores que los embeddings entrenados con data propia. Por otro lado, es posible ver el poder de predicci贸n de los embedding pre entrenados (*transfer learning*), pues con la mera implementaci贸n de estos embedding se logran resultados mejores que las t茅cnicas ya citadas. Finalmente, es posible mejorar el poder de predicci贸n de estos embedding concatenando features, resampling y reducci贸n de dimensionalidad, logrando ser la mejor combinaci贸n de la tabla."
      ],
      "metadata": {
        "id": "wssolCDzuWV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 8. **Conclusiones**"
      ],
      "metadata": {
        "id": "BsceEkM7ODs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A lo largo de todo el trabajo hemos visto como ciertas configuraciones han demostrado ser bastante superiores que otras alternativas. En primer lugar, vimos como los embeddings logran generar una mejor representaci贸n de los datos en comparaci贸n a metodolog铆as como Bag of Words o TF-IDF. Tambi茅n vimos como el **Feature Engineering** logra mejorar mucho el poder de predicci贸n de los clasificadores, sobre todo cuando se incorporan lexicones y librer铆as externas隆. Adem谩s, a pesar de que el set de datos est谩 desbalanceado, metodolog铆as de *resampling* lograron mejoras poco significativas sobre el poder de predicci贸n de los modelos. Finalmente, vimos como la reducci贸n de dimensionalidad logra un mejor aprendizaje por parte de los clasificadores (`RandomForest` y `XGBoost`).\n",
        "\n",
        "Se plantean las siguientes lineas de investigaci贸n para seguir mejorardo el clasificador:\n",
        "\n",
        "1. Probar con clasificadores basados en **redes neuronales**, los cuales no pudieron ser implementados a cabalidad por falta de conocimiento en librerias como `Pytorch`.\n",
        "\n",
        "2. Probar con metodolog铆as de vectorizaci贸n como `BERT`, los cuales logran capturar el contexto mejor que los `Embeddings`.\n",
        "\n",
        "3. Incorporar a los clasificadores los features generados por **Bag of Words** y **TF-IDF**."
      ],
      "metadata": {
        "id": "AXW1fLu5OGej"
      }
    }
  ]
}