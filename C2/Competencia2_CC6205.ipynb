{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Competencia2_CC6205.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MocJN22HSJ1x",
        "iWlfabmkaSE7",
        "PzQlYlmGaSFH",
        "UbA1EmhCaSFI",
        "AaVhZ5iaaSFK",
        "u27WffRVUj4v",
        "lRYOEDiQaSHK",
        "rV9oLkN1aSHO",
        "Zpy3p7YaaSHT",
        "Pu_lXic2aSHd",
        "rVZvHtwpaSHq",
        "Fz39wa78wGYR",
        "8xlq48WjiW6U",
        "PYNcwKnAz5Hf",
        "uF1ysw_Kw6zz",
        "YwQp1Ru8Oht8",
        "LZEWJXrNaSIf"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0tyIsliieNr"
      },
      "source": [
        "# **Competencia 2 - CC6205 Natural Language Processing üìö**\n",
        "\n",
        "Integrantes:\n",
        "\n",
        "Usuario del equipo en CodaLab (Obligatorio):\n",
        "\n",
        "Fecha l√≠mite de entrega üìÜ: 24 de Junio.\n",
        "\n",
        "Tiempo estimado de dedicaci√≥n:\n",
        "\n",
        "Link competencia: Poner el link aqu√≠"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MocJN22HSJ1x"
      },
      "source": [
        "### **Objetivo**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwdgXS8FSLvc"
      },
      "source": [
        "El objetivo de esta competencia es resolver una de las tareas m√°s importantes en el √°rea del procesamiento de lenguage natural, relacionada con la extracci√≥n de informaci√≥n: [Named Entity Recognition (NER)](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf). \n",
        "\n",
        "En particular, y al igual que en la competencia anterior, deber√°n crear distintos modelos que apunten a resolver la tarea de NER en Espa√±ol. Para esto, les entregaremos un dataset real perteneciente a la lista de espera NO GES en Chile. Es importante destacar que existe una falta de trabajos realizados en el √°rea de NER en Espa√±ol y a√∫n m√°s en el contexto cl√≠nico, por ende puede ser considerado como una tarea bien desafiante y quiz√°s les interesa trabajar en el √°rea m√°s adelante en sus carreras.\n",
        "\n",
        "En este notebook les entregaremos un baseline como referencia de los resultados que esperamos puedan obtener. Recuerden que el no superar a los baselines en alguna de las tres m√©tricas conlleva un descuento de 0.5 puntos hasta 1.5 puntos.\n",
        "\n",
        "Como hemos estado viendo redes neuronales tanto en catedras, tareas y auxiliares (o pr√≥ximamente lo har√°n), esperamos que (por lo menos) utilicen Redes Neuronales Recurrentes (RNN) para resolverla. \n",
        "\n",
        "Nuevamente, hay total libertad para utilizar el software y los modelos que deseen, siempre y cuando estos no traigan los modelos ya implementados. (De todas maneras como es un corpus nuevo, es dif√≠cil que haya alg√∫n modelo ya implementado con estas entidades)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnjgmvjBSReb"
      },
      "source": [
        "### **Explicaci√≥n de la competencia**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH4HqnCjSWs-"
      },
      "source": [
        "La tarea **NER** que van a resolver en esta competencia es com√∫nmente abordada como un problema de Sequence Labeling.\n",
        "\n",
        "**¬øQu√© es Sequence Labeling?** \n",
        "\n",
        "En breves palabras, dada una secuencia de tokens (oraci√≥n) sequence labeling tiene por objetivo asignar una etiqueta a cada token de dicha secuencia. En pocas palabras, dada una lista de tokens esperamos encontrar la mejor secuencia de etiquetas asociadas a esa lista. Ahora veamos de qu√© se trata este problema.\n",
        "\n",
        "**Named Entity Recognition (NER)**\n",
        "\n",
        "NER es un ejemplo de un problema de Sequence Labeling. Pero antes de definir formalmente esta tarea, es necesario definir algunos conceptos claves para poder entenderla de la mejor manera:\n",
        "\n",
        "- *Token*: Un token es una secuencia de caracteres, puede ser una palabra, un n√∫mero o un s√≠mbolo.\n",
        "\n",
        "- *Entidad*: No es m√°s que un trozo de texto (uno o m√°s tokens) asociado a una categor√≠a predefinida. Originalmente se sol√≠an utilizar categor√≠as como nombres de personas, organizaciones, ubicaciones, pero actualmente se ha extendido a diferentes dominios.\n",
        "\n",
        "- *L√≠mites de una entidad*: Son los √≠ndices de los tokens de inicio y f√≠n dentro de una entidad.\n",
        "\n",
        "- *Tipo de entidad*: Es la categor√≠a predefinida asociada a la entidad.\n",
        "\n",
        "Dicho esto, definimos formalmente una entidad como una tupla: $(s, e, t)$, donde $s, t$ son los l√≠mites de la entidad (√≠ndices de los tokens de inicio y fin, respectivamente) y t corresponde al tipo de entidad o categor√≠a. Ya veremos m√°s ejemplos luego de describir el Dataset.\n",
        "\n",
        "**Corpus de la Lista de espera**\n",
        "\n",
        "Trabajaran con un conjunto de datos reales correspondiente a interconsultas de la lista de espera NO GES en Chile. Si quieren saber m√°s sobre c√≥mo fueron generados los datos pueden revisar el paper publicado hace unos meses atr√°s en el workshop de EMNLP, una de las conferencias m√°s importantes de NLP: [https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/](https://www.aclweb.org/anthology/2020.clinicalnlp-1.32/).\n",
        "\n",
        "Este corpus Chileno est√° constituido originalmente por 7 tipos de entidades pero por simplicidad en esta competencia trabajar√°n con las siguientes:\n",
        "\n",
        "- **Disease**\n",
        "- **Body_Part**\n",
        "- **Medication** \n",
        "- **Procedures** \n",
        "- **Family_Member**\n",
        "\n",
        "Si quieren obtener m√°s informaci√≥n sobre estas entidades pueden consultar la [gu√≠a de anotaci√≥n](https://plncmm.github.io/annodoc/). Adem√°s, mencionar que este corpus est√° restringido bajo una licencia que permite solamente su uso acad√©mico, as√≠ que no puede ser compartido m√°s all√° de este curso o sin permisos por parte de los autores en caso que quieran utilizarlo fuera. Si este √∫ltimo es el caso entonces pueden escribir directamente al correo: pln@cmm.uchile.cl. Al aceptar los t√©rminos y condiciones de la competencia est√°n de acuerdo con los puntos descritos anteriormente.\n",
        "\n",
        "\n",
        "**Formato ConLL**\n",
        "\n",
        "Los archivos que ser√°n entregados a ustedes vienen en un formato est√°ndar utilizado en NER, llamado ConLL. No es m√°s que un archivo de texto, que cumple las siguientes propiedades.\n",
        "\n",
        "- Un salto de linea corresponde a la separaci√≥n entre oraciones. Esto es importante ya que al entrenar una red neuronal ustedes pasaran una lista de oraciones como input, m√°s conocidos como batches.\n",
        "\n",
        "- La primera columna del archivo contiene todos los tokens de la partici√≥n.\n",
        "\n",
        "- La segunda columna del archivo contiene el tipo de entidad asociado al token de la primera columna.\n",
        "\n",
        "- Los tipos de entidades siguen un formato cl√°sico en NER denominado *IOB2*. Si un tipo de entidad comienza con el prefijo \"B-\" (Beginning) significa que es el token de inicio de una entidad, si comienza con \"I-\" (Inside) es un token distinto al de inicio y si un token est√° asociado a la categor√≠a O (Outside) significa que no pertenece a ninguna entidad.\n",
        "\n",
        "Aqu√≠ va un ejemplo:\n",
        "\n",
        "```\n",
        "PACIENTE O\n",
        "PRESENTA O\n",
        "FRACTURA B-Disease\n",
        "CORONARIA I-Disease\n",
        "COMPLICADA I-Disease\n",
        "EN O\n",
        "PIE B-Body_Part\n",
        "IZQUIERDO I-Body_Part\n",
        ". O\n",
        "SE O\n",
        "REALIZA O\n",
        "INSTRUMENTACION B-Procedure\n",
        "INTRACONDUCTO I-Procedure\n",
        ". O\n",
        "```\n",
        "\n",
        "Seg√∫n nuestra definici√≥n tenemos las siguientes tres entidades (enumerando desde 0): \n",
        "\n",
        "- $(2, 4, Disease)$\n",
        "- $(6, 7, Body Part)$\n",
        "- $(11, 12, Procedure)$\n",
        "\n",
        "Repasen un par de veces todos estos conceptos antes de pasar a la siguiente secci√≥n del notebook.\n",
        "Es importante entender bien este formato ya que al medir el rendimiento de sus modelos, consideraremos una **m√©trica estricta**. Esta m√©trica se llama as√≠ ya que considera correcta una predicci√≥n de su modelo, s√≥lo si al compararlo con las entidades reales **coinciden tanto los l√≠mites de la entidad como el tipo.** \n",
        "\n",
        "Para ejemplificar, tomando el caso anterior, si el modelo es capaz de encontrar la siguiente entidad: $(2, 3, Disease)$, entonces se considera incorrecto ya que pudo predecir dos de los tres tokens de dicha enfermedad. Es decir, buscamos una m√©trica que sea alta a nivel de entidad y no a nivel de token.\n",
        "\n",
        "Antes de pasar a explicar las reglas, se recomienda visitar los siguientes links para entender bien el baseline de la competencia:\n",
        "\n",
        "-  [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)       \n",
        "-  [Recurrent Neural Networks](slides/NLP-RNN.pdf) | [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)\n",
        "\n",
        "\n",
        "Recuerden que todo el material se encuentra disponible en el [github del curso](https://github.com/dccuchile/CC6205)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWlfabmkaSE7"
      },
      "source": [
        "### **Reglas de la competencia**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w9Dw4CSaSE8"
      },
      "source": [
        "**texto en negrita**- Para que su competencia sea evaluada, deben participar en la competencia y enviar este notebook con su informe.\n",
        "- Para participar, deben registrarse en la competencia en Codalab en grupos de m√°ximo 4 alumnos. Cada grupo debe tener un nombre de equipo. (¬°Y deben reportarlo en su informe, por favor!)\n",
        "- Las m√©tricas usadas ser√°n m√©tricas estrictas (ya explicado anteriormente) utilizando m√©tricas cl√°sicas como lo son precisi√≥n, recall y micro f1-score.\n",
        "- En esta tarea se recomienda usar GPU. Pueden ejecutar su tarea en colab (lo cual trae todo instalado) o pueden intentar ejecut√°ndolo en su computador. En este caso, deber√° ser compatible con cuda y deber√°n instalar todo por su cuenta.\n",
        "- En total pueden hacer un **m√°ximo de 5 env√≠os**.\n",
        "- Por favor, todas sus dudas haganlas por el canal de Discord. Los emails que lleguen al equipo docente ser√°n remitidos a ese medio. Recuerden el √°nimo colaborativo del curso.\n",
        "- Estar top 5 en alguna de las tres m√©tricas equivale a una bonificaci√≥n en su nota final.\n",
        "\n",
        "√âxito!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZyHBjU-R-wi"
      },
      "source": [
        "### **Baseline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WZ8G01aSBYX"
      },
      "source": [
        "En este punto esperamos que tengan conocimiento sobre redes neuronales y en particular redes neuronales recurrentes (RNN), si no siempre pueden escribirnos por el canal de Discord para aclarar dudas. La RNN del baseline adjunto a este notebook est√° programado en la librer√≠a [`pytorch`](https://pytorch.org/) pero ustedes pueden utilizar keras, tensorflow si as√≠ lo desean. El c√≥digo contiene lo siguiente:\n",
        "\n",
        "- La carga de los datasets, creaci√≥n de batches de texto y padding (esto es importante ya que si utilizan redes neuronales tienen que tener el mismo largo los inputs). \n",
        "\n",
        "- La implementaci√≥n b√°sica de una red `LSTM` simple de solo un nivel y sin bi-direccionalidad. \n",
        "\n",
        "- La construcci√≥n del formato del output requerido para que lo puedan probar en la tarea en codalab.\n",
        "\n",
        "Se espera que como m√≠nimo ustedes puedan experimentar con el baseline utilizando (pero no limit√°ndose) estas sugerencias:\n",
        "\n",
        "*   Probar la t√©cnica de early stopping.\n",
        "*   Variar la cantidad de par√°metros de la capa de embeddings.\n",
        "*   Variar la cantidad de capas RNN.\n",
        "*   Variar la cantidad de par√°metros de las capas de RNN.\n",
        "*   Inicializar la capa de embeddings con modelos pre-entrenados. (word2vec, glove, conceptnet, etc...). [Embeddings en espa√±ol aqu√≠](https://github.com/dccuchile/spanish-word-embeddings). Tambi√©n aqu√≠ pueden encontrar unos embeddings cl√≠nicos en Espa√±ol: [https://zenodo.org/record/3924799](https://zenodo.org/record/3924799)\n",
        "*   Variar la cantidad de √©pocas de entrenamiento.\n",
        "*   Variar el optimizador, learning rate, batch size, usar CRF loss, etc.\n",
        "*   Probar una capa de CRF para garantizar el     formato IOB2.\n",
        "*   Probar bi-direccionalidad.\n",
        "*   Incluir dropout.\n",
        "*   Probar modelos de tipo GRU.\n",
        "*   Probar usando capas de atenci√≥n.\n",
        "*   Probar Embedding Contextuales (les puede ser de utilidad [flair](https://github.com/flairNLP/flair))\n",
        "*   Probar modelos de transformers en espa√±ol usando [Huggingface](https://github.com/huggingface/transformers) o el framework Flair."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Rr2mzxPTzNd"
      },
      "source": [
        "### **Reporte**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEf33mnxT0rf"
      },
      "source": [
        "Este debe cumplir la siguiente estructura:\n",
        "\n",
        "1.\t**Introducci√≥n**: Presentar brevemente el contexto, problema a resolver, incluyendo la formalizaci√≥n de la task (c√≥mo son los inputs y outputs del problema) y los desaf√≠os que ven al analizar el corpus entregado. (**0.5 puntos**)\n",
        "\n",
        "2.\t**Modelos**: Describir brevemente los modelos, m√©todos e hiperpar√°metros utilizados. (**1.0 puntos**)\n",
        "\n",
        "4.\t**M√©tricas de evaluaci√≥n**: Describir las m√©tricas utilizadas en la evaluaci√≥n indicando qu√© miden y cu√°l es su interpretaci√≥n en este problema en particular. (**0.5 puntos**)\n",
        "\n",
        "5.  **Dise√±o experimental**: Esta es una de las secciones m√°s importantes del reporte. Deben describir minuciosamente los experimentos que realizar√°n en la siguiente secci√≥n. Describir las variables de control que manejar√°n, algunos ejemplos pueden ser: Los hiperpar√°metros de los modelos, tipo de embeddings utilizados, tipos de arquitecturas. Ser claros con el conjunto de hiperpar√°metros que probar√°n, la decisi√≥n en las funciones de optimizaci√≥n, funci√≥n de p√©rdida,  regulaci√≥n, etc. B√°sicamente explicar qu√© es lo que veremos en la siguiente secci√≥n.\n",
        "(**1 punto**)\n",
        "\n",
        "6.\t**Experimentos**: Reportar todos sus experimentos y c√≥digo en esta secci√≥n. Comparar los resultados obtenidos utilizando diferentes modelos. ¬°Es vital haber realizado varios experimentos para sacar una buena nota! (**2.0 puntos**)\n",
        "\n",
        "7.\t**Conclusiones**: Discutir resultados, proponer trabajo futuro. (**1 punto**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaoU1EXfUDbl"
      },
      "source": [
        "# **Entregable.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzQlYlmGaSFH"
      },
      "source": [
        "## **Introducci√≥n**\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rekGPTcgL4qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVL0-01AUOzL"
      },
      "source": [
        "    Escriba su introducci√≥n aqu√≠"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbA1EmhCaSFI"
      },
      "source": [
        "## **Modelos**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HsvlfPJUSId"
      },
      "source": [
        "    Explique claramente los modelos utilizados aqu√≠"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaVhZ5iaaSFK"
      },
      "source": [
        "## **M√©tricas de evaluaci√≥n**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXl3GaVMUYA7"
      },
      "source": [
        "- **M√©trica estricta:**\n",
        "- **Precision:** \n",
        "- **Recall:** \n",
        "- **Micro F1 score:**\n",
        "Recuerde hacer la distinci√≥n entre lo que ser√≠a una m√©trica de micro f1-score vs macro f1-score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u27WffRVUj4v"
      },
      "source": [
        "## **Dise√±o experimental**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O228DbeUmE7"
      },
      "source": [
        "    Descripci√≥n de la metodolog√≠a "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFM-wNt8aSFM"
      },
      "source": [
        "## **Experimentos**\n",
        "\n",
        "\n",
        "El c√≥digo que les entregaremos servir√° de baseline para luego implementar mejores modelos. \n",
        "En general, el c√≥digo asociado a la carga de los datos, las funciones de entrenamiento, de evaluaci√≥n y la predicci√≥n de los datos de la competencia no deber√≠an cambiar. \n",
        "Solo deben preocuparse de cambiar la arquitectura del modelo, sus hiperpar√°metros y reportar, lo cual lo pueden hacer en las subsecciones *modelos*.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMgKjfYC_Go-"
      },
      "source": [
        "###  **Carga de datos y Preprocesamiento**\n",
        "\n",
        "Para cargar los datos y preprocesarlos usaremos la librer√≠a [`torchtext`](https://github.com/pytorch/text). Tener cuidado ya que hace algunos meses esta librer√≠a tuvo cambios radicales, quedando las funcionalidades pasadas en un nuevo paquete llamado legacy. Esto ya que si quieren usar m√°s funciones de la librer√≠a entonces vean los cambios en la documentaci√≥n.\n",
        "\n",
        "En particular usaremos su m√≥dulo `data`, el cual seg√∫n su documentaci√≥n original provee: \n",
        "\n",
        "    - Ability to describe declaratively how to load a custom NLP dataset that's in a \"normal\" format\n",
        "    - Ability to define a preprocessing pipeline\n",
        "    - Batching, padding, and numericalizing (including building a vocabulary object)\n",
        "    - Wrapper for dataset splits (train, validation, test)\n",
        "\n",
        "\n",
        "El proceso ser√° el siguiente: \n",
        "\n",
        "1. Descargar los datos desde github y examinarlos.\n",
        "2. Definir los campos (`fields`) que cargaremos desde los archivos.\n",
        "3. Cargar los datasets.\n",
        "4. Crear el vocabulario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "27csY87GaSFO",
        "scrolled": false,
        "outputId": "d1d72977-379c-485e-8a51-ed250090e682"
      },
      "source": [
        "# Instalamos torchtext que nos facilitar√° la vida en el pre-procesamiento del formato ConLL.\n",
        "!pip install -U torchtext==0.10.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.10.0\n",
            "  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.6 MB 6.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 831.4 MB 2.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.2.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0 torchtext-0.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torch",
                  "torchtext"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng7wRGEyawjM"
      },
      "source": [
        "import torch\n",
        "from torchtext import data, datasets, legacy\n",
        "\n",
        "\n",
        "# Garantizar reproducibilidad de los experimentos\n",
        "SEED = 1234\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BehSou6rCvwg"
      },
      "source": [
        "#### **Obtener datos**\n",
        "\n",
        "Descargamos los datos de entrenamiento, validaci√≥n y prueba en nuestro directorio de trabajo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbT0g_kC18Jb",
        "outputId": "22da2bfb-c123-4514-df8e-ef38b1b5010e"
      },
      "source": [
        "#%%capture\n",
        "\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt -nc # Dataset de Entrenamiento\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt -nc    # Dataset de Validaci√≥n (Para probar y ajustar el modelo)\n",
        "!wget https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt -nc  # Dataset de la Competencia. Estos datos solo contienen los tokens. ¬°¬°SON LOS QUE DEBEN SER PREDICHOS!!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-04 01:36:18--  https://github.com/dccuchile/CC6205/releases/download/v1.0/train.txt\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/77198f00-c145-11eb-83d1-11e647241ab6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220604%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220604T013618Z&X-Amz-Expires=300&X-Amz-Signature=cdd4c725662155060dc55598e8162b24958210106ac512c4c993ec2a0170cc14&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtrain.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-06-04 01:36:18--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/77198f00-c145-11eb-83d1-11e647241ab6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220604%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220604T013618Z&X-Amz-Expires=300&X-Amz-Signature=cdd4c725662155060dc55598e8162b24958210106ac512c4c993ec2a0170cc14&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtrain.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1607913 (1.5M) [application/octet-stream]\n",
            "Saving to: ‚Äòtrain.txt‚Äô\n",
            "\n",
            "train.txt           100%[===================>]   1.53M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-06-04 01:36:18 (42.7 MB/s) - ‚Äòtrain.txt‚Äô saved [1607913/1607913]\n",
            "\n",
            "--2022-06-04 01:36:18--  https://github.com/dccuchile/CC6205/releases/download/v1.0/dev.txt\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/741e9e80-c145-11eb-813a-b9abac0d674c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220604%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220604T013618Z&X-Amz-Expires=300&X-Amz-Signature=662021db0441aecb6750866a55fa6419cf1af75759d3278c2abb645b329539d7&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Ddev.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-06-04 01:36:19--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/741e9e80-c145-11eb-813a-b9abac0d674c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220604%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220604T013618Z&X-Amz-Expires=300&X-Amz-Signature=662021db0441aecb6750866a55fa6419cf1af75759d3278c2abb645b329539d7&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Ddev.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 177166 (173K) [application/octet-stream]\n",
            "Saving to: ‚Äòdev.txt‚Äô\n",
            "\n",
            "dev.txt             100%[===================>] 173.01K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-06-04 01:36:19 (9.99 MB/s) - ‚Äòdev.txt‚Äô saved [177166/177166]\n",
            "\n",
            "--2022-06-04 01:36:19--  https://github.com/dccuchile/CC6205/releases/download/v1.0/test.txt\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/75e86200-c145-11eb-94f8-49517311d768?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220604%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220604T013619Z&X-Amz-Expires=300&X-Amz-Signature=7e156b973f3a95f8974c39011f843f41afdc7a9f7eabdd6dba808ae6f7b19b85&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtest.txt&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-06-04 01:36:19--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/196273020/75e86200-c145-11eb-94f8-49517311d768?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220604%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220604T013619Z&X-Amz-Expires=300&X-Amz-Signature=7e156b973f3a95f8974c39011f843f41afdc7a9f7eabdd6dba808ae6f7b19b85&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=196273020&response-content-disposition=attachment%3B%20filename%3Dtest.txt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 147052 (144K) [application/octet-stream]\n",
            "Saving to: ‚Äòtest.txt‚Äô\n",
            "\n",
            "test.txt            100%[===================>] 143.61K   678KB/s    in 0.2s    \n",
            "\n",
            "2022-06-04 01:36:20 (678 KB/s) - ‚Äòtest.txt‚Äô saved [147052/147052]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMud7YGMBZvg"
      },
      "source": [
        "####  **Fields**\n",
        "\n",
        "Un `field`:\n",
        "\n",
        "* Define un tipo de datos junto con instrucciones para convertir el texto a Tensor.\n",
        "* Contiene un objeto `Vocab` que contiene el vocabulario (palabras posibles que puede tomar ese campo).\n",
        "* Contiene otros par√°metros relacionados con la forma en que se debe numericalizar un tipo de datos, como un m√©todo de tokenizaci√≥n y el tipo de Tensor que se debe producir.\n",
        "\n",
        "\n",
        "Analizemos el siguiente cuadro el cual contiene un ejemplo cualquiera de entrenamiento:\n",
        "\n",
        "\n",
        "```\n",
        "El O\n",
        "paciente O\n",
        "padece O\n",
        "de O\n",
        "cancer B-Disease\n",
        "de I-Disease\n",
        "colon I-Disease\n",
        ". O\n",
        "```\n",
        "\n",
        "Cada linea contiene un token y el tipo de entidad asociado en el formato IOB2 ya explicado. Para que `torchtext` pueda cargar estos datos, debemos definir como va a leer y separar los componentes de cada una de las lineas.\n",
        "Para esto, definiremos un field para cada uno de esos componentes: Las palabras (`TEXT`) y las etiquetas o categor√≠as (`NER_TAGS`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DcM_IjgCdzz"
      },
      "source": [
        "# Primer Field: TEXT. Representan los tokens de la secuencia\n",
        "TEXT = legacy.data.Field(lower=False) \n",
        "\n",
        "# Segundo Field: NER_TAGS. Representan los Tags asociados a cada palabra.\n",
        "NER_TAGS = legacy.data.Field(unk_token=None)\n",
        "fields = ((\"text\", TEXT), (\"nertags\", NER_TAGS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCKTJOdgC5eC"
      },
      "source": [
        "####  **SequenceTaggingDataset**\n",
        "\n",
        "`SequenceTaggingDataset` es una clase de torchtext dise√±ada para contener datasets de sequence labeling. Los ejemplos que se guarden en una instancia de estos ser√°n arreglos de palabras asociados con sus respectivos tags.\n",
        "\n",
        "Por ejemplo, para Part-of-speech tagging:\n",
        "\n",
        "[I, love, PyTorch, .] estar√° asociado con [PRON, VERB, PROPN, PUNCT]\n",
        "\n",
        "\n",
        "La idea es que usando los fields que definimos antes, le indiquemos a la clase c√≥mo cargar los datasets de prueba, validaci√≥n y test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsHdGml62J21"
      },
      "source": [
        "train_data, valid_data, test_data = legacy.datasets.SequenceTaggingDataset.splits(\n",
        "    path=\"./\",\n",
        "    train=\"train.txt\",\n",
        "    validation=\"dev.txt\",\n",
        "    test=\"test.txt\",\n",
        "    fields=fields,\n",
        "    encoding=\"utf-8\",\n",
        "    separator=\" \"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu7q3HCliia5",
        "outputId": "77d1fb6e-a047-449c-c01c-4f83bb77c993"
      },
      "source": [
        "print(f\"Numero de ejemplos de entrenamiento: {len(train_data)}\")\n",
        "print(f\"N√∫mero de ejemplos de validaci√≥n: {len(valid_data)}\")\n",
        "print(f\"N√∫mero de ejemplos de test (competencia): {len(test_data)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numero de ejemplos de entrenamiento: 8025\n",
            "N√∫mero de ejemplos de validaci√≥n: 891\n",
            "N√∫mero de ejemplos de test (competencia): 992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDRnhXAdFGL-"
      },
      "source": [
        "Visualizemos un ejemplo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T023Ld4RaSF4",
        "scrolled": false,
        "outputId": "fcb246bb-6de3-4111-f196-2765bcbcd846"
      },
      "source": [
        "import random\n",
        "random_item_idx = random.randint(0, len(train_data))\n",
        "random_example = train_data.examples[random_item_idx]\n",
        "list(zip(random_example.text, random_example.nertags))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Clase', 'O'),\n",
              " ('I', 'O'),\n",
              " ('molar', 'O'),\n",
              " ('Otras', 'O'),\n",
              " ('consultas', 'O'),\n",
              " ('especificadas', 'O')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l05KYy5FSUy"
      },
      "source": [
        "#### **Construir los vocabularios para el texto y las etiquetas**\n",
        "\n",
        "Los vocabularios son los objetos que contienen todos los tokens (de entrenamiento) posibles para ambos fields. El siguiente paso consiste en construirlos. Para esto, hacemos uso del m√©todo `Field.build_vocab` sobre cada uno de nuestros `fields`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBhp7WICiibL"
      },
      "source": [
        "TEXT.build_vocab(train_data)\n",
        "NER_TAGS.build_vocab(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4OgUKM_iibO",
        "scrolled": true,
        "outputId": "e700db6f-23f6-4ae6-ca01-38ad93fd67bf"
      },
      "source": [
        "print(f\"Tokens √∫nicos en TEXT: {len(TEXT.vocab)}\")\n",
        "print(f\"Tokens √∫nicos en NER_TAGS: {len(NER_TAGS.vocab)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens √∫nicos en TEXT: 17591\n",
            "Tokens √∫nicos en NER_TAGS: 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4FeyL9nFnId",
        "outputId": "92b17be6-bf56-4da2-bd16-ca916479dd0b"
      },
      "source": [
        "#Veamos las posibles etiquetas que hemos cargado:\n",
        "NER_TAGS.vocab.itos"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<pad>',\n",
              " 'O',\n",
              " 'I-Disease',\n",
              " 'B-Disease',\n",
              " 'I-Body_Part',\n",
              " 'B-Body_Part',\n",
              " 'B-Procedure',\n",
              " 'I-Procedure',\n",
              " 'B-Medication',\n",
              " 'B-Family_Member',\n",
              " 'I-Medication',\n",
              " 'I-Family_Member']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYQDoUqSHFKj"
      },
      "source": [
        "Observen que ademas de los tags NER, tenemos \\<pad\\>, el cual es generado por el dataloader para cumplir con el padding de cada oraci√≥n.\n",
        "\n",
        "Veamos ahora los tokens mas frecuentes y especiales:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5eSLm4diibR",
        "outputId": "c9d0bd39-be94-482c-bfc0-d16d38dfe7cd"
      },
      "source": [
        "# Tokens mas frecuentes (Ser√° necesario usar stopwords, eliminar s√≠mbolos o nos entregan informaci√≥n (?) )\n",
        "TEXT.vocab.freqs.most_common(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('.', 7396),\n",
              " (',', 6821),\n",
              " ('-', 4985),\n",
              " ('de', 3811),\n",
              " ('DE', 3645),\n",
              " ('/', 2317),\n",
              " (':', 2209),\n",
              " ('con', 1484),\n",
              " ('y', 1439),\n",
              " ('APS', 1429)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDyNLMPz9duD"
      },
      "source": [
        "# Seteamos algunas variables que nos ser√°n de utilidad mas adelante...\n",
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "\n",
        "PAD_TAG_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "O_TAG_IDX = NER_TAGS.vocab.stoi['O']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrYvF3X0sjWL"
      },
      "source": [
        "#### **Frecuencia de los Tags**\n",
        "\n",
        "Visualizemos r√°pidamente las cantidades y frecuencias de cada tag:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuXOsbJUiibh",
        "outputId": "5f04c127-505b-4939-be53-9cfbf263f345"
      },
      "source": [
        "def tag_percentage(tag_counts):\n",
        "    \n",
        "    total_count = sum([count for tag, count in tag_counts])\n",
        "    tag_counts_percentages = [(tag, count, count/total_count) for tag, count in tag_counts]\n",
        "  \n",
        "    return tag_counts_percentages\n",
        "\n",
        "print(\"Tag Ocurrencia Porcentaje\\n\")\n",
        "\n",
        "for tag, count, percent in tag_percentage(NER_TAGS.vocab.freqs.most_common()):\n",
        "    print(f\"{tag}\\t{count}\\t{percent*100:4.1f}%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tag Ocurrencia Porcentaje\n",
            "\n",
            "O\t101671\t68.1%\n",
            "I-Disease\t21629\t14.5%\n",
            "B-Disease\t8831\t 5.9%\n",
            "I-Body_Part\t6489\t 4.3%\n",
            "B-Body_Part\t3755\t 2.5%\n",
            "B-Procedure\t2891\t 1.9%\n",
            "I-Procedure\t2819\t 1.9%\n",
            "B-Medication\t784\t 0.5%\n",
            "B-Family_Member\t228\t 0.2%\n",
            "I-Medication\t116\t 0.1%\n",
            "I-Family_Member\t9\t 0.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4wPiydnaSGs"
      },
      "source": [
        "#### **Configuramos pytorch y dividimos los datos.**\n",
        "\n",
        "Importante: si tienes problemas con la ram de la gpu, disminuye el tama√±o de los batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uB7cwLWpaSGs",
        "outputId": "a9bf7301-cdbc-4458-fb68-b3b013d9a313"
      },
      "source": [
        "BATCH_SIZE = 16  # disminuir si hay problemas de ram.\n",
        "\n",
        "# Usar cuda si es que est√° disponible.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using', device)\n",
        "\n",
        "# Dividir datos entre entrenamiento y test. Si van a hacer alg√∫n sort no puede ser sobre\n",
        "# el conjunto de testing ya que al hacer sus predicciones sobre el conjunto de test sin etiquetas\n",
        "# debe conservar el orden original para ser comparado con los golden_labels. \n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = legacy.data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    sort=False,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B21E1eAFId16"
      },
      "source": [
        "#### **M√©tricas de evaluaci√≥n**\n",
        "\n",
        "Adem√°s, definiremos las m√©tricas que ser√°n usadas tanto para la competencia como para evaluar el modelo: `precision`, `recall` y `micro f1-score`.\n",
        "**Importante**: Noten que la evaluaci√≥n solo se hace para las Named Entities (sin contar 'O'), toda esta funcionalidad nos la entrega la librer√≠a seqeval, pueden revisar m√°s documentaci√≥n aqu√≠: https://github.com/chakki-works/seqeval. No utilicen el c√≥digo entregado por sklearn para calcular las m√©tricas ya que esta lo hace a nivel de token y no a nivel de entidad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o63ov69_rX2T",
        "outputId": "46c1e752-dfa7-4aa9-f8ab-4ffc86a0b682"
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 43 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=8fdf71693f512ad2297c91725ade12053b508703cb1e65974bf81df07c9b1dbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mUOOLEWiicU"
      },
      "source": [
        "# Definimos las m√©tricas\n",
        "\n",
        "from seqeval.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def calculate_metrics(preds, y_true, pad_idx=PAD_TAG_IDX, o_idx=O_TAG_IDX):\n",
        "    \"\"\"\n",
        "    Calcula precision, recall y f1 de cada batch.\n",
        "    \"\"\"\n",
        "\n",
        "    # Obtener el indice de la clase con probabilidad mayor. (clases)\n",
        "    y_pred = preds.argmax(dim=1, keepdim=True)\n",
        "\n",
        "    # filtramos <pad> para calcular los scores.\n",
        "    mask = [(y_true != pad_idx)]\n",
        "    y_pred = y_pred[mask]\n",
        "    y_true = y_true[mask]\n",
        "\n",
        "    # traemos a la cpu\n",
        "    y_pred = y_pred.view(-1).to('cpu').numpy()\n",
        "    y_true = y_true.to('cpu').numpy()\n",
        "    y_pred = [[NER_TAGS.vocab.itos[v] for v in y_pred]]\n",
        "    y_true = [[NER_TAGS.vocab.itos[v] for v in y_true]]\n",
        "    \n",
        "    # calcular scores\n",
        "    f1 = f1_score(y_true, y_pred, mode='strict')\n",
        "    precision = precision_score(y_true, y_pred, mode='strict')\n",
        "    recall = recall_score(y_true, y_pred, mode='strict')\n",
        "\n",
        "    return precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hod516H1aSG2"
      },
      "source": [
        "-------------------\n",
        "\n",
        "### **Modelo Baseline**\n",
        "\n",
        "Teniendo ya cargado los datos, toca definir nuestro modelo. Este baseline tendr√° una capa de embedding, unas cuantas LSTM y una capa de salida y usar√° dropout en el entrenamiento.\n",
        "\n",
        "Este constar√° de los siguientes pasos: \n",
        "\n",
        "1. Definir la clase que contendr√° la red.\n",
        "2. Definir los hiperpar√°metros e inicializar la red. \n",
        "3. Definir el n√∫mero de √©pocas de entrenamiento\n",
        "4. Definir la funci√≥n de loss.\n",
        "\n",
        "\n",
        "\n",
        "Recomendamos que para experimentar, encapsules los modelos en una sola variable y luego la fijes en model para entrenarla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMPL08XqaSG3"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# Definir la red\n",
        "class NER_RNN(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 embedding_dim, \n",
        "                 hidden_dim, \n",
        "                 output_dim,\n",
        "                 n_layers, \n",
        "                 bidirectional, \n",
        "                 dropout, \n",
        "                 pad_idx):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(input_dim,\n",
        "                                      embedding_dim,\n",
        "                                      padding_idx=pad_idx,\n",
        "                                      )\n",
        "\n",
        "        # Capa LSTM\n",
        "        self.lstm = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=n_layers,\n",
        "                           bidirectional=bidirectional, \n",
        "                           dropout = dropout if n_layers > 1 else 0)\n",
        "\n",
        "        # Capa de salida\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim,\n",
        "                            output_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, text):\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Convertir lo enviado a embedding\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        \n",
        "        outputs, (hidden, cell) = self.lstm(embedded)\n",
        "        #embedded = [sent len, batch size, emb dim]\n",
        "\n",
        "        # Pasar los embeddings por la rnn (LSTM)\n",
        "\n",
        "        #output = [sent len, batch size, hid dim * n directions]\n",
        "        #hidden/cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        # Predecir usando la capa de salida.\n",
        "        predictions = self.fc(self.dropout(outputs))\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "\n",
        "        return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCl3530VaSG7"
      },
      "source": [
        "#### **Hiperpar√°metros de la red**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHdi3QdOaSG8"
      },
      "source": [
        "# tama√±o del vocabulario. recuerden que la entrada son vectores bag of word(one-hot).\n",
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 200  # dimensi√≥n de los embeddings.\n",
        "HIDDEN_DIM = 128  # dimensi√≥n de la capas LSTM\n",
        "OUTPUT_DIM = len(NER_TAGS.vocab)  # n√∫mero de clases\n",
        "\n",
        "N_LAYERS = 3  # n√∫mero de capas.\n",
        "DROPOUT = 0.5\n",
        "BIDIRECTIONAL = False\n",
        "\n",
        "# Creamos nuestro modelo.\n",
        "baseline_model = NER_RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM,\n",
        "                         N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)\n",
        "\n",
        "baseline_model_name = 'baseline'  # nombre que tendr√° el modelo guardado..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlF1DhJeaSHA"
      },
      "source": [
        "baseline_n_epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3u4imJGaSHE"
      },
      "source": [
        "#### Definimos la funci√≥n de loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G_4k99_aSHG"
      },
      "source": [
        "# Loss: Cross Entropy\n",
        "TAG_PAD_IDX = NER_TAGS.vocab.stoi[NER_TAGS.pad_token]\n",
        "baseline_criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRYOEDiQaSHK"
      },
      "source": [
        "--------------------\n",
        "### Modelo 1\n",
        "\n",
        "En estas secciones pueden implementar nuevas redes al modificar los hiperpar√°metros, la cantidad de √©pocas de entrenamiento, el tama√±o de los batches, loss, optimizador, etc... como tambi√©n definir nuevas arquitecturas de red (mediante la creaci√≥n de clases nuevas)\n",
        "\n",
        "\n",
        "Al final de estas, hay 4 variables, las cuales deben setear con los modelos, √©pocas de entrenamiento, loss y optimizador que deseen probar.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c81f8ki5aSHL"
      },
      "source": [
        "# model_1 = ...\n",
        "# model_name_1 = ...\n",
        "# n_epochs_1 = ...\n",
        "# loss_1 = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rV9oLkN1aSHO"
      },
      "source": [
        "---------------\n",
        "\n",
        "### Modelo 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWPzETaNaSHP"
      },
      "source": [
        "# model_2 = ...\n",
        "# model_name_2 = ...\n",
        "# n_epochs_2 = ...\n",
        "# loss_2 = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpy3p7YaaSHT"
      },
      "source": [
        "---------------\n",
        "\n",
        "\n",
        "### Modelo 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w0CFjA8aSHU"
      },
      "source": [
        "# modelo_3 = ...\n",
        "# model_name_3 = ...\n",
        "# n_epochs_3 = ...\n",
        "# loss_3 = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPGdirx7aSHZ"
      },
      "source": [
        "------\n",
        "### **Entrenamos y evaluamos**\n",
        "\n",
        "\n",
        "**Importante** : Fijen el modelo, el n√∫mero de √©pocas de entrenamiento, la loss y el optimizador que usar√°n para entrenar y evaluar en las siguientes variables!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8YlGnjxaSHZ"
      },
      "source": [
        "model = baseline_model\n",
        "model_name = baseline_model_name\n",
        "criterion = baseline_criterion\n",
        "n_epochs = baseline_n_epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pu_lXic2aSHd"
      },
      "source": [
        "\n",
        "\n",
        "#### **Inicializamos la red**\n",
        "\n",
        "Iniciamos los pesos de la red de forma aleatoria (Usando una distribuci√≥n normal).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-G_NWFcaSHe",
        "outputId": "e62d50c3-aa76-49ba-e513-573532a23345"
      },
      "source": [
        "def init_weights(m):\n",
        "    # Inicializamos los pesos como aleatorios\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.normal_(param.data, mean=0, std=0.1) \n",
        "        \n",
        "    # Seteamos como 0 los embeddings de UNK y PAD.\n",
        "    model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "    model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NER_RNN(\n",
              "  (embedding): Embedding(17591, 200, padding_idx=1)\n",
              "  (lstm): LSTM(200, 128, num_layers=3, dropout=0.5)\n",
              "  (fc): Linear(in_features=128, out_features=12, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjWDX2CJaSHh",
        "outputId": "ffbe0b99-f020-4717-8eaf-708dc3efd61d"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'El modelo actual tiene {count_parameters(model):,} par√°metros entrenables.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El modelo actual tiene 3,952,900 par√°metros entrenables.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVqBqerlaSHk"
      },
      "source": [
        "Notar que definimos los embeddings que representan a \\<unk\\> y \\<pad\\>  como [0, 0, ..., 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVZvHtwpaSHq"
      },
      "source": [
        "#### **Definimos el optimizador**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AH6o8_cTaSHq"
      },
      "source": [
        "# Optimizador\n",
        "optimizer = optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz39wa78wGYR"
      },
      "source": [
        "#### **Enviamos el modelo a cuda**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqr0AJ6_iicR"
      },
      "source": [
        "# Enviamos el modelo y la loss a cuda (en el caso en que est√© disponible)\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xlq48WjiW6U"
      },
      "source": [
        "#### **Definimos el entrenamiento de la red**\n",
        "\n",
        "Algunos conceptos previos: \n",
        "\n",
        "- `epoch` : una pasada de entrenamiento completa de una dataset.\n",
        "- `batch`: una fracci√≥n de la √©poca. Se utilizan para entrenar mas r√°pidamente la red. (mas eficiente pasar n datos que uno en cada ejecuci√≥n del backpropagation)\n",
        "\n",
        "Esta funci√≥n est√° encargada de entrenar la red en una √©poca. Para esto, por cada batch de la √©poca actual, predice los tags del texto, calcula su loss y luego hace backpropagation para actualizar los pesos de la red.\n",
        "\n",
        "Observaci√≥n: En algunos comentarios aparecer√° el tama√±o de los tensores entre corchetes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DV6YLt0oiicW"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Por cada batch del iterador de la √©poca:\n",
        "    for batch in iterator:\n",
        "\n",
        "        # Extraemos el texto y los tags del batch que estamos procesado\n",
        "        text = batch.text\n",
        "        tags = batch.nertags\n",
        "\n",
        "        # Reiniciamos los gradientes calculados en la iteraci√≥n anterior\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        #text = [sent len, batch size]\n",
        "\n",
        "        # Predecimos los tags del texto del batch.\n",
        "        predictions = model(text)\n",
        "\n",
        "        #predictions = [sent len, batch size, output dim]\n",
        "        #tags = [sent len, batch size]\n",
        "\n",
        "        # Reordenamos los datos para calcular la loss\n",
        "        predictions = predictions.view(-1, predictions.shape[-1])\n",
        "        tags = tags.view(-1)\n",
        "\n",
        "        #predictions = [sent len * batch size, output dim]\n",
        "\n",
        "\n",
        "\n",
        "        # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "        loss = criterion(predictions, tags)\n",
        "        \n",
        "        # Calculamos el accuracy\n",
        "        precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "        # Calculamos los gradientes\n",
        "        loss.backward()\n",
        "\n",
        "        # Actualizamos los par√°metros de la red\n",
        "        optimizer.step()\n",
        "\n",
        "        # Actualizamos el loss y las m√©tricas\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_precision += precision\n",
        "        epoch_recall += recall\n",
        "        epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYNcwKnAz5Hf"
      },
      "source": [
        "#### **Definimos la funci√≥n de evaluaci√≥n**\n",
        "\n",
        "Evalua el rendimiento actual de la red usando los datos de validaci√≥n. \n",
        "\n",
        "Por cada batch de estos datos, calcula y reporta el loss y las m√©tricas asociadas al conjunto de validaci√≥n. \n",
        "Ya que las m√©tricas son calculadas por cada batch, estas son retornadas promediadas por el n√∫mero de batches entregados. (ver linea del return)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsRuiUuHiicY"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Indicamos que ahora no guardaremos los gradientes\n",
        "    with torch.no_grad():\n",
        "        # Por cada batch\n",
        "        for batch in iterator:\n",
        "\n",
        "            text = batch.text\n",
        "            tags = batch.nertags\n",
        "\n",
        "            # Predecimos\n",
        "            predictions = model(text)\n",
        "\n",
        "            predictions = predictions.view(-1, predictions.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "\n",
        "            # Calculamos el Cross Entropy de las predicciones con respecto a las etiquetas reales\n",
        "            loss = criterion(predictions, tags)\n",
        "\n",
        "            # Calculamos las m√©tricas\n",
        "            precision, recall, f1 = calculate_metrics(predictions, tags)\n",
        "\n",
        "            # Actualizamos el loss y las m√©tricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_precision += precision\n",
        "            epoch_recall += recall\n",
        "            epoch_f1 += f1\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_precision / len(\n",
        "        iterator), epoch_recall / len(iterator), epoch_f1 / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xs-n9Y5yiica"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy3MVf5H0A94"
      },
      "source": [
        "\n",
        "#### **Entrenamiento de la red**\n",
        "\n",
        "En este cuadro de c√≥digo ejecutaremos el entrenamiento de la red.\n",
        "Para esto, primero definiremos el n√∫mero de √©pocas y luego por cada √©poca, ejecutaremos `train` y `evaluate`.\n",
        "\n",
        "**Importante: Reiniciar los pesos del modelo**\n",
        "\n",
        "Si ejecutas nuevamente esta celda, se seguira entrenando el mismo modelo una y otra vez. \n",
        "Para reiniciar el modelo se debe ejecutar nuevamente la celda que contiene la funci√≥n `init_weights`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK5lQqpviicf",
        "outputId": "7789af98-1af8-4a38-a82a-7d9798a3347a"
      },
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Recuerdo: train_iterator y valid_iterator contienen el dataset dividido en batches.\n",
        "\n",
        "    # Entrenar\n",
        "    train_loss, train_precision, train_recall, train_f1 = train(\n",
        "        model, train_iterator, optimizer, criterion)\n",
        "\n",
        "    # Evaluar (valid = validaci√≥n)\n",
        "    valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "        model, valid_iterator, criterion)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    # Si obtuvimos mejores resultados, guardamos este modelo en el almacenamiento (para poder cargarlo luego)\n",
        "    # Si detienen el entrenamiento prematuramente, pueden cargar el modelo en el siguiente recuadro de c√≥digo.\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), '{}.pt'.format(model_name))\n",
        "    # Si ya no mejoramos el loss de validaci√≥n, terminamos de entrenar.\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(\n",
        "        f'\\tTrain Loss: {train_loss:.3f} | Train f1: {train_f1:.2f} | Train precision: {train_precision:.2f} | Train recall: {train_recall:.2f}'\n",
        "    )\n",
        "    print(\n",
        "        f'\\t Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} |  Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.940 | Train f1: 0.29 | Train precision: 0.47 | Train recall: 0.23\n",
            "\t Val. Loss: 0.606 |  Val. f1: 0.53 |  Val. precision: 0.73 | Val. recall: 0.43\n",
            "Epoch: 02 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.533 | Train f1: 0.62 | Train precision: 0.70 | Train recall: 0.56\n",
            "\t Val. Loss: 0.464 |  Val. f1: 0.66 |  Val. precision: 0.74 | Val. recall: 0.60\n",
            "Epoch: 03 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.397 | Train f1: 0.72 | Train precision: 0.75 | Train recall: 0.69\n",
            "\t Val. Loss: 0.436 |  Val. f1: 0.70 |  Val. precision: 0.73 | Val. recall: 0.67\n",
            "Epoch: 04 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.325 | Train f1: 0.77 | Train precision: 0.79 | Train recall: 0.76\n",
            "\t Val. Loss: 0.402 |  Val. f1: 0.72 |  Val. precision: 0.75 | Val. recall: 0.69\n",
            "Epoch: 05 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.275 | Train f1: 0.81 | Train precision: 0.81 | Train recall: 0.80\n",
            "\t Val. Loss: 0.387 |  Val. f1: 0.73 |  Val. precision: 0.74 | Val. recall: 0.72\n",
            "Epoch: 06 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.238 | Train f1: 0.83 | Train precision: 0.83 | Train recall: 0.83\n",
            "\t Val. Loss: 0.411 |  Val. f1: 0.73 |  Val. precision: 0.74 | Val. recall: 0.72\n",
            "Epoch: 07 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.208 | Train f1: 0.85 | Train precision: 0.86 | Train recall: 0.85\n",
            "\t Val. Loss: 0.417 |  Val. f1: 0.73 |  Val. precision: 0.74 | Val. recall: 0.73\n",
            "Epoch: 08 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.191 | Train f1: 0.86 | Train precision: 0.87 | Train recall: 0.86\n",
            "\t Val. Loss: 0.428 |  Val. f1: 0.73 |  Val. precision: 0.73 | Val. recall: 0.74\n",
            "Epoch: 09 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.176 | Train f1: 0.87 | Train precision: 0.87 | Train recall: 0.87\n",
            "\t Val. Loss: 0.431 |  Val. f1: 0.75 |  Val. precision: 0.76 | Val. recall: 0.74\n",
            "Epoch: 10 | Epoch Time: 0m 8s\n",
            "\tTrain Loss: 0.159 | Train f1: 0.89 | Train precision: 0.89 | Train recall: 0.89\n",
            "\t Val. Loss: 0.455 |  Val. f1: 0.74 |  Val. precision: 0.72 | Val. recall: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcZPraG-9duO"
      },
      "source": [
        "**Importante**: Recuerden que el √∫ltimo modelo entrenado no es el mejor (probablemente est√© *overfitteado*), si no el que guardamos con la menor loss del conjunto de validaci√≥n. Este problema lo pueden solucionar con *early stopping*.\n",
        "Para cargar el mejor modelo entrenado, ejecuten la siguiente celda.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y27CNYfrjtQ-"
      },
      "source": [
        "# cargar el mejor modelo entrenado.\n",
        "model.load_state_dict(torch.load('{}.pt'.format(model_name)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLuqFKFR9duO"
      },
      "source": [
        "# Limpiar ram de cuda\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBctQHTh0lxD"
      },
      "source": [
        "#### **Evaluamos el set de validaci√≥n con el modelo final**\n",
        "\n",
        "Estos son los resultados de predecir el dataset de evaluaci√≥n con el *mejor* modelo entrenado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0gVbP8yiicj"
      },
      "source": [
        "valid_loss, valid_precision, valid_recall, valid_f1 = evaluate(\n",
        "    model, valid_iterator, criterion)\n",
        "\n",
        "print(\n",
        "    f'Val. Loss: {valid_loss:.3f} |  Val. f1: {valid_f1:.2f} | Val. precision: {valid_precision:.2f} | Val. recall: {valid_recall:.2f}'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF1ysw_Kw6zz"
      },
      "source": [
        "### **Predecir datos para la competencia**\n",
        "\n",
        "Ahora, a partir de los datos de **test** y nuestro modelo entrenado, vamos a predecir las etiquetas que ser√°n evaluadas en la competencia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RBs3UU4wLk3"
      },
      "source": [
        "def predict_labels(model, iterator, criterion, fields=fields):\n",
        "\n",
        "    # Extraemos los vocabularios.\n",
        "    text_field = fields[0][1]\n",
        "    nertags_field = fields[1][1]\n",
        "    tags_vocab = nertags_field.vocab.itos\n",
        "    words_vocab = text_field.vocab.itos\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for batch in iterator:\n",
        "\n",
        "            text_batch = batch.text\n",
        "            text_batch = torch.transpose(text_batch, 0, 1).tolist()\n",
        "\n",
        "            # Predecir los tags de las sentences del batch\n",
        "            predictions_batch = model(batch.text)\n",
        "            predictions_batch = torch.transpose(predictions_batch, 0, 1)\n",
        "\n",
        "            # por cada oraci√≥n predicha:\n",
        "            for sentence, sentence_prediction in zip(text_batch,\n",
        "                                                     predictions_batch):\n",
        "                for word_idx, word_predictions in zip(sentence,\n",
        "                                                      sentence_prediction):\n",
        "                    # Obtener el indice del tag con la probabilidad mas alta.\n",
        "                    argmax_index = word_predictions.topk(1)[1]\n",
        "\n",
        "                    current_tag = tags_vocab[argmax_index]\n",
        "                    # Obtenemos la palabra\n",
        "                    current_word = words_vocab[word_idx]\n",
        "\n",
        "                    if current_word != '<pad>':\n",
        "                        predictions.append([current_word, current_tag])\n",
        "                predictions.append(['EOS', 'EOS'])\n",
        "\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "predictions = predict_labels(model, test_iterator, criterion)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwQp1Ru8Oht8"
      },
      "source": [
        "### **Generar el archivo para la submission**\n",
        "\n",
        "No hay problema si aparecen unk en la salida. Estos no son relevantes para evaluarlos, usamos solo los tags."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPfZkjJGkWyq"
      },
      "source": [
        "import os, shutil\n",
        "\n",
        "if (os.path.isfile('./predictions.zip')):\n",
        "    os.remove('./predictions.zip')\n",
        "\n",
        "if (not os.path.isdir('./predictions')):\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "else:\n",
        "    # Eliminar predicciones anteriores:\n",
        "    shutil.rmtree('./predictions')\n",
        "    os.mkdir('./predictions')\n",
        "\n",
        "f = open('predictions/predictions.txt', 'w')\n",
        "for i, (word, tag) in enumerate(predictions[:-1]):\n",
        "    if word=='EOS' and tag=='EOS': f.write('\\n')\n",
        "    else: \n",
        "      if i == len(predictions[:-1])-1:\n",
        "        f.write(word + ' ' + tag)\n",
        "      else: f.write(word + ' ' + tag + '\\n')\n",
        "\n",
        "f.close()\n",
        "\n",
        "a = shutil.make_archive('predictions', 'zip', './predictions')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZEWJXrNaSIf"
      },
      "source": [
        "## **Conclusiones**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAtK7y43V7Z_"
      },
      "source": [
        "    Escriba aqu√≠ sus conclusiones"
      ]
    }
  ]
}