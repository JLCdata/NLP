{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Tarea_3_Word_Embeddings.ipynb","provenance":[{"file_id":"1QROgsPGrHdXWTXt8KUMIdbbudwMXkQEB","timestamp":1651075050468}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ckbt7VPDhBwb"},"source":["# **Tarea 3 - Word Embeddings 📚**\n","\n","**Integrantes:** Sebastián Tinoco, José Luis Cádiz\n","\n","**Fecha límite de entrega 📆:** 3 de mayo.\n","\n","**Tiempo estimado de dedicación:** 200 minutos"]},{"cell_type":"markdown","metadata":{"ExecuteTime":{"end_time":"2020-03-19T18:30:18.109327Z","start_time":"2020-03-19T18:30:18.103344Z"},"id":"q5CSRY4oNCHK"},"source":["\n","**Instrucciones:**\n","- El ejercicio consiste en:\n","    - Responder preguntas relativas a los contenidos vistos en los vídeos y slides de las clases. \n","    - Entrenar Word2Vec y Word Context Matrix sobre un pequeño corpus.\n","    - Evaluar los embeddings obtenidos en una tarea de clasificación.\n","- La tarea se realiza en grupos de **máximo** 2 personas. Puede ser invidivual pero no es recomendable.\n","- La entrega es a través de u-cursos a más tardar el día estipulado arriba. No se aceptan atrasos.\n","- El formato de entrega es este mismo Jupyter Notebook.\n","- Al momento de la revisión tu código será ejecutado. Por favor verifica que tu entrega no tenga errores de compilación. \n","- En el horario de auxiliar pueden realizar consultas acerca de la tarea a través del canal de Discord del curso. \n","\n","\n","**Referencias**\n","\n","Vídeos: \n","\n","- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n","- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n","- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"]},{"cell_type":"markdown","metadata":{"id":"G4wYf0vgnbTv"},"source":["## **Preguntas teóricas 📕 (2 puntos).** ##\n","Para estas preguntas no es necesario implementar código, pero pueden utilizar pseudo código."]},{"cell_type":"markdown","metadata":{"id":"B5hUG6-8ngoK"},"source":["### **Parte 1: Modelos Lineales (1 ptos)**"]},{"cell_type":"markdown","metadata":{"id":"5yRvZbhsoi8f"},"source":["Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categorías: política, deporte, negocios y otros. "]},{"cell_type":"markdown","metadata":{"id":"irsqBVmCnx3M"},"source":["**Pregunta 1**: Diseñe un modelo lineal capaz de clasificar un documento según estas categorías donde el output sea un vector con una distribución de probabilidad con la pertenencia a cada clase. \n","\n","Especifique: representación de los documentos de entrada, parámetros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y función de pérdida escogida. **(0.5 puntos)**\n","\n","**Respuesta**: A continuación se enumeran los pasos a seguir para diseñar un modelo ilneal que clasifique documentos.\n","1. **Representación**: Una representación básica podría ser One-Hot Vector, el cual consiste en representar cada oración del corpus como un vector sparse del tamaño del vacabulario, el cual tiene un 1 si la palabra aparece y 0 si no. Otra opción es Bag-of-Words (BOW), el cual también representa las oraciones como vectores sparse pero intenta capturar además, el orden de las palabras a traves de bigramas,trigramas etc. Esta representación vendrá dada por\n","el vector $\\vec{x}$.\n","\n","2. **Modelo**: Consideremos un modelo lineal de la forma $\\vec{\\hat{y}}=f(x)=\\vec{x}\\cdot W+\\vec{b}$, donde $\\vec{x}$ es el input. $W$, $\\vec{b}$ son parámetros del modelo e $\\vec{\\hat{y}}$ es el Output de dimensión igual al número de etiquetas de la tarea de clasificación, en este caso dimensión 4.\n","\n","\n","3. **Transformaciones del Ouput para obtener probabilidad de etiqueta**: Para obtener una representación probabilistica del Output del modelo lineal, dicho Ouput es pasado por una función $\\textit{Softmax}$ :\n","\n","$\\vec{\\hat{y}}=\\textit{Softmax}(\\vec{x}\\cdot W+\\vec{b})$, donde la predicción estará dada por:  $Prediction=\\hat{y}=argmax_{i}(\\vec{\\hat{y}}_{[i]})$, es decir, se asigna la etiqueta que tenga mayor probabilidad.\n","\n","\n","4. **Entrenamiento**: para mejorar la calidad de las predicciones se deben ajustar los parámetros del modelo de modo de maximizar la capacidad predictiva del modelo. Para esto es de vital importancia definir una\n","función de pérdida o también conocida como $\\textit{Loss}$. Para una predicción dada, La función de $\\textit{Loss}$, puede ser definida como\n","$\\textit{Loss}$=$L(f(\\vec{x},\\vec{\\Theta}),y_{true})$=$L(\\hat{y},y_{true})$, donde $\\vec{\\Theta}$ representa los parámetros del modelo, $\\hat{y}$ la predicción e $y_{true}$ la etiqueta correcta. Para considerar todas las predicciones que se harían sobre el conjunto de entrenamiento se define la función de $\\textit{Loss}$ para todo el $\\textit{Corpus}$: $\\mathcal{L}(\\vec{\\Theta})=\\frac{1}{N}\\sum_{i=1}^{N}L(\\hat{y_{i}},y_{true_{i}})$. Una función de pérdida adecuada para la tarea de clasificación es la conocida $\\textit{Cross-Entropy-Loss}$. "]},{"cell_type":"markdown","metadata":{"id":"G5FaWqBVvL90"},"source":["**Pregunta 2**: Explique cómo funciona el proceso de entrenamiento en este tipo de modelos y su evaluación. **(0.5 puntos)**\n","\n","**Respuesta**: Como se menciono anteriormente, para el proceso de entrenamiento se debe definir una función de pérdida a optimizar con el objetivo de obtener los parámetros $\\vec{\\Theta}$ que minimicen la función de pérdida. \n","Matemáticamente estos parámetros estarán dados por la siguiente expresión:\n","$$\\hat{\\Theta}=\\textit{Argmin}_{\\vec{\\Theta}}\\mathcal{L}(\\vec{\\Theta})=\\textit{Argmin}_{\\vec{\\Theta}}\\frac{1}{N}\\sum_{i=1}^{N}L(\\hat{y_{i}},y_{true_{i}})$$\n","\n","\n","En este caso para la tarea que se busca cumplir una buena función de pérdida es  $\\textit{Cross-Entropy-Loss}$, la cual cuantifica la no similitud entre la etiqueta real y la etiqueta predicha, matemáticamente esta definida por la siguiente expresión:\n","$$\\mathcal{L}_{cross-entropy}(\\vec{\\hat{y}},\\vec{y}_{true})=-\\sum_{i=1}^{N}\\vec{y}_{true_{i}}\\cdot\\log(\\vec{\\hat{y_{i}}})$$ \n","\n","El método de optimización utilizado estan basandos en los métodos basados en gradientes. Estos métodos son más económicos computacionalmente que derivar sobre todas las variables y calcular las diversas soluciones de cada derivada parcial particular. Un ejemplo básico de este tipo de métodos es el $\\textit{``Online Stochastic Gradient Descent''} $. \n","\n","Este método consiste en inicializar los parámetros del modelo con valores aleatorios y mientras no se cumpla un criterio de stop, como por ejemplo que la variación porcentual entre el $\\vec{\\Theta}$ anterior y el $\\vec{\\Theta}$ actualizado no sea lo suficiemente pequeño a un $\\vec{\\varepsilon}$, se iterará sobre cada registro del conjunto de entrenamiento, calculando la predicción que haría el modelo, luego se computa el error mediante la función de pérdida y la etiqueta verdadera, para luego calcular el gradiente respecto a los parámetros $\\vec{\\Theta}$. Luego $\\vec{\\Theta}$ es actualizado: $\\vec{\\Theta}=\\vec{\\Theta}-\\eta\\cdot\\nabla\\mathcal{L}$, donde $\\eta$ se conoce como la tasa de aprendizaje. Esta actualización continuará hasta que se cumpla el criterio de stop, con esto obteniendo los parámetros del modelo ya entrenado.\n","\n","Para evaluar el modelo, se debe contar con un conjunto de test, totalmente distinto al conjunto de entrenamiento, el cual debe contar también con las etiquetas verdaderas, de modo de hacer predicciones con el modelo ya entrenado, para luego calcular alguna métrica de clasificación como $\\textit{Accuracy, Precision, Recall, F1}$ etc."]},{"cell_type":"markdown","metadata":{"id":"XkK7pc54njZq"},"source":["### **Parte 2: Redes Neuronales (1 ptos)** "]},{"cell_type":"markdown","metadata":{"id":"VUbJjlj_9AFC"},"source":["Supongamos que tenemos la siguiente red neuronal."]},{"cell_type":"markdown","metadata":{"id":"obUfuOYB_TOC"},"source":["![image.png](https://drive.google.com/uc?export=view&id=1fFTjtMvH6MY8o42_vj010y8eTuCVb5a3)"]},{"cell_type":"markdown","metadata":{"id":"s2z-8zKW0_6q"},"source":["**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matemática. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en función del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n","\n","Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.5 Puntos)**\n","\n","**Respuesta**: \n","\n","Formula:\n","$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) =\\vec{h}^{3}\\cdot\\vec{W}^{4}$, $\\vec{W}^{4}$ dim (1,4).\n","\n","donde:\n","\n","* $\\vec{h}^{3}=h(\\vec{h}^{2}\\cdot\\vec{W}^{3}+\\vec{b}^{3})$, donde $\\vec{W}^{3}$ dim (3,1), $\\vec{b}^{3}$ dim (1,1).\n","* $\\vec{h}^{2}=f(\\vec{h}^{1}\\cdot\\vec{W}^{2}+\\vec{b}^{2})$, donde $\\vec{W}^{2}$ dim (2,3), $\\vec{b}^{2}$ dim (1,3).\n","* $\\vec{h}^{1}=g(\\vec{x}\\cdot\\vec{W}^{1}+\\vec{b}^{1})$, donde $\\vec{W}^{1}$ dim (3,2), $\\vec{b}^{1}$ dim (1,2), $\\vec{x}$ dim (1,3).\n","\n","**Pregunta 2**: Explique qué es backpropagation. ¿Cuales serían los parámetros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n","\n","**Respuesta**: Los parámetros a optimizar dentro de una red neuronal son los pesos sinapticos asociados a las conexiones entre una capa y otra, junto con sus respectivos bias de cada capa. Para optimizar los parámetros de la red neuronal se utilizan métodos del descenso del gradiente, pero el problema es que las redes neuronales se caracterizan por tener una gran cantidad de parámetros, lo que en la practica hace muy costoso cumputacionalmente calcular todas las derivadas. Por esto es necesario calcular estas derivadas de una manera más eficiente. \n","\n","Backpropagation es un técnica que permite obtener las derivadas parciales de una función $\\textit{Loss}$ respecto a todos los parámetros de un modelo de una manera más eficiente guardando las derivadas que se repiten. El gradiente se calcula de forma recursiva, es decir se calculan las derivadas parciales desde las capas más profundas hasta las más superficiales, de modo de ir guardando las derivadas profundas que luego son utilizadas en las derivadas parciales de las capas menos profundas producto de la regla de la cadena. Esto sucede porque se entiende que el error imputado de un parámetro depende de los errores imputados de los parámetros de las capas más profundas, es decir el error imputado de cada parámetro se propaga de adelante hacia atras ($\\textit{Back-Propagation}$). Con esto finalmente, se logra reducir el costo cumputacional al no recalcular derivadas. \n","\n","\n","**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.25 puntos)**\n","\n","**Respuesta**: La idea es calcular el gradiente de manera eficiente, i.e. calcular las derivadas parciales de los parámetros de las capas superiores secuencialmente hasta las capas  inferiores. Del tal modo de aprovechar la dependencia de las derivadas parciales de las capas inferiores respecto a las superiores, reciclando ciertos calculos. Esto en computación se llama programación dinámica.\n","\n","Pasos:\n","\n","1. Se inicializa la red con valores aleatorios para todos sus pesos.\n","2. $\\textbf{Fordward propagation}$ - (Evaluar ejemplos): Para cada ejemplo de los datos de entrenamiento se alimenta la red neuronal, se calculan las funciones de activación desde las capas inferiores hasta las superiores de forma recursiva. según la ecuación:\n","$$h^{l}_{j}=\\left (\\sum_{i}W_{i,j}^{l}\\cdot z_{i}^{l-1}\\right )+b_{j}^{l}\\hspace{0.1cm}donde\\hspace{0.1cm} z_{j}^{l}=g\\left(h_{j}^{l}\\right), z_{j}^{0}=x_{j}$$\n","\n","3. Se obtiene la salida de la red y se evalua la $\\textit{Loss}$, $\\mathcal{L}(\\vec{\\Theta},\\vec{\\hat{y}},\\vec{y}_{true})$, obteniendo una $\\textit{Loss}$ de la forma  $\\mathcal{L}(\\vec{\\Theta})$.\n","4. $\\textbf{Back propagation}$: Calcular el gradiente de $\\mathcal{L}(\\vec{\\Theta})$ mediante el calculo de las derivadas parciales de los parámetros desde las capas superiores hasta las inferiores, guardando las derivadas que se van obteniendo. La derivada de un parámetro $W^{l}_{i,j}$ esta definida por la siguiente expresión:\n","$$\\frac{\\partial\\mathcal{L}}{\\partial W^{l}_{i,j}}=\\delta^{l}_{j}\\cdot z^{l-1}_{i}$$\n","\n","donde $\\delta^{l}_{j}=\\frac{\\partial\\mathcal{L}}{\\partial h^{l}_{j}}=\\sum_{k}\\left( \\delta^{l+1}_{k} \\cdot W^{l+1}_{j,k}\\cdot g'\\left(h^{l}_{j}\\right)    \\right)$, de este modo se deben  obtener los deltas de las capas superiores para obtener los deltas de las capas inferiores. Dicho de otra forma, se deben obtener las derivadas parciales del $\\textit{Loss}$ respecto a la salida de cada neurona desde las capas superiores hasta las capas inferiores. En este caso particular de la capa 3, 2 y 1 de forma secuencial."]},{"cell_type":"markdown","metadata":{"id":"ocS_vQhR1gcU"},"source":["## **Preguntas prácticas 💻 (4 puntos).** ##"]},{"cell_type":"markdown","metadata":{"id":"Ol82nJ0FnmcP"},"source":["### **Parte 3: Word Embeddings**"]},{"cell_type":"markdown","source":["En la auxiliar 2 se nombraron dos formas de crear word vectors:\n","\n","-  Distributional Vectors.\n","-  Distributed Vectors.\n","\n","El objetivo de esta parte es comparar las dos embeddings obtenidos de estas dos estrategias en una tarea de clasificación."],"metadata":{"id":"Daw7Ee5cdQTb"}},{"cell_type":"code","source":["import re  \n","import pandas as pd \n","from time import time  \n","from collections import defaultdict \n","import string \n","import multiprocessing\n","import os\n","import gensim\n","import sklearn\n","from sklearn import linear_model\n","from collections import Counter\n","import numpy as np\n","import scipy\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","# word2vec\n","from gensim.models import Word2Vec, KeyedVectors, FastText\n","from gensim.models.phrases import Phrases, Phraser\n","from sklearn.model_selection import train_test_split\n","import logging\n","\n","logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n","logger = logging.getLogger(__name__)"],"metadata":{"id":"E2G1qcb7AJqW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652042372291,"user_tz":240,"elapsed":65175,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}},"outputId":"ec4ed5cf-d845-440a-f999-50a5e5ee590a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["#### **Parte A (1 punto)** \n","\n","En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librerías ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n","\n","```python\n","class WordContextMatrix:\n","\n","  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n","    # se sugiere agregar un una estructura de datos para guardar las\n","    # palabras del vocab y para guardar el conteo de coocurrencia\n","    ...\n","    \n","  def add_word_to_vocab(self, word):\n","    # Le puede ser útil considerar un token unk al vocab\n","    # para palabras fuera del vocab\n","    ...\n","  \n","  def build_matrix(self):\n","    ...\n","\n","  def matrix2dict(self):\n","    # se recomienda transformar la matrix a un diccionario de embedding.\n","    ...\n","\n","```\n","\n","puede modificar los parámetros o métodos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n","\n","```python\n","corpus = [\n","  \"I like deep learning.\",\n","  \"I like NLP.\",\n","  \"I enjoy flying.\"\n","]\n","```\n","\n","Obteniendo una matriz parecia a esta:\n","\n","***Resultado esperado***: \n","\n","| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n","|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n","| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n","| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n","| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n","| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n","| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n","| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n","| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n","| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n","\n","``"],"metadata":{"id":"AuEAv-whdMCG"}},{"cell_type":"markdown","source":["**Respuesta:**"],"metadata":{"id":"ur16vkyO37B5"}},{"cell_type":"code","source":["import re\n","\n","# tokenizador usado en Tarea 1\n","class CoolTokenizer:\n","  def tokenize(self, text):\n","    ### Inicio del código ###\n","    tokens = re.findall(r\"[\\w']+|[().,!?;-]\", text) # se hace un match a las palabras + puntuaciones (. , ! ? ;)\n","    return tokens\n","    ### Fín del código ###"],"metadata":{"id":"X40l7vtyGSv_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class WordContextMatrix:\n","\n","  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n","    # se sugiere agregar un una estructura de datos para guardar las\n","    # palabras del vocab y para guardar el conteo de coocurrencia\n","    self.vocab_size = vocab_size # tamaño del vocabulario\n","    self.window_size = window_size # tamaño de la ventana\n","    self.dataset = dataset # corpus\n","    self.tokenizer = tokenizer # tokenizer \n","    self.vocab = None # vocabulario del corpus\n","    self.matrix = None # matriz de contexto\n","\n","  def build_matrix(self):\n","    # si vocabulario no tiene tokens, inicializar\n","    if self.matrix is None:\n","      self.get_vocabulary()\n","    \n","    # si vocab_size excede vocabulario del corpus, igualar al vocabulario del corpus\n","    if self.vocab_size >= len(self.vocab):\n","      self.vocab_size = len(self.vocab) - 1\n","\n","    self.matrix = pd.DataFrame(np.zeros((self.vocab_size + 1, self.vocab_size + 1)), columns = self.vocab, index = self.vocab) # init de matriz contexto\n","\n","    for doc in self.dataset: # para cada documento en el corpus\n","      tokens = [token for token in self.tokenizer.tokenize(doc)] # obtengo tokens del doc\n","      for i in range(len(tokens)): # para cada token\n","          LeftWindow = [tokens[i-j] if i-j >= 0 else None for j in range(1, self.window_size+1)] # obtengo la ventana izquierda, tokens con índice fuera del documento son None\n","          RightWindow = [tokens[i+j] if i+j+1 <= len(tokens) else None for j in range(1, self.window_size+1)] # obtengo la ventana derecha, tokens con índice fuera del documento son None\n","\n","          for left_token in LeftWindow: # para cada token de la ventana izquierda\n","            if left_token is not None: # si el token no es None\n","              LeftPair = [tokens[i], left_token] # genero par\n","              LeftPair = [token if token in self.vocab else 'unk' for token in LeftPair] # asigno unk a tokens que no esten en vocabulario\n","              self.matrix.loc[LeftPair[0], LeftPair[1]] += 1 # update a matriz contexto\n","\n","          for right_token in RightWindow: # para cada token de la ventana derecha\n","            if right_token is not None: # si el token no es None\n","              RightPair = [tokens[i], right_token] # genero par\n","              RightPair = [token if token in self.vocab else 'unk' for token in RightPair] # asigno unk a tokens que no esten en vocabulario \n","              self.matrix.loc[RightPair[0], RightPair[1]] += 1 # update a matriz contexto\n","\n","    return self.matrix\n","\n","  def matrix2dict(self):\n","    if self.matrix is not None:\n","      return self.matrix.to_dict()\n","    raise ValueError('Debes entrenar la matriz de contexto primero.')\n","\n","  def get_vocabulary(self):\n","    self.vocab = ['unk'] # init de vocabulario con token 'unk'\n","    for doc in self.dataset: # para cada documento en el corpus\n","      tokens = [token for token in self.tokenizer.tokenize(doc)] # obtengo tokens \n","      for token in tokens: # para cada token del documento\n","        if len(self.vocab) >= self.vocab_size+1: # break si vocabulario excede tamaño del vocabulario deseado\n","          break\n","        if token not in self.vocab: # si token no está en vocabulario\n","          self.vocab.append(token) # agrego token\n","\n","    return self.vocab"],"metadata":{"id":"gOI1FL8MlGZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["corpus = [\n","  \"I like deep learning.\",\n","  \"I like NLP.\",\n","  \"I enjoy flying.\"\n","]\n","\n","context_class = WordContextMatrix(vocab_size = 10, window_size = 1, dataset = corpus, tokenizer = CoolTokenizer())\n","ContextMatrix = context_class.build_matrix()\n","# obtenemos matriz con las columnas e índices ordenados para una mejor comparación :)\n","ContextMatrix[['I', 'like', 'enjoy', 'deep', 'learning', 'NLP', 'flying', '.', 'unk']].reindex(['I', 'like', 'enjoy', 'deep', 'learning', 'NLP', 'flying', '.', 'unk'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":328},"id":"G8Kg2bePuiCE","executionInfo":{"status":"ok","timestamp":1652042372294,"user_tz":240,"elapsed":16,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}},"outputId":"0b2b13eb-eb7c-443e-ff22-ea8543e31d9c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["            I  like  enjoy  deep  learning  NLP  flying    .  unk\n","I         0.0   2.0    1.0   0.0       0.0  0.0     0.0  0.0  0.0\n","like      2.0   0.0    0.0   1.0       0.0  1.0     0.0  0.0  0.0\n","enjoy     1.0   0.0    0.0   0.0       0.0  0.0     1.0  0.0  0.0\n","deep      0.0   1.0    0.0   0.0       1.0  0.0     0.0  0.0  0.0\n","learning  0.0   0.0    0.0   1.0       0.0  0.0     0.0  1.0  0.0\n","NLP       0.0   1.0    0.0   0.0       0.0  0.0     0.0  1.0  0.0\n","flying    0.0   0.0    1.0   0.0       0.0  0.0     0.0  1.0  0.0\n",".         0.0   0.0    0.0   0.0       1.0  1.0     1.0  0.0  0.0\n","unk       0.0   0.0    0.0   0.0       0.0  0.0     0.0  0.0  0.0"],"text/html":["\n","  <div id=\"df-fedfe1b7-431f-412d-a3b9-9097540b88d3\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>I</th>\n","      <th>like</th>\n","      <th>enjoy</th>\n","      <th>deep</th>\n","      <th>learning</th>\n","      <th>NLP</th>\n","      <th>flying</th>\n","      <th>.</th>\n","      <th>unk</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>I</th>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>like</th>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>enjoy</th>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>deep</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>learning</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>NLP</th>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>flying</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>.</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>unk</th>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fedfe1b7-431f-412d-a3b9-9097540b88d3')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-fedfe1b7-431f-412d-a3b9-9097540b88d3 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-fedfe1b7-431f-412d-a3b9-9097540b88d3');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"OgmeSFqKLpFL"},"source":["#### **Parte B (1.5 puntos)**\n","\n","En esta parte es debe entrenar Word2Vec de gensim y construir la matriz palabra contexto utilizando el dataset de diálogos de los Simpson. "]},{"cell_type":"markdown","metadata":{"id":"tZgN06q4QPi3"},"source":["Utilizando el dataset adjunto con la tarea:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"eY3kmg4onnsu","outputId":"6b16140c-4dfd-4018-ccf5-de3aa58146f9","executionInfo":{"status":"ok","timestamp":1652042373819,"user_tz":240,"elapsed":1537,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}}},"source":["path = '/content/drive/MyDrive/MDS/NLP/Tarea 3/'\n","data_file = \"dialogue-lines-of-the-simpsons.zip\"\n","df = pd.read_csv(path + data_file)\n","stopwords = pd.read_csv(\n","    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",").values\n","stopwords = Counter(stopwords.flatten().tolist())\n","df = df.dropna().reset_index(drop=True) # Quitar filas vacias\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        raw_character_text                                       spoken_words\n","0              Miss Hoover  No, actually, it was a little of both. Sometim...\n","1             Lisa Simpson                             Where's Mr. Bergstrom?\n","2              Miss Hoover  I don't know. Although I'd sure like to talk t...\n","3             Lisa Simpson                         That life is worth living.\n","4  Edna Krabappel-Flanders  The polls will be open from now until the end ..."],"text/html":["\n","  <div id=\"df-4e507a37-68d7-4dc2-96b4-fd32c13500b1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>raw_character_text</th>\n","      <th>spoken_words</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Miss Hoover</td>\n","      <td>No, actually, it was a little of both. Sometim...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lisa Simpson</td>\n","      <td>Where's Mr. Bergstrom?</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Miss Hoover</td>\n","      <td>I don't know. Although I'd sure like to talk t...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Lisa Simpson</td>\n","      <td>That life is worth living.</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Edna Krabappel-Flanders</td>\n","      <td>The polls will be open from now until the end ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4e507a37-68d7-4dc2-96b4-fd32c13500b1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-4e507a37-68d7-4dc2-96b4-fd32c13500b1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-4e507a37-68d7-4dc2-96b4-fd32c13500b1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"VAg5a5bmWk3T"},"source":["**Pregunta 1**: Ayudándose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec. **(0.75 punto)** (Hint, le puede servir explorar un poco los datos)"]},{"cell_type":"markdown","metadata":{"id":"MWw2fXFRXe5Y"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"Bvwplz7yTNcr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652042383204,"user_tz":240,"elapsed":9393,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}},"outputId":"67b7e3c5-c225-4a13-e027-b4ae16e1f124"},"source":["!pip install cpython # instalación de cpython para entrenar mas rápido el modelo\n","\n","model_w2v = Word2Vec(min_count=10,\n","                      window=4,\n","                      size=200,\n","                      sample=6e-5,\n","                      alpha=0.03,\n","                      min_alpha=0.0007,\n","                      negative=20,\n","                      workers=multiprocessing.cpu_count())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting cpython\n","  Downloading cPython-0.0.6.tar.gz (4.7 kB)\n","Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from cpython) (4.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from cpython) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->cpython) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->cpython) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->cpython) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->cpython) (3.0.4)\n","Building wheels for collected packages: cpython\n","  Building wheel for cpython (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cpython: filename=cPython-0.0.6-py3-none-any.whl size=4913 sha256=74a355611ede74c7458d2161d53489646eea27070ef138525c33a7c419b3c84b\n","  Stored in directory: /root/.cache/pip/wheels/88/92/ea/c32ad929e979a7303e010b29c736c793368f6f61c8c9902865\n","Successfully built cpython\n","Installing collected packages: cpython\n","Successfully installed cpython-0.0.6\n"]}]},{"cell_type":"code","source":["model_w2v.build_vocab(df['spoken_words'].apply(lambda x: CoolTokenizer().tokenize(x)), progress_per=10000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LIFYyob0a8GZ","executionInfo":{"status":"ok","timestamp":1652042387257,"user_tz":240,"elapsed":4060,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}},"outputId":"e4260873-debc-48e5-ec22-0f0a302f5891"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-08 20:39:44,242 : INFO : collecting all words and their counts\n","2022-05-08 20:39:44,244 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n","2022-05-08 20:39:44,282 : INFO : PROGRESS: at sentence #10000, processed 127439 words, keeping 11962 word types\n","2022-05-08 20:39:44,318 : INFO : PROGRESS: at sentence #20000, processed 256983 words, keeping 18463 word types\n","2022-05-08 20:39:44,370 : INFO : PROGRESS: at sentence #30000, processed 399063 words, keeping 24196 word types\n","2022-05-08 20:39:44,410 : INFO : PROGRESS: at sentence #40000, processed 532120 words, keeping 28179 word types\n","2022-05-08 20:39:44,450 : INFO : PROGRESS: at sentence #50000, processed 654826 words, keeping 31857 word types\n","2022-05-08 20:39:44,489 : INFO : PROGRESS: at sentence #60000, processed 768830 words, keeping 35006 word types\n","2022-05-08 20:39:44,530 : INFO : PROGRESS: at sentence #70000, processed 893426 words, keeping 38351 word types\n","2022-05-08 20:39:44,571 : INFO : PROGRESS: at sentence #80000, processed 1024979 words, keeping 41635 word types\n","2022-05-08 20:39:44,611 : INFO : PROGRESS: at sentence #90000, processed 1153951 words, keeping 44505 word types\n","2022-05-08 20:39:44,665 : INFO : PROGRESS: at sentence #100000, processed 1284468 words, keeping 47109 word types\n","2022-05-08 20:39:44,727 : INFO : PROGRESS: at sentence #110000, processed 1416296 words, keeping 49941 word types\n","2022-05-08 20:39:44,791 : INFO : PROGRESS: at sentence #120000, processed 1544410 words, keeping 52412 word types\n","2022-05-08 20:39:44,854 : INFO : PROGRESS: at sentence #130000, processed 1674803 words, keeping 54286 word types\n","2022-05-08 20:39:44,875 : INFO : collected 54613 word types from a corpus of 1699099 raw words and 131853 sentences\n","2022-05-08 20:39:44,883 : INFO : Loading a fresh vocabulary\n","2022-05-08 20:39:45,069 : INFO : effective_min_count=10 retains 8086 unique words (14% of original 54613, drops 46527)\n","2022-05-08 20:39:45,075 : INFO : effective_min_count=10 leaves 1595523 word corpus (93% of original 1699099, drops 103576)\n","2022-05-08 20:39:45,106 : INFO : deleting the raw counts dictionary of 54613 items\n","2022-05-08 20:39:45,109 : INFO : sample=6e-05 downsamples 628 most-common words\n","2022-05-08 20:39:45,112 : INFO : downsampling leaves estimated 556111 word corpus (34.9% of prior 1595523)\n","2022-05-08 20:39:45,148 : INFO : estimated required memory for 8086 words and 200 dimensions: 16980600 bytes\n","2022-05-08 20:39:45,150 : INFO : resetting layer weights\n"]}]},{"cell_type":"code","source":["t = time()\n","model_w2v.train(df['spoken_words'], total_examples=model_w2v.corpus_count, epochs=15, report_delay=10)\n","print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ikEFm6GpbN73","executionInfo":{"status":"ok","timestamp":1652042612535,"user_tz":240,"elapsed":225287,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}},"outputId":"87920b6d-7bd1-42da-c112-134aa8067abd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-08 20:39:47,188 : INFO : training model with 2 workers on 8086 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4\n","2022-05-08 20:39:48,210 : INFO : EPOCH 1 - PROGRESS: at 3.59% examples, 145590 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:39:58,291 : INFO : EPOCH 1 - PROGRESS: at 48.61% examples, 176588 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:40:08,320 : INFO : EPOCH 1 - PROGRESS: at 82.04% examples, 159640 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:40:13,534 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:40:13,557 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:40:13,564 : INFO : EPOCH - 1 : training on 6982745 raw words (4120749 effective words) took 26.4s, 156332 effective words/s\n","2022-05-08 20:40:14,601 : INFO : EPOCH 2 - PROGRESS: at 2.85% examples, 115298 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:40:24,607 : INFO : EPOCH 2 - PROGRESS: at 36.38% examples, 137125 words/s, in_qsize 4, out_qsize 0\n","2022-05-08 20:40:33,351 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:40:33,369 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:40:33,371 : INFO : EPOCH - 2 : training on 6982745 raw words (4120803 effective words) took 19.8s, 208270 effective words/s\n","2022-05-08 20:40:34,401 : INFO : EPOCH 3 - PROGRESS: at 7.44% examples, 292434 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:40:44,426 : INFO : EPOCH 3 - PROGRESS: at 81.09% examples, 301391 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:40:47,001 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:40:47,011 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:40:47,013 : INFO : EPOCH - 3 : training on 6982745 raw words (4120576 effective words) took 13.6s, 302239 effective words/s\n","2022-05-08 20:40:48,031 : INFO : EPOCH 4 - PROGRESS: at 7.44% examples, 295236 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:40:58,035 : INFO : EPOCH 4 - PROGRESS: at 80.95% examples, 301718 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:41:00,640 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:41:00,645 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:41:00,648 : INFO : EPOCH - 4 : training on 6982745 raw words (4120571 effective words) took 13.6s, 302341 effective words/s\n","2022-05-08 20:41:01,660 : INFO : EPOCH 5 - PROGRESS: at 7.44% examples, 297904 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:41:11,665 : INFO : EPOCH 5 - PROGRESS: at 80.69% examples, 300842 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:41:14,321 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:41:14,341 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:41:14,343 : INFO : EPOCH - 5 : training on 6982745 raw words (4120653 effective words) took 13.7s, 301086 effective words/s\n","2022-05-08 20:41:15,355 : INFO : EPOCH 6 - PROGRESS: at 7.30% examples, 291662 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:41:25,373 : INFO : EPOCH 6 - PROGRESS: at 81.75% examples, 304746 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:41:27,874 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:41:27,896 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:41:27,897 : INFO : EPOCH - 6 : training on 6982745 raw words (4120750 effective words) took 13.5s, 304193 effective words/s\n","2022-05-08 20:41:28,912 : INFO : EPOCH 7 - PROGRESS: at 7.44% examples, 297481 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:41:38,919 : INFO : EPOCH 7 - PROGRESS: at 73.04% examples, 271411 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:41:42,996 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:41:43,015 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:41:43,017 : INFO : EPOCH - 7 : training on 6982745 raw words (4120917 effective words) took 15.1s, 272741 effective words/s\n","2022-05-08 20:41:44,033 : INFO : EPOCH 8 - PROGRESS: at 7.61% examples, 302494 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:41:54,046 : INFO : EPOCH 8 - PROGRESS: at 82.71% examples, 308542 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:41:56,371 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:41:56,378 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:41:56,380 : INFO : EPOCH - 8 : training on 6982745 raw words (4120905 effective words) took 13.4s, 308574 effective words/s\n","2022-05-08 20:41:57,393 : INFO : EPOCH 9 - PROGRESS: at 7.44% examples, 297448 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:42:07,401 : INFO : EPOCH 9 - PROGRESS: at 79.05% examples, 294342 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:42:10,217 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:42:10,237 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:42:10,239 : INFO : EPOCH - 9 : training on 6982745 raw words (4121169 effective words) took 13.9s, 297542 effective words/s\n","2022-05-08 20:42:11,275 : INFO : EPOCH 10 - PROGRESS: at 7.44% examples, 292385 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:42:21,280 : INFO : EPOCH 10 - PROGRESS: at 81.89% examples, 305146 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:42:23,707 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:42:23,721 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:42:23,722 : INFO : EPOCH - 10 : training on 6982745 raw words (4120939 effective words) took 13.5s, 305957 effective words/s\n","2022-05-08 20:42:24,741 : INFO : EPOCH 11 - PROGRESS: at 7.44% examples, 296518 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:42:34,763 : INFO : EPOCH 11 - PROGRESS: at 81.62% examples, 304029 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:42:37,245 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:42:37,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:42:37,255 : INFO : EPOCH - 11 : training on 6982745 raw words (4120998 effective words) took 13.5s, 304764 effective words/s\n","2022-05-08 20:42:38,277 : INFO : EPOCH 12 - PROGRESS: at 7.44% examples, 295271 words/s, in_qsize 2, out_qsize 1\n","2022-05-08 20:42:48,308 : INFO : EPOCH 12 - PROGRESS: at 82.02% examples, 305232 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:42:50,709 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:42:50,730 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:42:50,731 : INFO : EPOCH - 12 : training on 6982745 raw words (4120509 effective words) took 13.5s, 305985 effective words/s\n","2022-05-08 20:42:51,759 : INFO : EPOCH 13 - PROGRESS: at 7.61% examples, 301151 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:43:01,768 : INFO : EPOCH 13 - PROGRESS: at 81.89% examples, 305341 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:43:04,243 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:43:04,266 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:43:04,267 : INFO : EPOCH - 13 : training on 6982745 raw words (4120871 effective words) took 13.5s, 304798 effective words/s\n","2022-05-08 20:43:05,278 : INFO : EPOCH 14 - PROGRESS: at 7.44% examples, 298393 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:43:15,281 : INFO : EPOCH 14 - PROGRESS: at 81.75% examples, 305259 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:43:18,687 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:43:18,708 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:43:18,709 : INFO : EPOCH - 14 : training on 6982745 raw words (4120933 effective words) took 14.4s, 285537 effective words/s\n","2022-05-08 20:43:19,735 : INFO : EPOCH 15 - PROGRESS: at 7.61% examples, 299923 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:43:29,753 : INFO : EPOCH 15 - PROGRESS: at 81.75% examples, 304436 words/s, in_qsize 3, out_qsize 0\n","2022-05-08 20:43:32,222 : INFO : worker thread finished; awaiting finish of 1 more threads\n","2022-05-08 20:43:32,229 : INFO : worker thread finished; awaiting finish of 0 more threads\n","2022-05-08 20:43:32,231 : INFO : EPOCH - 15 : training on 6982745 raw words (4120808 effective words) took 13.5s, 304985 effective words/s\n","2022-05-08 20:43:32,232 : INFO : training on a 104741175 raw words (61812151 effective words) took 225.0s, 274677 effective words/s\n"]},{"output_type":"stream","name":"stdout","text":["Time to train the model: 3.75 mins\n"]}]},{"cell_type":"code","source":["model_w2v.init_sims(replace=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sUy2CKyjbql9","executionInfo":{"status":"ok","timestamp":1652042612536,"user_tz":240,"elapsed":21,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}},"outputId":"637dcfd9-caab-458e-a01f-8fd0216fd22e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-08 20:43:32,241 : INFO : precomputing L2-norms of word weight vectors\n"]}]},{"cell_type":"markdown","source":["**Pregunta 2**: Cree una matriz palabra contexto usando el mismo dataset. Configure el largo del vocabulario a 1000 o 2000 tokens, puede agregar valores mayores pero tenga en cuenta que la construcción de la matriz puede tomar varios minutos. Puede que esto tarde un poco. **(0.75 punto)** "],"metadata":{"id":"3vBkF3hreGjg"}},{"cell_type":"markdown","source":["**Respuesta:**"],"metadata":{"id":"zzLuH6MneWIY"}},{"cell_type":"code","source":["context_class = WordContextMatrix(vocab_size = 1000, window_size = 1, dataset = df['spoken_words'], tokenizer = CoolTokenizer())\n","context_class.build_matrix()"],"metadata":{"id":"9gPyW8fMeXNX","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1652043213931,"user_tz":240,"elapsed":601401,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}},"outputId":"346449a2-7734-41ef-dd90-9214a411ec7c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["               unk      No        ,  actually      it     was        a  \\\n","unk       222776.0   526.0  59101.0     114.0  4850.0  2835.0  24393.0   \n","No           526.0     0.0   1510.0       0.0    10.0     0.0      0.0   \n",",          59101.0  1510.0      2.0      84.0  1757.0    95.0    806.0   \n","actually     114.0     0.0     84.0       0.0     4.0     7.0     12.0   \n","it          4850.0    10.0   1757.0       4.0     2.0   551.0    117.0   \n","...            ...     ...      ...       ...     ...     ...      ...   \n","chip          20.0     0.0      1.0       0.0     0.0     0.0      2.0   \n","write         36.0     0.0     11.0       0.0    10.0     0.0     42.0   \n","shopping      28.0     0.0      4.0       0.0     0.0     0.0      2.0   \n","question      67.0     3.0     19.0       0.0     0.0     0.0     38.0   \n","What's        78.0     0.0      1.0       0.0    23.0     0.0     31.0   \n","\n","          little       of   both  ...  gallon  chocolate  Check  brownie  \\\n","unk       1494.0  15078.0  112.0  ...     1.0       60.0   14.0      3.0   \n","No           0.0      1.0    0.0  ...     0.0        0.0    0.0      0.0   \n",",          135.0    151.0   12.0  ...     0.0       19.0    5.0      0.0   \n","actually     0.0      1.0    0.0  ...     0.0        0.0    0.0      0.0   \n","it           0.0    183.0    2.0  ...     0.0        1.0   66.0      0.0   \n","...          ...      ...    ...  ...     ...        ...    ...      ...   \n","chip         0.0      0.0    0.0  ...     0.0        3.0    0.0      0.0   \n","write        0.0      1.0    0.0  ...     0.0        0.0    0.0      0.0   \n","shopping     1.0      1.0    0.0  ...     0.0        0.0    0.0      0.0   \n","question     0.0      1.0    0.0  ...     0.0        0.0    0.0      0.0   \n","What's       0.0      1.0    0.0  ...     0.0        0.0    0.0      0.0   \n","\n","          fudge  chip  write  shopping  question  What's  \n","unk        20.0  20.0   36.0      28.0      67.0    78.0  \n","No          0.0   0.0    0.0       0.0       3.0     0.0  \n",",           3.0   1.0   11.0       4.0      19.0     1.0  \n","actually    0.0   0.0    0.0       0.0       0.0     0.0  \n","it          0.0   0.0   10.0       0.0       0.0    23.0  \n","...         ...   ...    ...       ...       ...     ...  \n","chip        0.0   0.0    0.0       0.0       0.0     0.0  \n","write       0.0   0.0    0.0       0.0       0.0     0.0  \n","shopping    0.0   0.0    0.0       0.0       0.0     0.0  \n","question    0.0   0.0    0.0       0.0       0.0     0.0  \n","What's      0.0   0.0    0.0       0.0       0.0     0.0  \n","\n","[1001 rows x 1001 columns]"],"text/html":["\n","  <div id=\"df-0ee6069e-7100-43fb-a5c8-36243c64bc48\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>unk</th>\n","      <th>No</th>\n","      <th>,</th>\n","      <th>actually</th>\n","      <th>it</th>\n","      <th>was</th>\n","      <th>a</th>\n","      <th>little</th>\n","      <th>of</th>\n","      <th>both</th>\n","      <th>...</th>\n","      <th>gallon</th>\n","      <th>chocolate</th>\n","      <th>Check</th>\n","      <th>brownie</th>\n","      <th>fudge</th>\n","      <th>chip</th>\n","      <th>write</th>\n","      <th>shopping</th>\n","      <th>question</th>\n","      <th>What's</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>unk</th>\n","      <td>222776.0</td>\n","      <td>526.0</td>\n","      <td>59101.0</td>\n","      <td>114.0</td>\n","      <td>4850.0</td>\n","      <td>2835.0</td>\n","      <td>24393.0</td>\n","      <td>1494.0</td>\n","      <td>15078.0</td>\n","      <td>112.0</td>\n","      <td>...</td>\n","      <td>1.0</td>\n","      <td>60.0</td>\n","      <td>14.0</td>\n","      <td>3.0</td>\n","      <td>20.0</td>\n","      <td>20.0</td>\n","      <td>36.0</td>\n","      <td>28.0</td>\n","      <td>67.0</td>\n","      <td>78.0</td>\n","    </tr>\n","    <tr>\n","      <th>No</th>\n","      <td>526.0</td>\n","      <td>0.0</td>\n","      <td>1510.0</td>\n","      <td>0.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>,</th>\n","      <td>59101.0</td>\n","      <td>1510.0</td>\n","      <td>2.0</td>\n","      <td>84.0</td>\n","      <td>1757.0</td>\n","      <td>95.0</td>\n","      <td>806.0</td>\n","      <td>135.0</td>\n","      <td>151.0</td>\n","      <td>12.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>19.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>11.0</td>\n","      <td>4.0</td>\n","      <td>19.0</td>\n","      <td>1.0</td>\n","    </tr>\n","    <tr>\n","      <th>actually</th>\n","      <td>114.0</td>\n","      <td>0.0</td>\n","      <td>84.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>7.0</td>\n","      <td>12.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>it</th>\n","      <td>4850.0</td>\n","      <td>10.0</td>\n","      <td>1757.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>551.0</td>\n","      <td>117.0</td>\n","      <td>0.0</td>\n","      <td>183.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>66.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>23.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>chip</th>\n","      <td>20.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>write</th>\n","      <td>36.0</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>0.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>42.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>shopping</th>\n","      <td>28.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>question</th>\n","      <td>67.0</td>\n","      <td>3.0</td>\n","      <td>19.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>38.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>What's</th>\n","      <td>78.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>23.0</td>\n","      <td>0.0</td>\n","      <td>31.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1001 rows × 1001 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0ee6069e-7100-43fb-a5c8-36243c64bc48')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0ee6069e-7100-43fb-a5c8-36243c64bc48 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0ee6069e-7100-43fb-a5c8-36243c64bc48');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"IRCB-jqgTNcs"},"source":["#### **Parte C (1.5 puntos): Aplicar embeddings para clasificar**"]},{"cell_type":"markdown","metadata":{"id":"zlqzlJRSTNcs"},"source":["Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n","\n","Para esto ocuparemos el lexicón AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotación es positiva y un -1 si es negativa."]},{"cell_type":"code","metadata":{"id":"CMskFDmHTNcs"},"source":["AFINN = 'AFINN_full.csv'\n","df_afinn = pd.read_csv(path + AFINN, sep='\\t', header=None)\n","df_afinn.columns = ['tokens', 'sentiment']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uaKl8hsCTNcs"},"source":["Hint: Para w2v y la wcm son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendrán una representación en AFINN. Para el caso de la matriz palabra contexto se recomienda convertir su matrix a un diccionario. Pueden utilizar esta función auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."]},{"cell_type":"code","metadata":{"id":"tWSSuctiTNcs"},"source":["def try_apply(model,word):\n","    try:\n","        aux = model[word]\n","        return True\n","    except KeyError:\n","        #logger.error('Word {} not in dictionary'.format(word))\n","        return False"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LrVPeEzgTNcs"},"source":["**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representación en embedding que acabamos de calcular (con ambos modelos). \n","\n","Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n","\n","Para ambos modelos, separar train y test de acuerdo a la siguiente función. **(0.5 puntos)**"]},{"cell_type":"markdown","source":["```python\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)\n","```"],"metadata":{"id":"lp54th_ZcWKL"}},{"cell_type":"markdown","metadata":{"id":"iDcq5czXTNct"},"source":["**Respuesta**:"]},{"cell_type":"markdown","source":["Usando **Word2Vec**:"],"metadata":{"id":"J6GJVl0hcu9S"}},{"cell_type":"code","source":["from sklearn.base import BaseEstimator, TransformerMixin"],"metadata":{"id":"_s98ssXSoINi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BaseFeature(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        return self\n","\n","class Doc2VecTransformer(BaseFeature):\n","    \"\"\" Transforma tweets a representaciones vectoriales usando algún modelo de Word Embeddings.\n","    \"\"\"\n","    \n","    def __init__(self, model, aggregation_func):\n","        # extraemos los embeddings desde el objeto contenedor. ojo con esta parte.\n","        self.model = model.wv \n","        \n","        # indicamos la función de agregación (np.min, np.max, np.mean, np.sum, ...)\n","        self.aggregation_func = aggregation_func\n","\n","    def simple_tokenizer(self, doc, lower=False):\n","        \"\"\"Tokenizador. Elimina signos de puntuación, lleva las letras a minúscula(opcional) y \n","           separa el tweet por espacios.\n","        \"\"\"\n","        if lower:\n","            doc.translate(str.maketrans('', '', string.punctuation)).lower().split()\n","        return doc.translate(str.maketrans('', '', string.punctuation)).split()\n","\n","    def transform(self, X, y = None):\n","        \n","        doc_embeddings = []\n","        \n","        for doc in X:\n","            tokens = self.simple_tokenizer(doc, lower = True) \n","            \n","            selected_wv = []\n","            for token in tokens:\n","                if token in self.model.vocab:\n","                    selected_wv.append(self.model[token])\n","                    \n","            if len(selected_wv) > 0:\n","                doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n","                doc_embeddings.append(doc_embedding)\n","            else: \n","                print('No pude encontrar ningún embedding en el tweet: {}. Agregando vector de ceros.'.format(doc))\n","                doc_embeddings.append(np.zeros(self.model.vector_size)) # la dimension del modelo \n","\n","        return np.array(doc_embeddings)"],"metadata":{"id":"uy8dB0t5nxWy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["doc2vec = Doc2VecTransformer(model = model_w2v, aggregation_func = np.mean)\n","X_w2v = df_afinn.copy()\n","X_w2v = X_w2v[X_w2v['tokens'].apply(lambda x: try_apply(model_w2v, x))]\n","y_w2v = X_w2v['sentiment'].copy()\n","X_w2v = pd.DataFrame(doc2vec.transform(X_w2v['tokens']), index = X_w2v.index)\n","X_w2v['sentiment'] = y_w2v\n","X_w2v"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":453},"id":"iYnykvh6oUyg","executionInfo":{"status":"ok","timestamp":1652043214434,"user_tz":240,"elapsed":10,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}},"outputId":"920d1bf5-fe4a-407f-afb8-e926a7a83b18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]},{"output_type":"execute_result","data":{"text/plain":["             0         1         2         3         4         5         6  \\\n","0    -0.005876 -0.054479  0.065025  0.110985 -0.023531  0.109618  0.013256   \n","7    -0.110011 -0.112570 -0.044812 -0.058394 -0.053793 -0.123762  0.075538   \n","9     0.072491 -0.086120  0.046757  0.117633 -0.102832  0.034850 -0.022824   \n","10   -0.104290  0.119675  0.080029 -0.016221 -0.049657  0.068766  0.111652   \n","11    0.052578 -0.075405  0.102743 -0.085386  0.020334  0.126363 -0.063047   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","3371 -0.005660 -0.083179  0.016850  0.003787 -0.009689 -0.082245 -0.075694   \n","3373  0.084346  0.051804 -0.050418 -0.084778  0.065223 -0.062585 -0.009248   \n","3374  0.091559 -0.017416  0.051470 -0.014099  0.074353 -0.050761  0.120351   \n","3376  0.007774  0.038983 -0.110513  0.086117 -0.050191  0.060727 -0.026059   \n","3378 -0.060579 -0.057265  0.093019 -0.000994 -0.024072  0.077668  0.078352   \n","\n","             7         8         9  ...       191       192       193  \\\n","0    -0.036617 -0.116491 -0.031573  ... -0.128080  0.059815 -0.009228   \n","7    -0.068715 -0.094696 -0.037426  ... -0.038582 -0.098724  0.103043   \n","9     0.027937  0.039232  0.021773  ...  0.019732 -0.062010 -0.041085   \n","10   -0.039882  0.035907  0.109498  ... -0.123379  0.094522 -0.093058   \n","11    0.101807  0.062998 -0.031244  ...  0.039064 -0.109978 -0.109080   \n","...        ...       ...       ...  ...       ...       ...       ...   \n","3371 -0.009051 -0.094080  0.003337  ... -0.030801  0.106755  0.023161   \n","3373 -0.104003  0.085828 -0.099530  ...  0.064509 -0.017097 -0.103393   \n","3374 -0.081860 -0.038195  0.009496  ... -0.120570 -0.047766  0.011517   \n","3376 -0.014616  0.062172 -0.024311  ...  0.039644  0.060710 -0.095590   \n","3378  0.040943  0.021705  0.010735  ... -0.025357  0.119116 -0.026094   \n","\n","           194       195       196       197       198       199  sentiment  \n","0    -0.093840 -0.072648 -0.068154  0.094327 -0.025039 -0.077103          1  \n","7    -0.013736  0.124108 -0.023571 -0.009953 -0.114544  0.024868         -1  \n","9    -0.083030 -0.068607  0.004719  0.010059  0.115425  0.048619         -1  \n","10   -0.082436 -0.081324  0.075243  0.102320 -0.122094 -0.122374          1  \n","11   -0.053130 -0.036817 -0.120643 -0.066891  0.080402 -0.082499          1  \n","...        ...       ...       ...       ...       ...       ...        ...  \n","3371 -0.000579  0.086996  0.080699 -0.021623  0.058399 -0.055221         -1  \n","3373  0.111755 -0.046810  0.091307 -0.048306  0.054815 -0.003193          1  \n","3374  0.007626 -0.094805 -0.022648 -0.122114 -0.117493 -0.031219          1  \n","3376 -0.102332  0.005206  0.094312  0.118114 -0.042657  0.000738          1  \n","3378 -0.086929  0.027949  0.089864  0.062721 -0.005883  0.008142          1  \n","\n","[920 rows x 201 columns]"],"text/html":["\n","  <div id=\"df-a423bcc1-33e3-49ba-a95b-e60e731ba677\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>191</th>\n","      <th>192</th>\n","      <th>193</th>\n","      <th>194</th>\n","      <th>195</th>\n","      <th>196</th>\n","      <th>197</th>\n","      <th>198</th>\n","      <th>199</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.005876</td>\n","      <td>-0.054479</td>\n","      <td>0.065025</td>\n","      <td>0.110985</td>\n","      <td>-0.023531</td>\n","      <td>0.109618</td>\n","      <td>0.013256</td>\n","      <td>-0.036617</td>\n","      <td>-0.116491</td>\n","      <td>-0.031573</td>\n","      <td>...</td>\n","      <td>-0.128080</td>\n","      <td>0.059815</td>\n","      <td>-0.009228</td>\n","      <td>-0.093840</td>\n","      <td>-0.072648</td>\n","      <td>-0.068154</td>\n","      <td>0.094327</td>\n","      <td>-0.025039</td>\n","      <td>-0.077103</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>-0.110011</td>\n","      <td>-0.112570</td>\n","      <td>-0.044812</td>\n","      <td>-0.058394</td>\n","      <td>-0.053793</td>\n","      <td>-0.123762</td>\n","      <td>0.075538</td>\n","      <td>-0.068715</td>\n","      <td>-0.094696</td>\n","      <td>-0.037426</td>\n","      <td>...</td>\n","      <td>-0.038582</td>\n","      <td>-0.098724</td>\n","      <td>0.103043</td>\n","      <td>-0.013736</td>\n","      <td>0.124108</td>\n","      <td>-0.023571</td>\n","      <td>-0.009953</td>\n","      <td>-0.114544</td>\n","      <td>0.024868</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>0.072491</td>\n","      <td>-0.086120</td>\n","      <td>0.046757</td>\n","      <td>0.117633</td>\n","      <td>-0.102832</td>\n","      <td>0.034850</td>\n","      <td>-0.022824</td>\n","      <td>0.027937</td>\n","      <td>0.039232</td>\n","      <td>0.021773</td>\n","      <td>...</td>\n","      <td>0.019732</td>\n","      <td>-0.062010</td>\n","      <td>-0.041085</td>\n","      <td>-0.083030</td>\n","      <td>-0.068607</td>\n","      <td>0.004719</td>\n","      <td>0.010059</td>\n","      <td>0.115425</td>\n","      <td>0.048619</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>-0.104290</td>\n","      <td>0.119675</td>\n","      <td>0.080029</td>\n","      <td>-0.016221</td>\n","      <td>-0.049657</td>\n","      <td>0.068766</td>\n","      <td>0.111652</td>\n","      <td>-0.039882</td>\n","      <td>0.035907</td>\n","      <td>0.109498</td>\n","      <td>...</td>\n","      <td>-0.123379</td>\n","      <td>0.094522</td>\n","      <td>-0.093058</td>\n","      <td>-0.082436</td>\n","      <td>-0.081324</td>\n","      <td>0.075243</td>\n","      <td>0.102320</td>\n","      <td>-0.122094</td>\n","      <td>-0.122374</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.052578</td>\n","      <td>-0.075405</td>\n","      <td>0.102743</td>\n","      <td>-0.085386</td>\n","      <td>0.020334</td>\n","      <td>0.126363</td>\n","      <td>-0.063047</td>\n","      <td>0.101807</td>\n","      <td>0.062998</td>\n","      <td>-0.031244</td>\n","      <td>...</td>\n","      <td>0.039064</td>\n","      <td>-0.109978</td>\n","      <td>-0.109080</td>\n","      <td>-0.053130</td>\n","      <td>-0.036817</td>\n","      <td>-0.120643</td>\n","      <td>-0.066891</td>\n","      <td>0.080402</td>\n","      <td>-0.082499</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3371</th>\n","      <td>-0.005660</td>\n","      <td>-0.083179</td>\n","      <td>0.016850</td>\n","      <td>0.003787</td>\n","      <td>-0.009689</td>\n","      <td>-0.082245</td>\n","      <td>-0.075694</td>\n","      <td>-0.009051</td>\n","      <td>-0.094080</td>\n","      <td>0.003337</td>\n","      <td>...</td>\n","      <td>-0.030801</td>\n","      <td>0.106755</td>\n","      <td>0.023161</td>\n","      <td>-0.000579</td>\n","      <td>0.086996</td>\n","      <td>0.080699</td>\n","      <td>-0.021623</td>\n","      <td>0.058399</td>\n","      <td>-0.055221</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3373</th>\n","      <td>0.084346</td>\n","      <td>0.051804</td>\n","      <td>-0.050418</td>\n","      <td>-0.084778</td>\n","      <td>0.065223</td>\n","      <td>-0.062585</td>\n","      <td>-0.009248</td>\n","      <td>-0.104003</td>\n","      <td>0.085828</td>\n","      <td>-0.099530</td>\n","      <td>...</td>\n","      <td>0.064509</td>\n","      <td>-0.017097</td>\n","      <td>-0.103393</td>\n","      <td>0.111755</td>\n","      <td>-0.046810</td>\n","      <td>0.091307</td>\n","      <td>-0.048306</td>\n","      <td>0.054815</td>\n","      <td>-0.003193</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3374</th>\n","      <td>0.091559</td>\n","      <td>-0.017416</td>\n","      <td>0.051470</td>\n","      <td>-0.014099</td>\n","      <td>0.074353</td>\n","      <td>-0.050761</td>\n","      <td>0.120351</td>\n","      <td>-0.081860</td>\n","      <td>-0.038195</td>\n","      <td>0.009496</td>\n","      <td>...</td>\n","      <td>-0.120570</td>\n","      <td>-0.047766</td>\n","      <td>0.011517</td>\n","      <td>0.007626</td>\n","      <td>-0.094805</td>\n","      <td>-0.022648</td>\n","      <td>-0.122114</td>\n","      <td>-0.117493</td>\n","      <td>-0.031219</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3376</th>\n","      <td>0.007774</td>\n","      <td>0.038983</td>\n","      <td>-0.110513</td>\n","      <td>0.086117</td>\n","      <td>-0.050191</td>\n","      <td>0.060727</td>\n","      <td>-0.026059</td>\n","      <td>-0.014616</td>\n","      <td>0.062172</td>\n","      <td>-0.024311</td>\n","      <td>...</td>\n","      <td>0.039644</td>\n","      <td>0.060710</td>\n","      <td>-0.095590</td>\n","      <td>-0.102332</td>\n","      <td>0.005206</td>\n","      <td>0.094312</td>\n","      <td>0.118114</td>\n","      <td>-0.042657</td>\n","      <td>0.000738</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3378</th>\n","      <td>-0.060579</td>\n","      <td>-0.057265</td>\n","      <td>0.093019</td>\n","      <td>-0.000994</td>\n","      <td>-0.024072</td>\n","      <td>0.077668</td>\n","      <td>0.078352</td>\n","      <td>0.040943</td>\n","      <td>0.021705</td>\n","      <td>0.010735</td>\n","      <td>...</td>\n","      <td>-0.025357</td>\n","      <td>0.119116</td>\n","      <td>-0.026094</td>\n","      <td>-0.086929</td>\n","      <td>0.027949</td>\n","      <td>0.089864</td>\n","      <td>0.062721</td>\n","      <td>-0.005883</td>\n","      <td>0.008142</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>920 rows × 201 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a423bcc1-33e3-49ba-a95b-e60e731ba677')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a423bcc1-33e3-49ba-a95b-e60e731ba677 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a423bcc1-33e3-49ba-a95b-e60e731ba677');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","source":["Usando la **wcm**:"],"metadata":{"id":"mcX7JUVRdRse"}},{"cell_type":"code","source":["context_dict = context_class.matrix2dict()\n","X_context = df_afinn.copy()\n","X_context = X_context[X_context['tokens'].apply(lambda x: try_apply(context_dict, x))] # filtro de lexicon\n","y_context = X_context['sentiment'].copy().reset_index(drop = True)\n","X_context = X_context['tokens'].apply(lambda x: context_dict[x]) # transformacion de palabras\n","X_context = pd.json_normalize(X_context)\n","X_context['sentiment'] = y_context\n","X_context"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"0_IZhKSiJgMM","executionInfo":{"status":"ok","timestamp":1652043215304,"user_tz":240,"elapsed":877,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}},"outputId":"c1b248ca-2197-4241-9257-146481e8ff65"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      unk   No     ,  actually    it   was     a  little    of  both  ...  \\\n","0    36.0  0.0   4.0       0.0   0.0   0.0  10.0     0.0   0.0   0.0  ...   \n","1     8.0  0.0   1.0       0.0   0.0   0.0   0.0     0.0   1.0   0.0  ...   \n","2    44.0  0.0   9.0       0.0   0.0   2.0  10.0     2.0   0.0   0.0  ...   \n","3    13.0  0.0   5.0       0.0   2.0   0.0   0.0     0.0   0.0   0.0  ...   \n","4    10.0  0.0   1.0       0.0   0.0   0.0   0.0     0.0   0.0   0.0  ...   \n","..    ...  ...   ...       ...   ...   ...   ...     ...   ...   ...  ...   \n","95  122.0  0.0  96.0       0.0  11.0  35.0   0.0     0.0   4.0   2.0  ...   \n","96    4.0  0.0   0.0       0.0   0.0   0.0   0.0     0.0  25.0   0.0  ...   \n","97   69.0  0.0  29.0       0.0  19.0   2.0   4.0     0.0   5.0   0.0  ...   \n","98   19.0  0.0   5.0       0.0   0.0   0.0   2.0     0.0   2.0   0.0  ...   \n","99  116.0  0.0  42.0       0.0   0.0   8.0  22.0     3.0  13.0   0.0  ...   \n","\n","    chocolate  Check  brownie  fudge  chip  write  shopping  question  What's  \\\n","0         0.0    0.0      0.0    0.0   0.0    0.0       0.0       0.0     0.0   \n","1         0.0    0.0      0.0    0.0   0.0    0.0       0.0       0.0     0.0   \n","2         0.0    0.0      0.0    0.0   0.0    0.0       0.0       0.0     0.0   \n","3         0.0    0.0      0.0    0.0   0.0    0.0       0.0       0.0     0.0   \n","4         0.0    0.0      0.0    0.0   0.0    0.0       0.0       0.0     0.0   \n","..        ...    ...      ...    ...   ...    ...       ...       ...     ...   \n","95        0.0    0.0      0.0    0.0   0.0    0.0       0.0       0.0    84.0   \n","96        0.0    0.0      0.0    0.0   0.0    0.0       0.0       0.0     0.0   \n","97        0.0    0.0      0.0    0.0   0.0    0.0       0.0       0.0     0.0   \n","98        0.0    0.0      0.0    0.0   0.0    0.0       0.0       0.0     0.0   \n","99        0.0    0.0      0.0    0.0   0.0    0.0       0.0       0.0     0.0   \n","\n","    sentiment  \n","0           1  \n","1          -1  \n","2           1  \n","3          -1  \n","4          -1  \n","..        ...  \n","95         -1  \n","96          1  \n","97         -1  \n","98         -1  \n","99         -1  \n","\n","[100 rows x 1002 columns]"],"text/html":["\n","  <div id=\"df-c4652f4b-8211-4bc6-8fc2-af117797ebd8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>unk</th>\n","      <th>No</th>\n","      <th>,</th>\n","      <th>actually</th>\n","      <th>it</th>\n","      <th>was</th>\n","      <th>a</th>\n","      <th>little</th>\n","      <th>of</th>\n","      <th>both</th>\n","      <th>...</th>\n","      <th>chocolate</th>\n","      <th>Check</th>\n","      <th>brownie</th>\n","      <th>fudge</th>\n","      <th>chip</th>\n","      <th>write</th>\n","      <th>shopping</th>\n","      <th>question</th>\n","      <th>What's</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>36.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>44.0</td>\n","      <td>0.0</td>\n","      <td>9.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>10.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>13.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>122.0</td>\n","      <td>0.0</td>\n","      <td>96.0</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","      <td>35.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>4.0</td>\n","      <td>2.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>84.0</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>25.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>69.0</td>\n","      <td>0.0</td>\n","      <td>29.0</td>\n","      <td>0.0</td>\n","      <td>19.0</td>\n","      <td>2.0</td>\n","      <td>4.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>19.0</td>\n","      <td>0.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>116.0</td>\n","      <td>0.0</td>\n","      <td>42.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>8.0</td>\n","      <td>22.0</td>\n","      <td>3.0</td>\n","      <td>13.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 1002 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c4652f4b-8211-4bc6-8fc2-af117797ebd8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c4652f4b-8211-4bc6-8fc2-af117797ebd8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c4652f4b-8211-4bc6-8fc2-af117797ebd8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["y_context = X_context['sentiment']\n","X_context = X_context.drop(columns = 'sentiment')\n","X_train_context, X_test_context, y_train_context, y_test_context = train_test_split(X_context, y_context, \n","                                                                                    random_state=0, test_size=0.1,\n","                                                                                    stratify=y_context)\n","\n","y_w2v = X_w2v['sentiment']\n","X_w2v = X_w2v.drop(columns = 'sentiment')\n","X_train_w2v, X_test_w2v, y_train_w2v, y_test_w2v = train_test_split(X_w2v, y_w2v, random_state=0, test_size=0.1, \n","                                                                    stratify=y_w2v)"],"metadata":{"id":"45ZcBBt5uE_D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kDKe4gA3TNct"},"source":["**Pregunta 2**: Entrenar una regresión logística (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qué se obtienen estos resultados? Cómo los mejorarías? Como podrías mejorar los resultados de la matriz palabra contexto? es equivalente al modelo word2vec? **(1 punto)**"]},{"cell_type":"markdown","metadata":{"id":"hJMzq_dETNct"},"source":["**Respuesta**:"]},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import classification_report, confusion_matrix"],"metadata":{"id":"bj1r_BnKn_7L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_w2v = LogisticRegression(max_iter=1000000)\n","lr_w2v.fit(X_train_w2v, y_train_w2v)\n","y_pred_w2v = lr_w2v.predict(X_test_w2v)\n","\n","conf_matrix_w2v = confusion_matrix(y_test_w2v, y_pred_w2v)\n","print(conf_matrix_w2v)\n","\n","print(classification_report(y_test_w2v, y_pred_w2v))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZGxCVEKmtWYh","executionInfo":{"status":"ok","timestamp":1652043215305,"user_tz":240,"elapsed":6,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}},"outputId":"b00de7fb-f4c8-4998-d003-1cb6186f69af"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[37 15]\n"," [26 14]]\n","              precision    recall  f1-score   support\n","\n","          -1       0.59      0.71      0.64        52\n","           1       0.48      0.35      0.41        40\n","\n","    accuracy                           0.55        92\n","   macro avg       0.54      0.53      0.52        92\n","weighted avg       0.54      0.55      0.54        92\n","\n"]}]},{"cell_type":"code","source":["lr_context = LogisticRegression(max_iter=1000000)\n","lr_context.fit(X_train_context, y_train_context)\n","y_pred_context = lr_context.predict(X_test_context)\n","\n","conf_matrix_context = confusion_matrix(y_test_context, y_pred_context)\n","print(conf_matrix_context)\n","\n","print(classification_report(y_test_context, y_pred_context))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"miAi9ad2ypel","executionInfo":{"status":"ok","timestamp":1652043215678,"user_tz":240,"elapsed":377,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}},"outputId":"9a754ee6-7e3a-4c1d-df58-36e9b3b62044"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[2 3]\n"," [1 4]]\n","              precision    recall  f1-score   support\n","\n","          -1       0.67      0.40      0.50         5\n","           1       0.57      0.80      0.67         5\n","\n","    accuracy                           0.60        10\n","   macro avg       0.62      0.60      0.58        10\n","weighted avg       0.62      0.60      0.58        10\n","\n"]}]},{"cell_type":"markdown","source":["Se concluye que, para este caso particular, la **wcm** obtiene un mejor rendimiento en casi todas las métricas. Pese a esto, es de notar que los resultados de ambos modelos tienen un poder de predicción pobre, ligeramente mejor a la clasificación aleatoria (la cual posee un accuracy de 0.5). Esto se puede explicar en los datos de entrenamiento (diálogo de los Simpsons), los que no necesariamente pueden representar la positividad de un comentario o palabra. Además, es de notar que ambos modelos no son tan comparables, pues el vocabulario usado para generar las representaciones no es el mismo (en **word2vec** se usó un vocabulario con tamaño del corpus, mientras que en **wcm** se usó un vocabulario con tamaño 1.000). Finalmente, ambos modelos poseen bastante espacio de mejora, tomando desde aumentar el tamaño de la ventana para construir los embeddings, aumentar el tamaño del vocabulario, mejorar el tokenizador (aplicando preprocesamiento como lowercase) y cambiar el set de entrenamiento."],"metadata":{"id":"qYm4mNxYdx8B"}},{"cell_type":"markdown","metadata":{"id":"izppruGQTNct"},"source":["# Bonus: +0.25 puntos en cualquier pregunta"]},{"cell_type":"markdown","metadata":{"id":"YW0aeK2KTNct"},"source":["**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset más grande y obtener mejores resultados. Les puede servir [ésta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."]},{"cell_type":"markdown","metadata":{"id":"qvHcVS3sTNct"},"source":["**Respuesta**:"]},{"cell_type":"code","metadata":{"id":"MSc8p-T8TNcu","colab":{"base_uri":"https://localhost:8080/","height":538},"executionInfo":{"status":"ok","timestamp":1652043984212,"user_tz":240,"elapsed":768543,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}},"outputId":"104931bb-30b3-4561-bbd8-b904af200e8a"},"source":["import gensim.downloader as api\n","\n","model_pretrained = api.load('word2vec-google-news-300') # embedding preentrenado a usar\n","doc2vec = Doc2VecTransformer(model = model_pretrained, aggregation_func = np.mean)\n","X_pretrained = df_afinn.copy()\n","X_pretrained = X_pretrained[X_pretrained['tokens'].apply(lambda x: try_apply(model_pretrained, x))]\n","y_pretrained = X_pretrained['sentiment'].copy()\n","X_pretrained = pd.DataFrame(doc2vec.transform(X_pretrained['tokens']), index = X_pretrained.index)\n","X_pretrained['sentiment'] = y_pretrained\n","X_pretrained"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["2022-05-08 20:53:35,576 : INFO : Creating /root/gensim-data\n"]},{"output_type":"stream","name":"stdout","text":["[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"]},{"output_type":"stream","name":"stderr","text":["2022-05-08 21:00:04,755 : INFO : word2vec-google-news-300 downloaded\n","2022-05-08 21:00:04,765 : INFO : loading projection weights from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n","2022-05-08 21:06:23,836 : INFO : loaded (3000000, 300) matrix from /root/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"execute_result","data":{"text/plain":["             0         1         2         3         4         5         6  \\\n","0    -0.057861  0.160156 -0.031128  0.190430  0.087891 -0.182617 -0.187500   \n","1     0.365234 -0.089355 -0.154297  0.408203 -0.375000 -0.232422  0.312500   \n","2     0.384766  0.425781  0.093262  0.032471 -0.131836  0.203125  0.349609   \n","3     0.191406 -0.174805  0.046143  0.324219 -0.027954  0.394531 -0.242188   \n","4    -0.010925  0.431641  0.283203  0.335938  0.056152 -0.460938  0.061279   \n","...        ...       ...       ...       ...       ...       ...       ...   \n","3377 -0.139648  0.154297 -0.035645  0.402344  0.086914 -0.039307  0.051025   \n","3378 -0.045410 -0.019043  0.059326  0.060791  0.068848 -0.437500 -0.013245   \n","3379  0.267578  0.125977  0.225586  0.234375 -0.236328  0.355469  0.059570   \n","3380  0.072754  0.182617 -0.075684 -0.125000 -0.112305  0.152344  0.061035   \n","3381  0.048096  0.138672 -0.117188 -0.031982 -0.421875  0.032959  0.296875   \n","\n","             7         8         9  ...       291       292       293  \\\n","0    -0.176758  0.159180  0.232422  ... -0.304688 -0.357422  0.150391   \n","1     0.123535  0.149414  0.322266  ... -0.013855  0.041260 -0.002197   \n","2    -0.236328 -0.036133 -0.071289  ...  0.154297 -0.437500 -0.096191   \n","3     0.419922  0.402344  0.019287  ...  0.158203 -0.173828 -0.047363   \n","4    -0.028931  0.077148  0.182617  ... -0.145508 -0.332031  0.133789   \n","...        ...       ...       ...  ...       ...       ...       ...   \n","3377 -0.423828 -0.123535  0.221680  ...  0.044678 -0.451172 -0.029907   \n","3378 -0.027710  0.103027  0.253906  ... -0.205078  0.009277 -0.139648   \n","3379 -0.160156  0.015564 -0.076172  ...  0.125000 -0.208984  0.074219   \n","3380  0.083496  0.188477  0.250000  ...  0.414062 -0.130859 -0.001709   \n","3381 -0.109863  0.332031  0.145508  ...  0.212891 -0.237305  0.437500   \n","\n","           294       295       296       297       298       299  sentiment  \n","0    -0.183594 -0.148438  0.014893 -0.003006  0.038574  0.175781          1  \n","1     0.146484 -0.029907 -0.176758 -0.234375  0.103516  0.112793         -1  \n","2    -0.024292  0.064941  0.112305 -0.155273 -0.192383  0.306641          1  \n","3     0.170898  0.020142  0.092285  0.010498  0.041016 -0.001282         -1  \n","4    -0.020874 -0.078613 -0.149414 -0.077637 -0.151367  0.165039          1  \n","...        ...       ...       ...       ...       ...       ...        ...  \n","3377 -0.051514  0.060791  0.275391 -0.121582  0.145508  0.045654         -1  \n","3378 -0.057129 -0.175781  0.189453  0.212891 -0.047852  0.088379          1  \n","3379 -0.263672 -0.265625  0.042480 -0.026855  0.006012  0.090332         -1  \n","3380  0.109375  0.005524  0.144531  0.277344 -0.075684  0.111328         -1  \n","3381 -0.177734 -0.090332  0.006500 -0.159180  0.096191  0.251953         -1  \n","\n","[3224 rows x 301 columns]"],"text/html":["\n","  <div id=\"df-f9ffe3c9-b569-40c2-b566-5cf5cee99847\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>291</th>\n","      <th>292</th>\n","      <th>293</th>\n","      <th>294</th>\n","      <th>295</th>\n","      <th>296</th>\n","      <th>297</th>\n","      <th>298</th>\n","      <th>299</th>\n","      <th>sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.057861</td>\n","      <td>0.160156</td>\n","      <td>-0.031128</td>\n","      <td>0.190430</td>\n","      <td>0.087891</td>\n","      <td>-0.182617</td>\n","      <td>-0.187500</td>\n","      <td>-0.176758</td>\n","      <td>0.159180</td>\n","      <td>0.232422</td>\n","      <td>...</td>\n","      <td>-0.304688</td>\n","      <td>-0.357422</td>\n","      <td>0.150391</td>\n","      <td>-0.183594</td>\n","      <td>-0.148438</td>\n","      <td>0.014893</td>\n","      <td>-0.003006</td>\n","      <td>0.038574</td>\n","      <td>0.175781</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.365234</td>\n","      <td>-0.089355</td>\n","      <td>-0.154297</td>\n","      <td>0.408203</td>\n","      <td>-0.375000</td>\n","      <td>-0.232422</td>\n","      <td>0.312500</td>\n","      <td>0.123535</td>\n","      <td>0.149414</td>\n","      <td>0.322266</td>\n","      <td>...</td>\n","      <td>-0.013855</td>\n","      <td>0.041260</td>\n","      <td>-0.002197</td>\n","      <td>0.146484</td>\n","      <td>-0.029907</td>\n","      <td>-0.176758</td>\n","      <td>-0.234375</td>\n","      <td>0.103516</td>\n","      <td>0.112793</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.384766</td>\n","      <td>0.425781</td>\n","      <td>0.093262</td>\n","      <td>0.032471</td>\n","      <td>-0.131836</td>\n","      <td>0.203125</td>\n","      <td>0.349609</td>\n","      <td>-0.236328</td>\n","      <td>-0.036133</td>\n","      <td>-0.071289</td>\n","      <td>...</td>\n","      <td>0.154297</td>\n","      <td>-0.437500</td>\n","      <td>-0.096191</td>\n","      <td>-0.024292</td>\n","      <td>0.064941</td>\n","      <td>0.112305</td>\n","      <td>-0.155273</td>\n","      <td>-0.192383</td>\n","      <td>0.306641</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.191406</td>\n","      <td>-0.174805</td>\n","      <td>0.046143</td>\n","      <td>0.324219</td>\n","      <td>-0.027954</td>\n","      <td>0.394531</td>\n","      <td>-0.242188</td>\n","      <td>0.419922</td>\n","      <td>0.402344</td>\n","      <td>0.019287</td>\n","      <td>...</td>\n","      <td>0.158203</td>\n","      <td>-0.173828</td>\n","      <td>-0.047363</td>\n","      <td>0.170898</td>\n","      <td>0.020142</td>\n","      <td>0.092285</td>\n","      <td>0.010498</td>\n","      <td>0.041016</td>\n","      <td>-0.001282</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.010925</td>\n","      <td>0.431641</td>\n","      <td>0.283203</td>\n","      <td>0.335938</td>\n","      <td>0.056152</td>\n","      <td>-0.460938</td>\n","      <td>0.061279</td>\n","      <td>-0.028931</td>\n","      <td>0.077148</td>\n","      <td>0.182617</td>\n","      <td>...</td>\n","      <td>-0.145508</td>\n","      <td>-0.332031</td>\n","      <td>0.133789</td>\n","      <td>-0.020874</td>\n","      <td>-0.078613</td>\n","      <td>-0.149414</td>\n","      <td>-0.077637</td>\n","      <td>-0.151367</td>\n","      <td>0.165039</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3377</th>\n","      <td>-0.139648</td>\n","      <td>0.154297</td>\n","      <td>-0.035645</td>\n","      <td>0.402344</td>\n","      <td>0.086914</td>\n","      <td>-0.039307</td>\n","      <td>0.051025</td>\n","      <td>-0.423828</td>\n","      <td>-0.123535</td>\n","      <td>0.221680</td>\n","      <td>...</td>\n","      <td>0.044678</td>\n","      <td>-0.451172</td>\n","      <td>-0.029907</td>\n","      <td>-0.051514</td>\n","      <td>0.060791</td>\n","      <td>0.275391</td>\n","      <td>-0.121582</td>\n","      <td>0.145508</td>\n","      <td>0.045654</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3378</th>\n","      <td>-0.045410</td>\n","      <td>-0.019043</td>\n","      <td>0.059326</td>\n","      <td>0.060791</td>\n","      <td>0.068848</td>\n","      <td>-0.437500</td>\n","      <td>-0.013245</td>\n","      <td>-0.027710</td>\n","      <td>0.103027</td>\n","      <td>0.253906</td>\n","      <td>...</td>\n","      <td>-0.205078</td>\n","      <td>0.009277</td>\n","      <td>-0.139648</td>\n","      <td>-0.057129</td>\n","      <td>-0.175781</td>\n","      <td>0.189453</td>\n","      <td>0.212891</td>\n","      <td>-0.047852</td>\n","      <td>0.088379</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3379</th>\n","      <td>0.267578</td>\n","      <td>0.125977</td>\n","      <td>0.225586</td>\n","      <td>0.234375</td>\n","      <td>-0.236328</td>\n","      <td>0.355469</td>\n","      <td>0.059570</td>\n","      <td>-0.160156</td>\n","      <td>0.015564</td>\n","      <td>-0.076172</td>\n","      <td>...</td>\n","      <td>0.125000</td>\n","      <td>-0.208984</td>\n","      <td>0.074219</td>\n","      <td>-0.263672</td>\n","      <td>-0.265625</td>\n","      <td>0.042480</td>\n","      <td>-0.026855</td>\n","      <td>0.006012</td>\n","      <td>0.090332</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3380</th>\n","      <td>0.072754</td>\n","      <td>0.182617</td>\n","      <td>-0.075684</td>\n","      <td>-0.125000</td>\n","      <td>-0.112305</td>\n","      <td>0.152344</td>\n","      <td>0.061035</td>\n","      <td>0.083496</td>\n","      <td>0.188477</td>\n","      <td>0.250000</td>\n","      <td>...</td>\n","      <td>0.414062</td>\n","      <td>-0.130859</td>\n","      <td>-0.001709</td>\n","      <td>0.109375</td>\n","      <td>0.005524</td>\n","      <td>0.144531</td>\n","      <td>0.277344</td>\n","      <td>-0.075684</td>\n","      <td>0.111328</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3381</th>\n","      <td>0.048096</td>\n","      <td>0.138672</td>\n","      <td>-0.117188</td>\n","      <td>-0.031982</td>\n","      <td>-0.421875</td>\n","      <td>0.032959</td>\n","      <td>0.296875</td>\n","      <td>-0.109863</td>\n","      <td>0.332031</td>\n","      <td>0.145508</td>\n","      <td>...</td>\n","      <td>0.212891</td>\n","      <td>-0.237305</td>\n","      <td>0.437500</td>\n","      <td>-0.177734</td>\n","      <td>-0.090332</td>\n","      <td>0.006500</td>\n","      <td>-0.159180</td>\n","      <td>0.096191</td>\n","      <td>0.251953</td>\n","      <td>-1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3224 rows × 301 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9ffe3c9-b569-40c2-b566-5cf5cee99847')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f9ffe3c9-b569-40c2-b566-5cf5cee99847 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f9ffe3c9-b569-40c2-b566-5cf5cee99847');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["y_pretrained = X_pretrained['sentiment'].copy()\n","X_pretrained = X_pretrained.drop(columns = 'sentiment')\n","X_train_pretrained, X_test_pretrained, y_train_pretrained, y_test_pretrained = train_test_split(X_pretrained, y_pretrained, random_state=0, test_size=0.1,\n","                                                                                                stratify=y_pretrained)"],"metadata":{"id":"0ncjW6aKFFn6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_pretrained = LogisticRegression(max_iter=1000000)\n","lr_pretrained.fit(X_train_pretrained, y_train_pretrained)\n","y_pred_pretrained = lr_pretrained.predict(X_test_pretrained)\n","\n","conf_matrix_pretrained = confusion_matrix(y_test_pretrained, y_pred_pretrained)\n","print(conf_matrix_pretrained)\n","\n","print(classification_report(y_test_pretrained, y_pred_pretrained))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wlYsMYO29dEA","executionInfo":{"status":"ok","timestamp":1652043984214,"user_tz":240,"elapsed":14,"user":{"displayName":"Sebastián Tinoco Pérez","userId":"06104689937544930682"}},"outputId":"8bac77bf-fbb4-414e-ee2a-89ebe2d3382a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[206   5]\n"," [  6 106]]\n","              precision    recall  f1-score   support\n","\n","          -1       0.97      0.98      0.97       211\n","           1       0.95      0.95      0.95       112\n","\n","    accuracy                           0.97       323\n","   macro avg       0.96      0.96      0.96       323\n","weighted avg       0.97      0.97      0.97       323\n","\n"]}]},{"cell_type":"markdown","source":["Se concluye que los resultados mejorar substancialmente al usar embeddings entrenados con un corpus mejor y más grande."],"metadata":{"id":"g_rjiBekh6LT"}}]}