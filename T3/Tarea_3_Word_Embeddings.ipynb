{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckbt7VPDhBwb"
      },
      "source": [
        "# **Tarea 3 - Word Embeddings üìö**\n",
        "\n",
        "**Nombre: Jos√© Luis C√°diz Sejas**\n",
        "\n",
        "**Fecha l√≠mite de entrega üìÜ:** 10 de mayo.\n",
        "\n",
        "**Tiempo estimado de dedicaci√≥n:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-19T18:30:18.109327Z",
          "start_time": "2020-03-19T18:30:18.103344Z"
        },
        "id": "q5CSRY4oNCHK"
      },
      "source": [
        "\n",
        "**Instrucciones:**\n",
        "- El ejercicio consiste en:\n",
        "    - Responder preguntas relativas a los contenidos vistos en los v√≠deos y slides de las clases. \n",
        "    - Entrenar Word2Vec y Word Context Matrix sobre un peque√±o corpus.\n",
        "    - Evaluar los embeddings obtenidos en una tarea de clasificaci√≥n.\n",
        "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo Jupyter Notebook.\n",
        "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n. \n",
        "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso. \n",
        "\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "V√≠deos: \n",
        "\n",
        "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
        "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
        "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4wYf0vgnbTv"
      },
      "source": [
        "## **Preguntas te√≥ricas üìï (2 puntos).** ##\n",
        "Para estas preguntas no es necesario implementar c√≥digo, pero pueden utilizar pseudo c√≥digo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5hUG6-8ngoK"
      },
      "source": [
        "### **Parte 1: Modelos Lineales (1 ptos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yRvZbhsoi8f"
      },
      "source": [
        "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor√≠as: pol√≠tica, deporte, negocios y otros. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irsqBVmCnx3M"
      },
      "source": [
        "**Pregunta 1**: Dise√±e un modelo lineal capaz de clasificar un documento seg√∫n estas categor√≠as donde el output sea un vector con una distribuci√≥n de probabilidad con la pertenencia a cada clase. \n",
        "\n",
        "Especifique: representaci√≥n de los documentos de entrada, par√°metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci√≥n de p√©rdida escogida. **(0.5 puntos)**\n",
        "\n",
        "**Respuesta**: A continuaci√≥n se enumeran los pasos a seguir para dise√±ar un modelo ilneal que clasifique documentos.\n",
        "1. **Representaci√≥n**: Una representaci√≥n b√°sica podr√≠a ser One-Hot Vector, el cual consiste en representar cada oraci√≥n del corpus como un vector sparse del tama√±o del vacabulario, el cual tiene un 1 si la palabra aparece y 0 si no. Otra opci√≥n es Bag-of-Words (BOW), el cual tambi√©n representa las oraciones como vectores sparse pero intenta capturar adem√°s, el orden de las palabras a traves de bigramas,trigramas etc. Esta representaci√≥n vendr√° dada por\n",
        "el vector $\\vec{x}$.\n",
        "\n",
        "2. **Modelo**: Consideremos un modelo lineal de la forma $\\vec{\\hat{y}}=f(x)=\\vec{x}\\cdot W+\\vec{b}$, donde $\\vec{x}$ es el input. $W$, $\\vec{b}$ son par√°metros del modelo e $\\vec{\\hat{y}}$ es el Output de dimensi√≥n igual al n√∫mero de etiquetas de la tarea de clasificaci√≥n, en este caso dimensi√≥n 4.\n",
        "\n",
        "\n",
        "3. **Transformaciones del Ouput para obtener probabilidad de etiqueta**: Para obtener una representaci√≥n probabilistica del Output del modelo lineal, dicho Ouput es pasado por una funci√≥n $\\textit{Softmax}$ :\n",
        "\n",
        "$\\vec{\\hat{y}}=\\textit{Softmax}(\\vec{x}\\cdot W+\\vec{b})$, donde la predicci√≥n estar√° dada por:  $Prediction=\\hat{y}=argmax_{i}(\\vec{\\hat{y}}_{[i]})$, es decir, se asigna la etiqueta que tenga mayor probabilidad.\n",
        "\n",
        "\n",
        "4. **Entrenamiento**: para mejorar la calidad de las predicciones se deben ajustar los par√°metros del modelo de modo de maximizar la capacidad predictiva del modelo. Para esto es de vital importancia definir una\n",
        "funci√≥n de p√©rdida o tambi√©n conocida como $\\textit{Loss}$. Para una predicci√≥n dada, La funci√≥n de $\\textit{Loss}$, puede ser definida como\n",
        "$\\textit{Loss}$=$L(f(\\vec{x},\\vec{\\Theta}),y_{true})$=$L(\\hat{y},y_{true})$, donde $\\vec{\\Theta}$ representa los par√°metros del modelo, $\\hat{y}$ la predicci√≥n e $y_{true}$ la etiqueta correcta. Para considerar todas las predicciones que se har√≠an sobre el conjunto de entrenamiento se define la funci√≥n de $\\textit{Loss}$ para todo el $\\textit{Corpus}$: $\\mathcal{L}(\\vec{\\Theta})=\\frac{1}{N}\\sum_{i=1}^{N}L(\\hat{y_{i}},y_{true_{i}})$. Una funci√≥n de p√©rdida adecuada para la tarea de clasificaci√≥n es la conocida $\\textit{Cross-Entropy-Loss}$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5FaWqBVvL90"
      },
      "source": [
        "**Pregunta 2**: Explique c√≥mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci√≥n. **(0.5 puntos)**\n",
        "\n",
        "**Respuesta**: Como se menciono anteriormente, para el proceso de entrenamiento se debe definir una funci√≥n de p√©rdida a optimizar con el objetivo de obtener los par√°metros $\\vec{\\Theta}$ que minimicen la funci√≥n de p√©rdida. \n",
        "Matem√°ticamente estos par√°metros estar√°n dados por la siguiente expresi√≥n:\n",
        "$$\\hat{\\Theta}=\\textit{Argmin}_{\\vec{\\Theta}}\\mathcal{L}(\\vec{\\Theta})=\\textit{Argmin}_{\\vec{\\Theta}}\\frac{1}{N}\\sum_{i=1}^{N}L(\\hat{y_{i}},y_{true_{i}})$$\n",
        "\n",
        "\n",
        "En este caso para la tarea que se busca cumplir una buena funci√≥n de p√©rdida es  $\\textit{Cross-Entropy-Loss}$, la cual cuantifica la no similitud entre la etiqueta real y la etiqueta predicha, matem√°ticamente esta definida por la siguiente expresi√≥n:\n",
        "$$\\mathcal{L}_{cross-entropy}(\\vec{\\hat{y}},\\vec{y}_{true})=-\\sum_{i=1}^{N}\\vec{y}_{true_{i}}\\cdot\\log(\\vec{\\hat{y_{i}}})$$ \n",
        "\n",
        "El m√©todo de optimizaci√≥n utilizado estan basandos en los m√©todos basados en gradientes. Estos m√©todos son m√°s econ√≥micos computacionalmente que derivar sobre todas las variables y calcular las diversas soluciones de cada derivada parcial particular. Un ejemplo b√°sico de este tipo de m√©todos es el $\\textit{``Online Stochastic Gradient Descent''} $. \n",
        "\n",
        "Este m√©todo consiste en inicializar los par√°metros del modelo con valores aleatorios y mientras no se cumpla un criterio de stop, como por ejemplo que la variaci√≥n porcentual entre el $\\vec{\\Theta}$ anterior y el $\\vec{\\Theta}$ actualizado no sea lo suficiemente peque√±o a un $\\vec{\\varepsilon}$, se iterar√° sobre cada registro del conjunto de entrenamiento, calculando la predicci√≥n que har√≠a el modelo, luego se computa el error mediante la funci√≥n de p√©rdida y la etiqueta verdadera, para luego calcular el gradiente respecto a los par√°metros $\\vec{\\Theta}$. Luego $\\vec{\\Theta}$ es actualizado: $\\vec{\\Theta}=\\vec{\\Theta}-\\eta\\cdot\\nabla\\mathcal{L}$, donde $\\eta$ se conoce como la tasa de aprendizaje. Esta actualizaci√≥n continuar√° hasta que se cumpla el criterio de stop, con esto obteniendo los par√°metros del modelo ya entrenado.\n",
        "\n",
        "Para evaluar el modelo, se debe contar con un conjunto de test, totalmente distinto al conjunto de entrenamiento, el cual debe contar tambi√©n con las etiquetas verdaderas, de modo de hacer predicciones con el modelo ya entrenado, para luego calcular alguna m√©trica de clasificaci√≥n como $\\textit{Accuracy, Precision, Recall, F1}$ etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkK7pc54njZq"
      },
      "source": [
        "### **Parte 2: Redes Neuronales (1 ptos)** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUbJjlj_9AFC"
      },
      "source": [
        "Supongamos que tenemos la siguiente red neuronal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obUfuOYB_TOC"
      },
      "source": [
        "![image.png](https://drive.google.com/uc?export=view&id=1fFTjtMvH6MY8o42_vj010y8eTuCVb5a3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2z-8zKW0_6q"
      },
      "source": [
        "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem√°tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci√≥n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n",
        "\n",
        "Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.5 Puntos)**\n",
        "\n",
        "**Respuesta**: \n",
        "\n",
        "Formula:\n",
        "$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) =\\vec{h}^{3}\\cdot\\vec{W}^{4}$, $\\vec{W}^{4}$ dim (1,4).\n",
        "\n",
        "donde:\n",
        "\n",
        "* $\\vec{h}^{3}=h(\\vec{h}^{2}\\cdot\\vec{W}^{3}+\\vec{b}^{3})$, donde $\\vec{W}^{3}$ dim (3,1), $\\vec{b}^{3}$ dim (1,1).\n",
        "* $\\vec{h}^{2}=f(\\vec{h}^{1}\\cdot\\vec{W}^{2}+\\vec{b}^{2})$, donde $\\vec{W}^{2}$ dim (2,3), $\\vec{b}^{2}$ dim (1,3).\n",
        "* $\\vec{h}^{1}=g(\\vec{x}\\cdot\\vec{W}^{1}+\\vec{b}^{1})$, donde $\\vec{W}^{1}$ dim (3,2), $\\vec{b}^{1}$ dim (1,2), $\\vec{x}$ dim (1,3).\n",
        "\n",
        "**Pregunta 2**: Explique qu√© es backpropagation. ¬øCuales ser√≠an los par√°metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n",
        "\n",
        "**Respuesta**: Los par√°metros a optimizar dentro de una red neuronal son los pesos sinapticos asociados a las conexiones entre una capa y otra, junto con sus respectivos bias de cada capa. Para optimizar los par√°metros de la red neuronal se utilizan m√©todos del descenso del gradiente, pero el problema es que las redes neuronales se caracterizan por tener una gran cantidad de par√°metros, lo que en la practica hace muy costoso cumputacionalmente calcular todas las derivadas. Por esto es necesario calcular estas derivadas de una manera m√°s eficiente. \n",
        "\n",
        "Backpropagation es un t√©cnica que permite obtener las derivadas parciales de una funci√≥n $\\textit{Loss}$ respecto a todos los par√°metros de un modelo de una manera m√°s eficiente guardando las derivadas que se repiten. El gradiente se calcula de forma recursiva, es decir se calculan las derivadas parciales desde las capas m√°s profundas hasta las m√°s superficiales, de modo de ir guardando las derivadas profundas que luego son utilizadas en las derivadas parciales de las capas menos profundas producto de la regla de la cadena. Esto sucede porque se entiende que el error imputado de un par√°metro depende de los errores imputados de los par√°metros de las capas m√°s profundas, es decir el error imputado de cada par√°metro se propaga de adelante hacia atras ($\\textit{Back-Propagation}$). Con esto finalmente, se logra reducir el costo cumputacional al no recalcular derivadas. \n",
        "\n",
        "\n",
        "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.25 puntos)**\n",
        "\n",
        "**Respuesta**: La idea es calcular el gradiente de manera eficiente, i.e. calcular las derivadas parciales de los par√°metros de las capas superiores secuencialmente hasta las capas  inferiores. Del tal modo de aprovechar la dependencia de las derivadas parciales de las capas inferiores respecto a las superiores, reciclando ciertos calculos. Esto en computaci√≥n se llama programaci√≥n din√°mica.\n",
        "\n",
        "Pasos:\n",
        "\n",
        "1. Se inicializa la red con valores aleatorios para todos sus pesos.\n",
        "2. $\\textbf{Fordward propagation}$ - (Evaluar ejemplos): Para cada ejemplo de los datos de entrenamiento se alimenta la red neuronal, se calculan las funciones de activaci√≥n desde las capas inferiores hasta las superiores de forma recursiva. seg√∫n la ecuaci√≥n:\n",
        "$$h^{l}_{j}=\\left (\\sum_{i}W_{i,j}^{l}\\cdot z_{i}^{l-1}\\right )+b_{j}^{l}\\hspace{0.1cm}donde\\hspace{0.1cm} z_{j}^{l}=g\\left(h_{j}^{l}\\right), z_{j}^{0}=x_{j}$$\n",
        "\n",
        "3. Se obtiene la salida de la red y se evalua la $\\textit{Loss}$, $\\mathcal{L}(\\vec{\\Theta},\\vec{\\hat{y}},\\vec{y}_{true})$, obteniendo una $\\textit{Loss}$ de la forma  $\\mathcal{L}(\\vec{\\Theta})$.\n",
        "4. $\\textbf{Back propagation}$: Calcular el gradiente de $\\mathcal{L}(\\vec{\\Theta})$ mediante el calculo de las derivadas parciales de los par√°metros desde las capas superiores hasta las inferiores, guardando las derivadas que se van obteniendo. La derivada de un par√°metro $W^{l}_{i,j}$ esta definida por la siguiente expresi√≥n:\n",
        "$$\\frac{\\partial\\mathcal{L}}{\\partial W^{l}_{i,j}}=\\delta^{l}_{j}\\cdot z^{l-1}_{i}$$\n",
        "\n",
        "donde $\\delta^{l}_{j}=\\frac{\\partial\\mathcal{L}}{\\partial h^{l}_{j}}=\\sum_{k}\\left( \\delta^{l+1}_{k} \\cdot W^{l+1}_{j,k}\\cdot g'\\left(h^{l}_{j}\\right)    \\right)$, de este modo se deben  obtener los deltas de las capas superiores para obtener los deltas de las capas inferiores. Dicho de otra forma, se deben obtener las derivadas parciales del $\\textit{Loss}$ respecto a la salida de cada neurona desde las capas superiores hasta las capas inferiores. En este caso particular de la capa 3, 2 y 1 de forma secuencial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocS_vQhR1gcU"
      },
      "source": [
        "## **Preguntas pr√°cticas üíª (4 puntos).** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol82nJ0FnmcP"
      },
      "source": [
        "### **Parte 3: Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Daw7Ee5cdQTb"
      },
      "source": [
        "En la auxiliar 2 se nombraron dos formas de crear word vectors:\n",
        "\n",
        "-  Distributional Vectors.\n",
        "-  Distributed Vectors.\n",
        "\n",
        "El objetivo de esta parte es comparar las dos embeddings obtenidos de estas dos estrategias en una tarea de clasificaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E2G1qcb7AJqW"
      },
      "outputs": [],
      "source": [
        "import re  \n",
        "import pandas as pd \n",
        "from time import time  \n",
        "from collections import defaultdict \n",
        "import string \n",
        "import multiprocessing\n",
        "import os\n",
        "import gensim\n",
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import scipy\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
        "\n",
        "# word2vec\n",
        "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuEAv-whdMCG"
      },
      "source": [
        "#### **Parte A (1 punto)** \n",
        "\n",
        "En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librer√≠as ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n",
        "\n",
        "```python\n",
        "class WordContextMatrix:\n",
        "\n",
        "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Utilice el constructor para definir los parametros.\n",
        "    \"\"\"\n",
        "\n",
        "    # se sugiere agregar un una estructura de datos para guardar las\n",
        "    # palabras del vocab y para guardar el conteo de coocurrencia\n",
        "    ...\n",
        "    \n",
        "  def add_word_to_vocab(self, word):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para agregar token\n",
        "    a sus vocabulario\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # Le puede ser √∫til considerar un token unk al vocab\n",
        "    # para palabras fuera del vocab\n",
        "    ...\n",
        "  \n",
        "  def build_matrix(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para crear la palabra contexto\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def matrix2dict(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para convertir la matriz a un diccionario de embeddings, donde las llaves deben ser los token del vocabulario y los embeddings los valores obtenidos de la matriz. \n",
        "    \"\"\"\n",
        "\n",
        "    # se recomienda transformar la matrix a un diccionario de embedding.\n",
        "    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n",
        "    ...\n",
        "\n",
        "```\n",
        "\n",
        "puede modificar los par√°metros o m√©todos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n",
        "\n",
        "```python\n",
        "corpus = [\n",
        "  \"I like deep learning.\",\n",
        "  \"I like NLP.\",\n",
        "  \"I enjoy flying.\"\n",
        "]\n",
        "```\n",
        "\n",
        "Obteniendo una matriz parecia a esta:\n",
        "\n",
        "***Resultado esperado***: \n",
        "\n",
        "| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n",
        "|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n",
        "| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n",
        "| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n",
        "| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n",
        "| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n",
        "| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n",
        "| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n",
        "| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n",
        "| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n",
        "\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur16vkyO37B5"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOI1FL8MlGZB"
      },
      "outputs": [],
      "source": [
        "class WordContextMatrix:\n",
        "\n",
        "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Utilice el constructor para definir los parametros.\n",
        "    \"\"\"\n",
        "\n",
        "    # se sugiere agregar un una estructura de datos para guardar las\n",
        "    # palabras del vocab y para guardar el conteo de coocurrencia\n",
        "    ...\n",
        "    \n",
        "  def add_word_to_vocab(self, word):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para agregar token\n",
        "    a sus vocabulario\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # Le puede ser √∫til considerar un token unk al vocab\n",
        "    # para palabras fuera del vocab\n",
        "    ...\n",
        "  \n",
        "  def build_matrix(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para crear la palabra contexto\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def matrix2dict(self):\n",
        "    \"\"\"\n",
        "    Utilice este m√©todo para convertir la matriz a un diccionario de embeddings, donde las llaves deben ser los token del vocabulario y los embeddings los valores obtenidos de la matriz. \n",
        "    \"\"\"\n",
        "\n",
        "    # se recomienda transformar la matrix a un diccionario de embedding.\n",
        "    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\n",
        "  \"I like deep learning.\",\n",
        "  \"I like NLP.\",\n",
        "  \"I enjoy flying.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['deep', 'learning', 'NLP', 'like', 'flying', '.', 'enjoy', 'I']"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vocabulario\n",
        "\n",
        "from nltk.tokenize import wordpunct_tokenize \n",
        "vocabulario=[]\n",
        "for i in corpus:\n",
        "    vocabulario=vocabulario+wordpunct_tokenize(i)\n",
        "\n",
        "vocabulario=list(set(vocabulario))\n",
        "vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocabulario=vocabulario+['jeje']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'deep': ['I', 'like', 'learning', '.'],\n",
              " 'learning': ['like', 'deep', '.'],\n",
              " 'NLP': ['I', 'like', '.'],\n",
              " 'like': ['I', 'deep', 'learning', 'I', 'NLP', '.'],\n",
              " 'flying': ['I', 'enjoy', '.'],\n",
              " '.': ['deep', 'learning', 'like', 'NLP', 'enjoy', 'flying'],\n",
              " 'enjoy': ['I', 'flying', '.'],\n",
              " 'I': ['like', 'deep', 'like', 'NLP', 'enjoy', 'flying'],\n",
              " 'jeje': []}"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k=2\n",
        "dic_embeddings={}\n",
        "for i in vocabulario:\n",
        "    dic_embeddings[i]=[]\n",
        "    #print(i)\n",
        "    for j in corpus:\n",
        "        sentence_tokenized=wordpunct_tokenize(j)\n",
        "        if i in sentence_tokenized:\n",
        "            #print(i)\n",
        "            #print(sentence_tokenized)\n",
        "            indice=sentence_tokenized.index(i) # Ubicaci√≥n de la palabra del vocabulario en la oraci√≥n a inspeccionar\n",
        "\n",
        "\n",
        "            # Lado izquierdo\n",
        "            izquierdos=sentence_tokenized[max(indice-k,0):indice]\n",
        "            #print(izquierdos)\n",
        "\n",
        "            \n",
        "            # Lado derecho\n",
        "            derechos=sentence_tokenized[indice+1:min(indice+k+1,len(sentence_tokenized)+1)]\n",
        "            #print(derechos)\n",
        "\n",
        "            dic_embeddings[i]=dic_embeddings[i]+izquierdos+derechos\n",
        "            \n",
        "dic_embeddings            \n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Estructurar diccionario final\n",
        "dic_fin={}\n",
        "for i in vocabulario:\n",
        "    dic_fin[i]={}\n",
        "    counts=np.unique(dic_embeddings[i],return_counts=True)\n",
        "    \n",
        "    for a,b in  zip(counts[0],counts[1]):\n",
        "        \n",
        "        dic_fin[i][a]=b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'deep': {'.': 1, 'I': 1, 'learning': 1, 'like': 1},\n",
              " 'learning': {'.': 1, 'deep': 1, 'like': 1},\n",
              " 'NLP': {'.': 1, 'I': 1, 'like': 1},\n",
              " 'like': {'.': 1, 'I': 2, 'NLP': 1, 'deep': 1, 'learning': 1},\n",
              " 'flying': {'.': 1, 'I': 1, 'enjoy': 1},\n",
              " '.': {'NLP': 1, 'deep': 1, 'enjoy': 1, 'flying': 1, 'learning': 1, 'like': 1},\n",
              " 'enjoy': {'.': 1, 'I': 1, 'flying': 1},\n",
              " 'I': {'NLP': 1, 'deep': 1, 'enjoy': 1, 'flying': 1, 'like': 2},\n",
              " 'jeje': {}}"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dic_fin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgmeSFqKLpFL"
      },
      "source": [
        "#### **Parte B (1.5 puntos)**\n",
        "\n",
        "En esta parte es debe entrenar Word2Vec de gensim y construir la matriz palabra contexto utilizando el dataset de di√°logos de los Simpson. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZgN06q4QPi3"
      },
      "source": [
        "Utilizando el dataset adjunto con la tarea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY3kmg4onnsu",
        "outputId": "d3525a54-0c10-401e-b3e2-9c6e9e714a2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-05-05 17:58:59,568 : INFO : NumExpr defaulting to 2 threads.\n"
          ]
        }
      ],
      "source": [
        "data_file = \"dialogue-lines-of-the-simpsons.zip\"\n",
        "df = pd.read_csv(data_file)\n",
        "stopwords = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
        ").values\n",
        "stopwords = Counter(stopwords.flatten().tolist())\n",
        "df = df.dropna().reset_index(drop=True) # Quitar filas vacias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAg5a5bmWk3T"
      },
      "source": [
        "**Pregunta 1**: Ayud√°ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec. **(0.75 punto)** (Hint, le puede servir explorar un poco los datos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWw2fXFRXe5Y"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvwplz7yTNcr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vBkF3hreGjg"
      },
      "source": [
        "**Pregunta 2**: Cree una matriz palabra contexto usando el mismo dataset. Configure el largo del vocabulario a 1000 o 2000 tokens, puede agregar valores mayores pero tenga en cuenta que la construcci√≥n de la matriz puede tomar varios minutos. Puede que esto tarde un poco. **(0.75 punto)** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzLuH6MneWIY"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gPyW8fMeXNX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRCB-jqgTNcs"
      },
      "source": [
        "#### **Parte C (1.5 puntos): Aplicar embeddings para clasificar**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlqzlJRSTNcs"
      },
      "source": [
        "Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n",
        "\n",
        "Para esto ocuparemos el lexic√≥n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci√≥n es positiva y un -1 si es negativa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMskFDmHTNcs"
      },
      "outputs": [],
      "source": [
        "AFINN = 'AFINN_full.csv'\n",
        "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaKl8hsCTNcs"
      },
      "source": [
        "Hint: Para w2v y la wcm son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr√°n una representaci√≥n en AFINN. Para el caso de la matriz palabra contexto se recomienda convertir su matrix a un diccionario. Pueden utilizar esta funci√≥n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWSSuctiTNcs"
      },
      "outputs": [],
      "source": [
        "def try_apply(model,word):\n",
        "    try:\n",
        "        aux = model[word]\n",
        "        return True\n",
        "    except KeyError:\n",
        "        #logger.error('Word {} not in dictionary'.format(word))\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrVPeEzgTNcs"
      },
      "source": [
        "**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci√≥n en embedding que acabamos de calcular (con ambos modelos). \n",
        "\n",
        "Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n",
        "\n",
        "Para ambos modelos, separar train y test de acuerdo a la siguiente funci√≥n. **(0.5 puntos)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bkt26BwTNcs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmFoKWKO2EKA"
      },
      "source": [
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDcq5czXTNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upAn_eT4TNct"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDKe4gA3TNct"
      },
      "source": [
        "**Pregunta 2**: Entrenar una regresi√≥n log√≠stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu√© se obtienen estos resultados? C√≥mo los mejorar√≠as? Como podr√≠as mejorar los resultados de la matriz palabra contexto? es equivalente al modelo word2vec? **(1 punto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJMzq_dETNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj1r_BnKn_7L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izppruGQTNct"
      },
      "source": [
        "# Bonus: +0.25 puntos en cualquier pregunta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW0aeK2KTNct"
      },
      "source": [
        "**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m√°s grande y obtener mejores resultados. Les puede servir [√©sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvHcVS3sTNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSc8p-T8TNcu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tarea_3_Word_Embeddings.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "65e97ffa20fbdd895c76c3a1d2c2ade704754874d6e45df701862667e65d73d7"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 ('Programacion_Cientifica')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
