{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckbt7VPDhBwb"
      },
      "source": [
        "# **Tarea 3 - Word Embeddings **\n",
        "\n",
        "**Nombre: Jos茅 Luis C谩diz Sejas**\n",
        "\n",
        "**Fecha l铆mite de entrega :** 10 de mayo.\n",
        "\n",
        "**Tiempo estimado de dedicaci贸n:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-19T18:30:18.109327Z",
          "start_time": "2020-03-19T18:30:18.103344Z"
        },
        "id": "q5CSRY4oNCHK"
      },
      "source": [
        "\n",
        "**Instrucciones:**\n",
        "- El ejercicio consiste en:\n",
        "    - Responder preguntas relativas a los contenidos vistos en los v铆deos y slides de las clases. \n",
        "    - Entrenar Word2Vec y Word Context Matrix sobre un peque帽o corpus.\n",
        "    - Evaluar los embeddings obtenidos en una tarea de clasificaci贸n.\n",
        "- La tarea se realiza en grupos de **m谩ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a trav茅s de u-cursos a m谩s tardar el d铆a estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo Jupyter Notebook.\n",
        "- Al momento de la revisi贸n tu c贸digo ser谩 ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci贸n. \n",
        "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav茅s del canal de Discord del curso. \n",
        "\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "V铆deos: \n",
        "\n",
        "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
        "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
        "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4wYf0vgnbTv"
      },
      "source": [
        "## **Preguntas te贸ricas  (2 puntos).** ##\n",
        "Para estas preguntas no es necesario implementar c贸digo, pero pueden utilizar pseudo c贸digo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5hUG6-8ngoK"
      },
      "source": [
        "### **Parte 1: Modelos Lineales (1 ptos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yRvZbhsoi8f"
      },
      "source": [
        "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categor铆as: pol铆tica, deporte, negocios y otros. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irsqBVmCnx3M"
      },
      "source": [
        "**Pregunta 1**: Dise帽e un modelo lineal capaz de clasificar un documento seg煤n estas categor铆as donde el output sea un vector con una distribuci贸n de probabilidad con la pertenencia a cada clase. \n",
        "\n",
        "Especifique: representaci贸n de los documentos de entrada, par谩metros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y funci贸n de p茅rdida escogida. **(0.5 puntos)**\n",
        "\n",
        "**Respuesta**: A continuaci贸n se enumeran los pasos a seguir para dise帽ar un modelo ilneal que clasifique documentos.\n",
        "1. **Representaci贸n**: Una representaci贸n b谩sica podr铆a ser One-Hot Vector, el cual consiste en representar cada oraci贸n del corpus como un vector sparse del tama帽o del vacabulario, el cual tiene un 1 si la palabra aparece y 0 si no. Otra opci贸n es Bag-of-Words (BOW), el cual tambi茅n representa las oraciones como vectores sparse pero intenta capturar adem谩s, el orden de las palabras a traves de bigramas,trigramas etc. Esta representaci贸n vendr谩 dada por\n",
        "el vector $\\vec{x}$.\n",
        "\n",
        "2. **Modelo**: Consideremos un modelo lineal de la forma $\\vec{\\hat{y}}=f(x)=\\vec{x}\\cdot W+\\vec{b}$, donde $\\vec{x}$ es el input. $W$, $\\vec{b}$ son par谩metros del modelo e $\\vec{\\hat{y}}$ es el Output de dimensi贸n igual al n煤mero de etiquetas de la tarea de clasificaci贸n, en este caso dimensi贸n 4.\n",
        "\n",
        "\n",
        "3. **Transformaciones del Ouput para obtener probabilidad de etiqueta**: Para obtener una representaci贸n probabilistica del Output del modelo lineal, dicho Ouput es pasado por una funci贸n $\\textit{Softmax}$ :\n",
        "\n",
        "$\\vec{\\hat{y}}=\\textit{Softmax}(\\vec{x}\\cdot W+\\vec{b})$, donde la predicci贸n estar谩 dada por:  $Prediction=\\hat{y}=argmax_{i}(\\vec{\\hat{y}}_{[i]})$, es decir, se asigna la etiqueta que tenga mayor probabilidad.\n",
        "\n",
        "\n",
        "4. **Entrenamiento**: para mejorar la calidad de las predicciones se deben ajustar los par谩metros del modelo de modo de maximizar la capacidad predictiva del modelo. Para esto es de vital importancia definir una\n",
        "funci贸n de p茅rdida o tambi茅n conocida como $\\textit{Loss}$. Para una predicci贸n dada, La funci贸n de $\\textit{Loss}$, puede ser definida como\n",
        "$\\textit{Loss}$=$L(f(\\vec{x},\\vec{\\Theta}),y_{true})$=$L(\\hat{y},y_{true})$, donde $\\vec{\\Theta}$ representa los par谩metros del modelo, $\\hat{y}$ la predicci贸n e $y_{true}$ la etiqueta correcta. Para considerar todas las predicciones que se har铆an sobre el conjunto de entrenamiento se define la funci贸n de $\\textit{Loss}$ para todo el $\\textit{Corpus}$: $\\mathcal{L}(\\vec{\\Theta})=\\frac{1}{N}\\sum_{i=1}^{N}L(\\hat{y_{i}},y_{true_{i}})$. Una funci贸n de p茅rdida adecuada para la tarea de clasificaci贸n es la conocida $\\textit{Cross-Entropy-Loss}$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5FaWqBVvL90"
      },
      "source": [
        "**Pregunta 2**: Explique c贸mo funciona el proceso de entrenamiento en este tipo de modelos y su evaluaci贸n. **(0.5 puntos)**\n",
        "\n",
        "**Respuesta**: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkK7pc54njZq"
      },
      "source": [
        "### **Parte 2: Redes Neuronales (1 ptos)** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUbJjlj_9AFC"
      },
      "source": [
        "Supongamos que tenemos la siguiente red neuronal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obUfuOYB_TOC"
      },
      "source": [
        "![image.png](https://drive.google.com/uc?export=view&id=1fFTjtMvH6MY8o42_vj010y8eTuCVb5a3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2z-8zKW0_6q"
      },
      "source": [
        "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matem谩tica. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en funci贸n del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n",
        "\n",
        "Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.5 Puntos)**\n",
        "\n",
        "**Respuesta**: \n",
        "\n",
        "Formula:\n",
        "$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) =$\n",
        "\n",
        "Dimensiones: \n",
        "\n",
        "**Pregunta 2**: Explique qu茅 es backpropagation. 驴Cuales ser铆an los par谩metros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n",
        "\n",
        "**Respuesta**:\n",
        "\n",
        "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.25 puntos)**\n",
        "\n",
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocS_vQhR1gcU"
      },
      "source": [
        "## **Preguntas pr谩cticas  (4 puntos).** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol82nJ0FnmcP"
      },
      "source": [
        "### **Parte 3: Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Daw7Ee5cdQTb"
      },
      "source": [
        "En la auxiliar 2 se nombraron dos formas de crear word vectors:\n",
        "\n",
        "-  Distributional Vectors.\n",
        "-  Distributed Vectors.\n",
        "\n",
        "El objetivo de esta parte es comparar las dos embeddings obtenidos de estas dos estrategias en una tarea de clasificaci贸n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E2G1qcb7AJqW"
      },
      "outputs": [],
      "source": [
        "import re  \n",
        "import pandas as pd \n",
        "from time import time  \n",
        "from collections import defaultdict \n",
        "import string \n",
        "import multiprocessing\n",
        "import os\n",
        "import gensim\n",
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import scipy\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
        "\n",
        "# word2vec\n",
        "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuEAv-whdMCG"
      },
      "source": [
        "#### **Parte A (1 punto)** \n",
        "\n",
        "En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librer铆as ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n",
        "\n",
        "```python\n",
        "class WordContextMatrix:\n",
        "\n",
        "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Utilice el constructor para definir los parametros.\n",
        "    \"\"\"\n",
        "\n",
        "    # se sugiere agregar un una estructura de datos para guardar las\n",
        "    # palabras del vocab y para guardar el conteo de coocurrencia\n",
        "    ...\n",
        "    \n",
        "  def add_word_to_vocab(self, word):\n",
        "    \"\"\"\n",
        "    Utilice este m茅todo para agregar token\n",
        "    a sus vocabulario\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # Le puede ser 煤til considerar un token unk al vocab\n",
        "    # para palabras fuera del vocab\n",
        "    ...\n",
        "  \n",
        "  def build_matrix(self):\n",
        "    \"\"\"\n",
        "    Utilice este m茅todo para crear la palabra contexto\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def matrix2dict(self):\n",
        "    \"\"\"\n",
        "    Utilice este m茅todo para convertir la matriz a un diccionario de embeddings, donde las llaves deben ser los token del vocabulario y los embeddings los valores obtenidos de la matriz. \n",
        "    \"\"\"\n",
        "\n",
        "    # se recomienda transformar la matrix a un diccionario de embedding.\n",
        "    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n",
        "    ...\n",
        "\n",
        "```\n",
        "\n",
        "puede modificar los par谩metros o m茅todos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n",
        "\n",
        "```python\n",
        "corpus = [\n",
        "  \"I like deep learning.\",\n",
        "  \"I like NLP.\",\n",
        "  \"I enjoy flying.\"\n",
        "]\n",
        "```\n",
        "\n",
        "Obteniendo una matriz parecia a esta:\n",
        "\n",
        "***Resultado esperado***: \n",
        "\n",
        "| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n",
        "|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n",
        "| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n",
        "| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n",
        "| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n",
        "| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n",
        "| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n",
        "| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n",
        "| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n",
        "| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n",
        "\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur16vkyO37B5"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOI1FL8MlGZB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgmeSFqKLpFL"
      },
      "source": [
        "#### **Parte B (1.5 puntos)**\n",
        "\n",
        "En esta parte es debe entrenar Word2Vec de gensim y construir la matriz palabra contexto utilizando el dataset de di谩logos de los Simpson. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZgN06q4QPi3"
      },
      "source": [
        "Utilizando el dataset adjunto con la tarea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY3kmg4onnsu",
        "outputId": "d3525a54-0c10-401e-b3e2-9c6e9e714a2c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-05-05 17:58:59,568 : INFO : NumExpr defaulting to 2 threads.\n"
          ]
        }
      ],
      "source": [
        "data_file = \"dialogue-lines-of-the-simpsons.zip\"\n",
        "df = pd.read_csv(data_file)\n",
        "stopwords = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
        ").values\n",
        "stopwords = Counter(stopwords.flatten().tolist())\n",
        "df = df.dropna().reset_index(drop=True) # Quitar filas vacias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAg5a5bmWk3T"
      },
      "source": [
        "**Pregunta 1**: Ayud谩ndose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec. **(0.75 punto)** (Hint, le puede servir explorar un poco los datos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWw2fXFRXe5Y"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bvwplz7yTNcr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vBkF3hreGjg"
      },
      "source": [
        "**Pregunta 2**: Cree una matriz palabra contexto usando el mismo dataset. Configure el largo del vocabulario a 1000 o 2000 tokens, puede agregar valores mayores pero tenga en cuenta que la construcci贸n de la matriz puede tomar varios minutos. Puede que esto tarde un poco. **(0.75 punto)** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzLuH6MneWIY"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gPyW8fMeXNX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRCB-jqgTNcs"
      },
      "source": [
        "#### **Parte C (1.5 puntos): Aplicar embeddings para clasificar**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlqzlJRSTNcs"
      },
      "source": [
        "Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n",
        "\n",
        "Para esto ocuparemos el lexic贸n AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotaci贸n es positiva y un -1 si es negativa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMskFDmHTNcs"
      },
      "outputs": [],
      "source": [
        "AFINN = 'AFINN_full.csv'\n",
        "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaKl8hsCTNcs"
      },
      "source": [
        "Hint: Para w2v y la wcm son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendr谩n una representaci贸n en AFINN. Para el caso de la matriz palabra contexto se recomienda convertir su matrix a un diccionario. Pueden utilizar esta funci贸n auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWSSuctiTNcs"
      },
      "outputs": [],
      "source": [
        "def try_apply(model,word):\n",
        "    try:\n",
        "        aux = model[word]\n",
        "        return True\n",
        "    except KeyError:\n",
        "        #logger.error('Word {} not in dictionary'.format(word))\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrVPeEzgTNcs"
      },
      "source": [
        "**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representaci贸n en embedding que acabamos de calcular (con ambos modelos). \n",
        "\n",
        "Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n",
        "\n",
        "Para ambos modelos, separar train y test de acuerdo a la siguiente funci贸n. **(0.5 puntos)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Bkt26BwTNcs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmFoKWKO2EKA"
      },
      "source": [
        "```python\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.1, stratify=y)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDcq5czXTNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upAn_eT4TNct"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDKe4gA3TNct"
      },
      "source": [
        "**Pregunta 2**: Entrenar una regresi贸n log铆stica (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qu茅 se obtienen estos resultados? C贸mo los mejorar铆as? Como podr铆as mejorar los resultados de la matriz palabra contexto? es equivalente al modelo word2vec? **(1 punto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJMzq_dETNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bj1r_BnKn_7L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izppruGQTNct"
      },
      "source": [
        "# Bonus: +0.25 puntos en cualquier pregunta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW0aeK2KTNct"
      },
      "source": [
        "**Pregunta 1**: Replicar la parte anterior utilizando embeddings pre-entrenados en un dataset m谩s grande y obtener mejores resultados. Les puede servir [茅sta](https://radimrehurek.com/gensim/downloader.html#module-gensim.downloader) documentacion de gensim **(0.25 puntos)**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvHcVS3sTNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSc8p-T8TNcu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tarea_3_Word_Embeddings.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
