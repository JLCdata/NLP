{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckbt7VPDhBwb"
      },
      "source": [
        "# **Tarea 3 - Word Embeddings 📚**\n",
        "\n",
        "**Nombre: José Luis Cádiz Sejas**\n",
        "\n",
        "**Fecha límite de entrega 📆:** 10 de mayo.\n",
        "\n",
        "**Tiempo estimado de dedicación:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-19T18:30:18.109327Z",
          "start_time": "2020-03-19T18:30:18.103344Z"
        },
        "id": "q5CSRY4oNCHK"
      },
      "source": [
        "\n",
        "**Instrucciones:**\n",
        "- El ejercicio consiste en:\n",
        "    - Responder preguntas relativas a los contenidos vistos en los vídeos y slides de las clases. \n",
        "    - Entrenar Word2Vec y Word Context Matrix sobre un pequeño corpus.\n",
        "    - Evaluar los embeddings obtenidos en una tarea de clasificación.\n",
        "- La tarea se realiza en grupos de **máximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a través de u-cursos a más tardar el día estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo Jupyter Notebook.\n",
        "- Al momento de la revisión tu código será ejecutado. Por favor verifica que tu entrega no tenga errores de compilación. \n",
        "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a través del canal de Discord del curso. \n",
        "\n",
        "\n",
        "**Referencias**\n",
        "\n",
        "Vídeos: \n",
        "\n",
        "- [Linear Models](https://youtu.be/zhBxDsNLZEA)\n",
        "- [Neural Networks](https://youtu.be/oHZHA8h2xN0)\n",
        "- [Word Embeddings](https://youtu.be/wtwUsJMC9CA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4wYf0vgnbTv"
      },
      "source": [
        "## **Preguntas teóricas 📕 (2 puntos).** ##\n",
        "Para estas preguntas no es necesario implementar código, pero pueden utilizar pseudo código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5hUG6-8ngoK"
      },
      "source": [
        "### **Parte 1: Modelos Lineales (1 ptos)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yRvZbhsoi8f"
      },
      "source": [
        "Suponga que tiene un dataset de 10.000 documentos etiquetados por 4 categorías: política, deporte, negocios y otros. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irsqBVmCnx3M"
      },
      "source": [
        "**Pregunta 1**: Diseñe un modelo lineal capaz de clasificar un documento según estas categorías donde el output sea un vector con una distribución de probabilidad con la pertenencia a cada clase. \n",
        "\n",
        "Especifique: representación de los documentos de entrada, parámetros del modelo, transformaciones necesarias para obtener la probabilidad de cada etiqueta y función de pérdida escogida. **(0.5 puntos)**\n",
        "\n",
        "**Respuesta**: A continuación se enumeran los pasos a seguir para diseñar un modelo ilneal que clasifique documentos.\n",
        "1. **Representación**: Una representación básica podría ser One-Hot Vector, el cual consiste en representar cada oración del corpus como un vector sparse del tamaño del vacabulario, el cual tiene un 1 si la palabra aparece y 0 si no. Otra opción es Bag-of-Words (BOW), el cual también representa las oraciones como vectores sparse pero intenta capturar además, el orden de las palabras a traves de bigramas,trigramas etc. Esta representación vendrá dada por\n",
        "el vector $\\vec{x}$.\n",
        "\n",
        "2. **Modelo**: Consideremos un modelo lineal de la forma $\\vec{\\hat{y}}=f(x)=\\vec{x}\\cdot W+\\vec{b}$, donde $\\vec{x}$ es el input. $W$, $\\vec{b}$ son parámetros del modelo e $\\vec{\\hat{y}}$ es el Output de dimensión igual al número de etiquetas de la tarea de clasificación, en este caso dimensión 4.\n",
        "\n",
        "\n",
        "3. **Transformaciones del Ouput para obtener probabilidad de etiqueta**: Para obtener una representación probabilistica del Output del modelo lineal, dicho Ouput es pasado por una función $\\textit{Softmax}$ :\n",
        "\n",
        "$\\vec{\\hat{y}}=\\textit{Softmax}(\\vec{x}\\cdot W+\\vec{b})$, donde la predicción estará dada por:  $Prediction=\\hat{y}=argmax_{i}(\\vec{\\hat{y}}_{[i]})$, es decir, se asigna la etiqueta que tenga mayor probabilidad.\n",
        "\n",
        "\n",
        "4. **Entrenamiento**: para mejorar la calidad de las predicciones se deben ajustar los parámetros del modelo de modo de maximizar la capacidad predictiva del modelo. Para esto es de vital importancia definir una\n",
        "función de pérdida o también conocida como $\\textit{Loss}$. Para una predicción dada, La función de $\\textit{Loss}$, puede ser definida como\n",
        "$\\textit{Loss}$=$L(f(\\vec{x},\\vec{\\Theta}),y_{true})$=$L(\\hat{y},y_{true})$, donde $\\vec{\\Theta}$ representa los parámetros del modelo, $\\hat{y}$ la predicción e $y_{true}$ la etiqueta correcta. Para considerar todas las predicciones que se harían sobre el conjunto de entrenamiento se define la función de $\\textit{Loss}$ para todo el $\\textit{Corpus}$: $\\mathcal{L}(\\vec{\\Theta})=\\frac{1}{N}\\sum_{i=1}^{N}L(\\hat{y_{i}},y_{true_{i}})$. Una función de pérdida adecuada para la tarea de clasificación es la conocida $\\textit{Cross-Entropy-Loss}$. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5FaWqBVvL90"
      },
      "source": [
        "**Pregunta 2**: Explique cómo funciona el proceso de entrenamiento en este tipo de modelos y su evaluación. **(0.5 puntos)**\n",
        "\n",
        "**Respuesta**: Como se menciono anteriormente, para el proceso de entrenamiento se debe definir una función de pérdida a optimizar con el objetivo de obtener los parámetros $\\vec{\\Theta}$ que minimicen la función de pérdida. \n",
        "Matemáticamente estos parámetros estarán dados por la siguiente expresión:\n",
        "$$\\hat{\\Theta}=\\textit{Argmin}_{\\vec{\\Theta}}\\mathcal{L}(\\vec{\\Theta})=\\textit{Argmin}_{\\vec{\\Theta}}\\frac{1}{N}\\sum_{i=1}^{N}L(\\hat{y_{i}},y_{true_{i}})$$\n",
        "\n",
        "\n",
        "En este caso para la tarea que se busca cumplir una buena función de pérdida es  $\\textit{Cross-Entropy-Loss}$, la cual cuantifica la no similitud entre la etiqueta real y la etiqueta predicha, matemáticamente esta definida por la siguiente expresión:\n",
        "$$\\mathcal{L}_{cross-entropy}(\\vec{\\hat{y}},\\vec{y}_{true})=-\\sum_{i=1}^{N}\\vec{y}_{true_{i}}\\cdot\\log(\\vec{\\hat{y_{i}}})$$ \n",
        "\n",
        "El método de optimización utilizado estan basandos en los métodos basados en gradientes. Estos métodos son más económicos computacionalmente que derivar sobre todas las variables y calcular las diversas soluciones de cada derivada parcial particular. Un ejemplo básico de este tipo de métodos es el $\\textit{``Online Stochastic Gradient Descent''} $. \n",
        "\n",
        "Este método consiste en inicializar los parámetros del modelo con valores aleatorios y mientras no se cumpla un criterio de stop, como por ejemplo que la variación porcentual entre el $\\vec{\\Theta}$ anterior y el $\\vec{\\Theta}$ actualizado no sea lo suficiemente pequeño a un $\\vec{\\varepsilon}$, se iterará sobre cada registro del conjunto de entrenamiento, calculando la predicción que haría el modelo, luego se computa el error mediante la función de pérdida y la etiqueta verdadera, para luego calcular el gradiente respecto a los parámetros $\\vec{\\Theta}$. Luego $\\vec{\\Theta}$ es actualizado: $\\vec{\\Theta}=\\vec{\\Theta}-\\eta\\cdot\\nabla\\mathcal{L}$, donde $\\eta$ se conoce como la tasa de aprendizaje. Esta actualización continuará hasta que se cumpla el criterio de stop, con esto obteniendo los parámetros del modelo ya entrenado.\n",
        "\n",
        "Para evaluar el modelo, se debe contar con un conjunto de test, totalmente distinto al conjunto de entrenamiento, el cual debe contar también con las etiquetas verdaderas, de modo de hacer predicciones con el modelo ya entrenado, para luego calcular alguna métrica de clasificación como $\\textit{Accuracy, Precision, Recall, F1}$ etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkK7pc54njZq"
      },
      "source": [
        "### **Parte 2: Redes Neuronales (1 ptos)** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUbJjlj_9AFC"
      },
      "source": [
        "Supongamos que tenemos la siguiente red neuronal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obUfuOYB_TOC"
      },
      "source": [
        "![image.png](https://drive.google.com/uc?export=view&id=1fFTjtMvH6MY8o42_vj010y8eTuCVb5a3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2z-8zKW0_6q"
      },
      "source": [
        "**Pregunta 1**: En clases les explicaron como se puede representar una red neuronal de una y dos capas de manera matemática. Dada la red neuronal anterior, defina la salida $\\vec{\\hat{y}}$ en función del vector $\\vec{x}$, pesos $W^i$, bias $b^i$ y funciones $g,f,h$. \n",
        "\n",
        "Adicionalmente liste y explicite las dimensiones de cada matriz y vector involucrado en la red neuronal. **(0.5 Puntos)**\n",
        "\n",
        "**Respuesta**: \n",
        "\n",
        "Formula:\n",
        "$\\vec{\\hat{y}} = NN_{MLP3}(\\vec{x}) =\\vec{h}^{3}\\cdot\\vec{W}^{4}$, $\\vec{W}^{4}$ dim (1,4).\n",
        "\n",
        "donde:\n",
        "\n",
        "* $\\vec{h}^{3}=h(\\vec{h}^{2}\\cdot\\vec{W}^{3}+\\vec{b}^{3})$, donde $\\vec{W}^{3}$ dim (3,1), $\\vec{b}^{3}$ dim (1,1).\n",
        "* $\\vec{h}^{2}=f(\\vec{h}^{1}\\cdot\\vec{W}^{2}+\\vec{b}^{2})$, donde $\\vec{W}^{2}$ dim (2,3), $\\vec{b}^{2}$ dim (1,3).\n",
        "* $\\vec{h}^{1}=g(\\vec{x}\\cdot\\vec{W}^{1}+\\vec{b}^{1})$, donde $\\vec{W}^{1}$ dim (3,2), $\\vec{b}^{1}$ dim (1,2), $\\vec{x}$ dim (1,3).\n",
        "\n",
        "**Pregunta 2**: Explique qué es backpropagation. ¿Cuales serían los parámetros a evaluar en la red neuronal anterior durante backpropagation? **(0.25 puntos)**\n",
        "\n",
        "**Respuesta**: Los parámetros a optimizar dentro de una red neuronal son los pesos sinapticos asociados a las conexiones entre una capa y otra, junto con sus respectivos bias de cada capa. Para optimizar los parámetros de la red neuronal se utilizan métodos del descenso del gradiente, pero el problema es que las redes neuronales se caracterizan por tener una gran cantidad de parámetros, lo que en la practica hace muy costoso cumputacionalmente calcular todas las derivadas. Por esto es necesario calcular estas derivadas de una manera más eficiente. \n",
        "\n",
        "Backpropagation es un técnica que permite obtener las derivadas parciales de una función $\\textit{Loss}$ respecto a todos los parámetros de un modelo de una manera más eficiente guardando las derivadas que se repiten. El gradiente se calcula de forma recursiva, es decir se calculan las derivadas parciales desde las capas más profundas hasta las más superficiales, de modo de ir guardando las derivadas profundas que luego son utilizadas en las derivadas parciales de las capas menos profundas producto de la regla de la cadena. Esto sucede porque se entiende que el error imputado de un parámetro depende de los errores imputados de los parámetros de las capas más profundas, es decir el error imputado de cada parámetro se propaga de adelante hacia atras ($\\textit{Back-Propagation}$). Con esto finalmente, se logra reducir el costo cumputacional al no recalcular derivadas. \n",
        "\n",
        "\n",
        "**Pregunta 3**: Explique los pasos de backpropagation. En la red neuronal anterior: Cuales son las derivadas que debemos calcular para poder obtener $\\vec{\\delta^l_{[j]}}$ en todas las capas? **(0.25 puntos)**\n",
        "\n",
        "**Respuesta**: La idea es calcular el gradiente de manera eficiente, i.e. calcular las derivadas parciales de los parámetros de las capas superiores secuencialmente hasta las capas  inferiores. Del tal modo de aprovechar la dependencia de las derivadas parciales de las capas inferiores respecto a las superiores, reciclando ciertos calculos. Esto en computación se llama programación dinámica.\n",
        "\n",
        "Pasos:\n",
        "\n",
        "1. Se inicializa la red con valores aleatorios para todos sus pesos.\n",
        "2. $\\textbf{Fordward propagation}$ - (Evaluar ejemplos): Para cada ejemplo de los datos de entrenamiento se alimenta la red neuronal, se calculan las funciones de activación desde las capas inferiores hasta las superiores de forma recursiva. según la ecuación:\n",
        "$$h^{l}_{j}=\\left (\\sum_{i}W_{i,j}^{l}\\cdot z_{i}^{l-1}\\right )+b_{j}^{l}\\hspace{0.1cm}donde\\hspace{0.1cm} z_{j}^{l}=g\\left(h_{j}^{l}\\right), z_{j}^{0}=x_{j}$$\n",
        "\n",
        "3. Se obtiene la salida de la red y se evalua la $\\textit{Loss}$, $\\mathcal{L}(\\vec{\\Theta},\\vec{\\hat{y}},\\vec{y}_{true})$, obteniendo una $\\textit{Loss}$ de la forma  $\\mathcal{L}(\\vec{\\Theta})$.\n",
        "4. $\\textbf{Back propagation}$: Calcular el gradiente de $\\mathcal{L}(\\vec{\\Theta})$ mediante el calculo de las derivadas parciales de los parámetros desde las capas superiores hasta las inferiores, guardando las derivadas que se van obteniendo. La derivada de un parámetro $W^{l}_{i,j}$ esta definida por la siguiente expresión:\n",
        "$$\\frac{\\partial\\mathcal{L}}{\\partial W^{l}_{i,j}}=\\delta^{l}_{j}\\cdot z^{l-1}_{i}$$\n",
        "\n",
        "donde $\\delta^{l}_{j}=\\frac{\\partial\\mathcal{L}}{\\partial h^{l}_{j}}=\\sum_{k}\\left( \\delta^{l+1}_{k} \\cdot W^{l+1}_{j,k}\\cdot g'\\left(h^{l}_{j}\\right)    \\right)$, de este modo se deben  obtener los deltas de las capas superiores para obtener los deltas de las capas inferiores. Dicho de otra forma, se deben obtener las derivadas parciales del $\\textit{Loss}$ respecto a la salida de cada neurona desde las capas superiores hasta las capas inferiores. En este caso particular de la capa 3, 2 y 1 de forma secuencial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocS_vQhR1gcU"
      },
      "source": [
        "## **Preguntas prácticas 💻 (4 puntos).** ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol82nJ0FnmcP"
      },
      "source": [
        "### **Parte 3: Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Daw7Ee5cdQTb"
      },
      "source": [
        "En la auxiliar 2 se nombraron dos formas de crear word vectors:\n",
        "\n",
        "-  Distributional Vectors.\n",
        "-  Distributed Vectors.\n",
        "\n",
        "El objetivo de esta parte es comparar las dos embeddings obtenidos de estas dos estrategias en una tarea de clasificación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "E2G1qcb7AJqW"
      },
      "outputs": [],
      "source": [
        "import re  \n",
        "import pandas as pd \n",
        "from time import time  \n",
        "from collections import defaultdict \n",
        "import string \n",
        "import multiprocessing\n",
        "import os\n",
        "import gensim\n",
        "import sklearn\n",
        "from sklearn import linear_model\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import scipy\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, cohen_kappa_score, classification_report\n",
        "\n",
        "# word2vec\n",
        "from gensim.models import Word2Vec, KeyedVectors, FastText\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "\n",
        "# Clasificador\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AuEAv-whdMCG"
      },
      "source": [
        "#### **Parte A (1 punto)** \n",
        "\n",
        "En esta parte debe crear una matriz palabra contexto, para esto, complete el siguiente template (para esta parte puede utilizar las librerías ```numpy``` y/o ```scipy```). Hint: revise como utilizar matrices sparse de ```scipy```\n",
        "\n",
        "```python\n",
        "class WordContextMatrix:\n",
        "\n",
        "  def __init__(self, vocab_size, window_size, dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Utilice el constructor para definir los parametros.\n",
        "    \"\"\"\n",
        "\n",
        "    # se sugiere agregar un una estructura de datos para guardar las\n",
        "    # palabras del vocab y para guardar el conteo de coocurrencia\n",
        "    ...\n",
        "    \n",
        "  def add_word_to_vocab(self, word):\n",
        "    \"\"\"\n",
        "    Utilice este método para agregar token\n",
        "    a sus vocabulario\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    # Le puede ser útil considerar un token unk al vocab\n",
        "    # para palabras fuera del vocab\n",
        "    ...\n",
        "  \n",
        "  def build_matrix(self):\n",
        "    \"\"\"\n",
        "    Utilice este método para crear la palabra contexto\n",
        "    \"\"\"\n",
        "    ...\n",
        "\n",
        "  def matrix2dict(self):\n",
        "    \"\"\"\n",
        "    Utilice este método para convertir la matriz a un diccionario de embeddings, donde las llaves deben ser los token del vocabulario y los embeddings los valores obtenidos de la matriz. \n",
        "    \"\"\"\n",
        "\n",
        "    # se recomienda transformar la matrix a un diccionario de embedding.\n",
        "    # por ejemplo {palabra1:vec1, palabra2:vec2, ...}\n",
        "    ...\n",
        "\n",
        "```\n",
        "\n",
        "puede modificar los parámetros o métodos si lo considera necesario. Para probar la matrix puede utilizar el siguiente corpus.\n",
        "\n",
        "```python\n",
        "corpus = [\n",
        "  \"I like deep learning.\",\n",
        "  \"I like NLP.\",\n",
        "  \"I enjoy flying.\"\n",
        "]\n",
        "```\n",
        "\n",
        "Obteniendo una matriz parecia a esta:\n",
        "\n",
        "***Resultado esperado***: \n",
        "\n",
        "| counts   | I  | like | enjoy | deep | learning | NLP | flying | . |   \n",
        "|----------|---:|-----:|------:|-----:|---------:|----:|-------:|--:|\n",
        "| I        | 0  |  2   |  1    |    0 |  0       |   0 | 0      | 0|            \n",
        "| like     |  2 |    0 |  0    |    1 |  0       |   1 | 0      | 0 | \n",
        "| enjoy    |  1 |    0 |  0    |    0 |  0       |   0 | 1      | 0 |\n",
        "| deep     |  0 |    1 |  0    |    0 |  1       |   0 | 0      | 0 |  \n",
        "| learning |  0 |    0 |  0    |    1 |  0       |   0 | 0      | 1 |          \n",
        "| NLP      |  0 |    1 |  0    |    0 |  0       |   0 | 0      | 1 |\n",
        "| flying   |  0 |    0 |  1    |    0 |  0       |   0 | 0      | 1 | \n",
        "| .        |  0 |    0 |  0    |    0 |  1       |   1 | 1      | 0 | \n",
        "\n",
        "``"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur16vkyO37B5"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SYjRXLGYLqub"
      },
      "outputs": [],
      "source": [
        "class WordContextMatrix:\n",
        "\n",
        "  def __init__(self,vocab_size,window_size, dataset, tokenizer):\n",
        "    \"\"\"\n",
        "    Utilice el constructor para definir los parametros.\n",
        "    \"\"\"\n",
        "    #self.stopwords=stopwords\n",
        "    self.vocab_size=vocab_size\n",
        "    self.k=window_size # Tamaño de la ventana\n",
        "    self.dataset=dataset # Dataset\n",
        "    self.tokenizer=tokenizer # Tokenizer\n",
        "    self.vocabulario=[] # Palabras del vocab\n",
        "\n",
        "    for i in dataset:\n",
        "        self.vocabulario=self.vocabulario+tokenizer(i)\n",
        "\n",
        "        #self.vocabulario=self.vocabulario+[token for token in tokenizer(i) if token.lower() not in self.stopwords]\n",
        "\n",
        "        self.vocabulario=list(set(self.vocabulario)) # Palabras del vocab actualizado\n",
        "\n",
        "        if len(self.vocabulario) >=vocab_size: # Si se supera vocab_size entonces break y se ajusta el vocabulario a vocab_size\n",
        "          self.vocabulario=self.vocabulario[:vocab_size]\n",
        "          break\n",
        "\n",
        "\n",
        "\n",
        "  def add_word_to_vocab(self, word):\n",
        "    \"\"\"\n",
        "    Utilice este método para agregar token\n",
        "    a sus vocabulario\n",
        "    \"\"\"\n",
        "    self.vocabulario=self.vocabulario+[word] # Se agrega un elemento nuevo a la lista de vocabulario\n",
        "\n",
        "    self.vocabulario=list(set(self.vocabulario)) # Se eliminan valores repetidos si es que se llega a agregar una palabra que ya existe\n",
        "\n",
        "  \n",
        "  def build_matrix(self):\n",
        "    \"\"\"\n",
        "    Utilice este método para crear la palabra contexto\n",
        "    \"\"\"\n",
        "    dic={}\n",
        "\n",
        "    for i in self.vocabulario+['unk']:\n",
        "        \n",
        "        dic[i]={}\n",
        "\n",
        "        for j in self.vocabulario+['unk']:\n",
        "\n",
        "          dic[i][j]=0    \n",
        "\n",
        "    for i in self.vocabulario:\n",
        "        \n",
        "        \n",
        "        for j in self.dataset:\n",
        "\n",
        "            sentence_split=self.tokenizer(j)\n",
        "\n",
        "            if i in sentence_split:\n",
        "                \n",
        "                indice=sentence_split.index(i) # Ubicación de la palabra del vocabulario en la oración a inspeccionar\n",
        "\n",
        "                # Lado izquierdo\n",
        "                izquierdos=sentence_split[max(indice-self.k,0):indice] # Palabras a la izquierda\n",
        "                \n",
        "                # Lado derecho\n",
        "                derechos=sentence_split[indice+1:min(indice+self.k+1,len(sentence_split)+1)] # Palabras a la derecha\n",
        "                \n",
        "                # Entorno\n",
        "                context=izquierdos+derechos\n",
        "                \n",
        "            \n",
        "                for l in context:\n",
        "                    if l in self.vocabulario:\n",
        "                      dic[i][l]+=1\n",
        "                    else:\n",
        "                      dic[i]['unk']+=1\n",
        "\n",
        "                  \n",
        "\n",
        "              \n",
        "    self.word_context_matrix=dic\n",
        "  \n",
        "    \n",
        "\n",
        "  def matrix2dict(self):\n",
        "    \"\"\"\n",
        "    Utilice este método para convertir la matriz a un diccionario de embeddings, donde las llaves deben ser los token del vocabulario y los embeddings los valores obtenidos de la matriz. \n",
        "    \"\"\"\n",
        "    self.dic_embeddings=pd.DataFrame(self.word_context_matrix).to_dict('list')\n",
        "    print(f'Representaciones: {self.dic_embeddings.keys()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwcrfIw5Lquc"
      },
      "source": [
        "### Prueba de la programación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "CQBNFrqTLquc",
        "outputId": "fae9aed4-1d4a-4e74-eace-e0bac1ff1c4c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-472ab0b0-7253-4586-bf36-b42cd598954e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>I</th>\n",
              "      <th>like</th>\n",
              "      <th>enjoy</th>\n",
              "      <th>deep</th>\n",
              "      <th>learning</th>\n",
              "      <th>NLP</th>\n",
              "      <th>flying</th>\n",
              "      <th>.</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>I</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>like</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enjoy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>deep</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learning</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NLP</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flying</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>.</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-472ab0b0-7253-4586-bf36-b42cd598954e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-472ab0b0-7253-4586-bf36-b42cd598954e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-472ab0b0-7253-4586-bf36-b42cd598954e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          I  like  enjoy  deep  learning  NLP  flying  .\n",
              "I         0     2      1     0         0    0       0  0\n",
              "like      2     0      0     1         0    1       0  0\n",
              "enjoy     1     0      0     0         0    0       1  0\n",
              "deep      0     1      0     0         1    0       0  0\n",
              "learning  0     0      0     1         0    0       0  1\n",
              "NLP       0     1      0     0         0    0       0  1\n",
              "flying    0     0      1     0         0    0       0  1\n",
              ".         0     0      0     0         1    1       1  0"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Representaciones: dict_keys(['.', 'NLP', 'like', 'enjoy', 'I', 'deep', 'learning', 'flying', 'unk'])\n",
            "{'.': [0, 1, 0, 0, 0, 0, 1, 1, 0], 'NLP': [1, 0, 1, 0, 0, 0, 0, 0, 0], 'like': [0, 1, 0, 0, 2, 1, 0, 0, 0], 'enjoy': [0, 0, 0, 0, 1, 0, 0, 1, 0], 'I': [0, 0, 2, 1, 0, 0, 0, 0, 0], 'deep': [0, 0, 1, 0, 0, 0, 1, 0, 0], 'learning': [1, 0, 0, 0, 0, 1, 0, 0, 0], 'flying': [1, 0, 0, 1, 0, 0, 0, 0, 0], 'unk': [0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ],
      "source": [
        "# Import tokenizador\n",
        "from nltk.tokenize import wordpunct_tokenize \n",
        "\n",
        "# Dataset\n",
        "corpus = [\n",
        "  \"I like deep learning.\",\n",
        "  \"I like NLP.\",\n",
        "  \"I enjoy flying.\"\n",
        "]\n",
        "\n",
        "# Se inicializa la clase\n",
        "test=WordContextMatrix(8,1,corpus,wordpunct_tokenize)\n",
        "\n",
        "# Se construye la matriz\n",
        "test.build_matrix()\n",
        "\n",
        "# Prueba matrix\n",
        "display(pd.DataFrame(test.word_context_matrix)[[\"I\",\"like\",\"enjoy\",\"deep\",\"learning\",\"NLP\",\"flying\",\".\"]].loc[[\"I\",\"like\",\"enjoy\",\"deep\",\"learning\",\"NLP\",\"flying\",\".\"]])\n",
        "\n",
        "# Se contruye diccionario de embeddings\n",
        "test.matrix2dict()\n",
        "\n",
        "# Prueba representaciones\n",
        "print(test.dic_embeddings) \n",
        "\n",
        "# El orden de las representaciones no es el mismo que la matriz de contextos, sin embargo si el orden de las representaciones es el mismo para todas las palabras, se obtendrán resultados consistentes!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NUmqUJhLqud"
      },
      "source": [
        "---------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebJ1estiLquf"
      },
      "source": [
        "---------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgmeSFqKLpFL"
      },
      "source": [
        "#### **Parte B (1.5 puntos)**\n",
        "\n",
        "En esta parte es debe entrenar Word2Vec de gensim y construir la matriz palabra contexto utilizando el dataset de diálogos de los Simpson. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZgN06q4QPi3"
      },
      "source": [
        "Utilizando el dataset adjunto con la tarea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "eY3kmg4onnsu"
      },
      "outputs": [],
      "source": [
        "data_file = \"dialogue-lines-of-the-simpsons.zip\"\n",
        "df = pd.read_csv(data_file)\n",
        "stopwords = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt'\n",
        ").values\n",
        "stopwords = Counter(stopwords.flatten().tolist())\n",
        "df = df.dropna().reset_index(drop=True) # Quitar filas vacias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zx6u37-9Mc5U",
        "outputId": "3c5d43bf-0103-41e9-fb2a-969e503f05a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~«»“”‘’…—\n"
          ]
        }
      ],
      "source": [
        "# limpiar puntuaciones y separar por tokens.\n",
        "from collections import Counter\n",
        "punctuation = string.punctuation + \"«»“”‘’…—\"\n",
        "stopwords = pd.read_csv(\n",
        "    'https://raw.githubusercontent.com/Alir3z4/stop-words/master/spanish.txt'\n",
        ").values\n",
        "stopwords = Counter(stopwords.flatten().tolist())\n",
        "\n",
        "\n",
        "def simple_tokenizer(doc, lower=False):\n",
        "    if lower:\n",
        "        tokenized_doc = doc.translate(str.maketrans(\n",
        "            '', '', punctuation)).lower().split()\n",
        "\n",
        "    tokenized_doc = doc.translate(str.maketrans('', '', punctuation)).split()\n",
        "\n",
        "    tokenized_doc = [\n",
        "        token for token in tokenized_doc if token.lower() not in stopwords\n",
        "    ]\n",
        "    return tokenized_doc\n",
        "\n",
        "\n",
        "print(punctuation)\n",
        "cleaned_content = [simple_tokenizer(doc) for doc in df[\"spoken_words\"].values]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpvCsndpMeqG",
        "outputId": "fe5ae615-fbae-4ef5-bc37-d70e51a0ccd1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-17 23:00:17,901 : INFO : collecting all words and their counts\n",
            "2022-05-17 23:00:17,904 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
            "2022-05-17 23:00:18,087 : INFO : PROGRESS: at sentence #5000, processed 47935 words and 35491 word types\n",
            "2022-05-17 23:00:18,292 : INFO : PROGRESS: at sentence #10000, processed 94410 words and 61831 word types\n",
            "2022-05-17 23:00:18,471 : INFO : PROGRESS: at sentence #15000, processed 138493 words and 84121 word types\n",
            "2022-05-17 23:00:18,669 : INFO : PROGRESS: at sentence #20000, processed 190912 words and 108466 word types\n",
            "2022-05-17 23:00:18,870 : INFO : PROGRESS: at sentence #25000, processed 242380 words and 131940 word types\n",
            "2022-05-17 23:00:18,988 : INFO : PROGRESS: at sentence #30000, processed 296955 words and 155498 word types\n",
            "2022-05-17 23:00:19,109 : INFO : PROGRESS: at sentence #35000, processed 348440 words and 175933 word types\n",
            "2022-05-17 23:00:19,209 : INFO : PROGRESS: at sentence #40000, processed 394461 words and 193510 word types\n",
            "2022-05-17 23:00:19,313 : INFO : PROGRESS: at sentence #45000, processed 439861 words and 210918 word types\n",
            "2022-05-17 23:00:19,412 : INFO : PROGRESS: at sentence #50000, processed 484215 words and 227811 word types\n",
            "2022-05-17 23:00:19,509 : INFO : PROGRESS: at sentence #55000, processed 527392 words and 243677 word types\n",
            "2022-05-17 23:00:19,601 : INFO : PROGRESS: at sentence #60000, processed 568064 words and 257975 word types\n",
            "2022-05-17 23:00:19,702 : INFO : PROGRESS: at sentence #65000, processed 612140 words and 273272 word types\n",
            "2022-05-17 23:00:19,808 : INFO : PROGRESS: at sentence #70000, processed 662050 words and 291479 word types\n",
            "2022-05-17 23:00:19,920 : INFO : PROGRESS: at sentence #75000, processed 711665 words and 308661 word types\n",
            "2022-05-17 23:00:20,033 : INFO : PROGRESS: at sentence #80000, processed 761569 words and 325353 word types\n",
            "2022-05-17 23:00:20,141 : INFO : PROGRESS: at sentence #85000, processed 811161 words and 341714 word types\n",
            "2022-05-17 23:00:20,262 : INFO : PROGRESS: at sentence #90000, processed 858907 words and 357394 word types\n",
            "2022-05-17 23:00:20,379 : INFO : PROGRESS: at sentence #95000, processed 908546 words and 372846 word types\n",
            "2022-05-17 23:00:20,487 : INFO : PROGRESS: at sentence #100000, processed 957136 words and 388543 word types\n",
            "2022-05-17 23:00:20,593 : INFO : PROGRESS: at sentence #105000, processed 1007005 words and 404052 word types\n",
            "2022-05-17 23:00:20,714 : INFO : PROGRESS: at sentence #110000, processed 1057500 words and 419942 word types\n",
            "2022-05-17 23:00:20,833 : INFO : PROGRESS: at sentence #115000, processed 1105932 words and 434529 word types\n",
            "2022-05-17 23:00:20,940 : INFO : PROGRESS: at sentence #120000, processed 1155194 words and 449374 word types\n",
            "2022-05-17 23:00:21,049 : INFO : PROGRESS: at sentence #125000, processed 1203797 words and 463038 word types\n",
            "2022-05-17 23:00:21,156 : INFO : PROGRESS: at sentence #130000, processed 1252051 words and 475515 word types\n",
            "2022-05-17 23:00:21,198 : INFO : collected 480129 word types from a corpus of 1270022 words (unigram + bigrams) and 131853 sentences\n",
            "2022-05-17 23:00:21,200 : INFO : using 480129 counts as vocab in Phrases<0 vocab, min_count=100, threshold=10.0, max_vocab_size=40000000>\n",
            "2022-05-17 23:00:21,201 : INFO : source_vocab length 480129\n",
            "2022-05-17 23:00:26,158 : INFO : Phraser built with 65 phrasegrams\n"
          ]
        }
      ],
      "source": [
        "# Generación de representaciones para palabras compuestas, para que sean considerados deberan aparecer un minimo de 100 veces\n",
        "\n",
        "phrases = Phrases(cleaned_content, min_count=100, progress_per=5000) \n",
        "bigram = Phraser(phrases)\n",
        "sentences = bigram[cleaned_content]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uF7JUA3-QNhn",
        "outputId": "b2ce805c-fdfd-41d1-91ce-b327f856d025"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-17 23:01:23,507 : INFO : source_vocab length 480129\n",
            "2022-05-17 23:01:28,814 : INFO : Phraser built with 65 phrasegrams\n"
          ]
        }
      ],
      "source": [
        "# Re-tokenizamos el corpus con los bigramas encontrados\n",
        "bigram = Phraser(phrases)\n",
        "sentences = bigram[cleaned_content]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NWY5a8xQX2G",
        "outputId": "06bd32a9-6198-47a5-f07b-dbaed2fdf004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Oh', 'when', 'youre', 'older', 'youll', 'know', 'better']\n"
          ]
        }
      ],
      "source": [
        "print(sentences[110])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAg5a5bmWk3T"
      },
      "source": [
        "**Pregunta 1**: Ayudándose de los pasos vistos en la auxiliar, entrene los modelos Word2Vec. **(0.75 punto)** (Hint, le puede servir explorar un poco los datos)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWw2fXFRXe5Y"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "xkpRgXgdMnhT"
      },
      "outputs": [],
      "source": [
        "# Definimos el modelo\n",
        "biobio_w2v = Word2Vec(min_count=10,\n",
        "                      window=4,\n",
        "                      size=200,\n",
        "                      sample=6e-5,\n",
        "                      alpha=0.03,\n",
        "                      min_alpha=0.0007,\n",
        "                      negative=20,\n",
        "                      workers=multiprocessing.cpu_count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It1iDpTfMofZ",
        "outputId": "eb17a918-1df7-476e-b198-f72f60861603"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-17 23:02:23,105 : INFO : collecting all words and their counts\n",
            "2022-05-17 23:02:23,109 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2022-05-17 23:02:23,478 : INFO : PROGRESS: at sentence #10000, processed 92724 words, keeping 12048 word types\n",
            "2022-05-17 23:02:23,825 : INFO : PROGRESS: at sentence #20000, processed 187453 words, keeping 18749 word types\n",
            "2022-05-17 23:02:24,208 : INFO : PROGRESS: at sentence #30000, processed 291647 words, keeping 24790 word types\n",
            "2022-05-17 23:02:24,586 : INFO : PROGRESS: at sentence #40000, processed 387455 words, keeping 29046 word types\n",
            "2022-05-17 23:02:24,917 : INFO : PROGRESS: at sentence #50000, processed 475583 words, keeping 33015 word types\n",
            "2022-05-17 23:02:25,232 : INFO : PROGRESS: at sentence #60000, processed 557919 words, keeping 36446 word types\n",
            "2022-05-17 23:02:25,605 : INFO : PROGRESS: at sentence #70000, processed 650303 words, keeping 40148 word types\n",
            "2022-05-17 23:02:25,998 : INFO : PROGRESS: at sentence #80000, processed 748239 words, keeping 43782 word types\n",
            "2022-05-17 23:02:26,361 : INFO : PROGRESS: at sentence #90000, processed 844112 words, keeping 47070 word types\n",
            "2022-05-17 23:02:26,725 : INFO : PROGRESS: at sentence #100000, processed 940821 words, keeping 50083 word types\n",
            "2022-05-17 23:02:27,087 : INFO : PROGRESS: at sentence #110000, processed 1039661 words, keeping 53396 word types\n",
            "2022-05-17 23:02:27,463 : INFO : PROGRESS: at sentence #120000, processed 1135869 words, keeping 56239 word types\n",
            "2022-05-17 23:02:27,812 : INFO : PROGRESS: at sentence #130000, processed 1230917 words, keeping 58387 word types\n",
            "2022-05-17 23:02:27,881 : INFO : collected 58742 word types from a corpus of 1248531 raw words and 131853 sentences\n",
            "2022-05-17 23:02:27,883 : INFO : Loading a fresh vocabulary\n",
            "2022-05-17 23:02:27,941 : INFO : effective_min_count=10 retains 7847 unique words (13% of original 58742, drops 50895)\n",
            "2022-05-17 23:02:27,943 : INFO : effective_min_count=10 leaves 1140695 word corpus (91% of original 1248531, drops 107836)\n",
            "2022-05-17 23:02:27,975 : INFO : deleting the raw counts dictionary of 58742 items\n",
            "2022-05-17 23:02:27,978 : INFO : sample=6e-05 downsamples 798 most-common words\n",
            "2022-05-17 23:02:27,984 : INFO : downsampling leaves estimated 476596 word corpus (41.8% of prior 1140695)\n",
            "2022-05-17 23:02:28,004 : INFO : estimated required memory for 7847 words and 200 dimensions: 16478700 bytes\n",
            "2022-05-17 23:02:28,012 : INFO : resetting layer weights\n"
          ]
        }
      ],
      "source": [
        "# Se contruye el vocabulario\n",
        "biobio_w2v.build_vocab(sentences, progress_per=10000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "JvWWJqniM48L"
      },
      "outputs": [],
      "source": [
        "vocab=biobio_w2v.wv.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZ262sXiNEms",
        "outputId": "a1002c13-ce6e-4d4b-e853-0d27d76ebd5f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-17 23:03:49,995 : INFO : training model with 2 workers on 7847 vocabulary and 200 features, using sg=0 hs=0 sample=6e-05 negative=20 window=4\n",
            "2022-05-17 23:03:51,050 : INFO : EPOCH 1 - PROGRESS: at 15.01% examples, 93978 words/s, in_qsize 3, out_qsize 0\n",
            "2022-05-17 23:03:56,850 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:03:56,868 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:03:56,870 : INFO : EPOCH - 1 : training on 6982745 raw words (615209 effective words) took 6.8s, 90053 effective words/s\n",
            "2022-05-17 23:03:57,921 : INFO : EPOCH 2 - PROGRESS: at 15.41% examples, 95907 words/s, in_qsize 4, out_qsize 0\n",
            "2022-05-17 23:04:01,797 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:01,799 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:01,804 : INFO : EPOCH - 2 : training on 6982745 raw words (614814 effective words) took 4.9s, 125526 effective words/s\n",
            "2022-05-17 23:04:02,813 : INFO : EPOCH 3 - PROGRESS: at 39.77% examples, 245981 words/s, in_qsize 3, out_qsize 0\n",
            "2022-05-17 23:04:04,326 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:04,329 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:04,330 : INFO : EPOCH - 3 : training on 6982745 raw words (614700 effective words) took 2.5s, 244211 effective words/s\n",
            "2022-05-17 23:04:05,342 : INFO : EPOCH 4 - PROGRESS: at 29.14% examples, 186057 words/s, in_qsize 3, out_qsize 0\n",
            "2022-05-17 23:04:08,606 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:08,608 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:08,615 : INFO : EPOCH - 4 : training on 6982745 raw words (615054 effective words) took 4.3s, 143903 effective words/s\n",
            "2022-05-17 23:04:09,642 : INFO : EPOCH 5 - PROGRESS: at 19.74% examples, 124812 words/s, in_qsize 3, out_qsize 0\n",
            "2022-05-17 23:04:11,917 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:11,922 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:11,923 : INFO : EPOCH - 5 : training on 6982745 raw words (614870 effective words) took 3.3s, 186931 effective words/s\n",
            "2022-05-17 23:04:12,937 : INFO : EPOCH 6 - PROGRESS: at 38.87% examples, 240213 words/s, in_qsize 3, out_qsize 1\n",
            "2022-05-17 23:04:14,421 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:14,422 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:14,429 : INFO : EPOCH - 6 : training on 6982745 raw words (615169 effective words) took 2.5s, 246567 effective words/s\n",
            "2022-05-17 23:04:15,438 : INFO : EPOCH 7 - PROGRESS: at 39.93% examples, 246428 words/s, in_qsize 3, out_qsize 0\n",
            "2022-05-17 23:04:17,120 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:17,126 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:17,129 : INFO : EPOCH - 7 : training on 6982745 raw words (614978 effective words) took 2.7s, 228414 effective words/s\n",
            "2022-05-17 23:04:18,149 : INFO : EPOCH 8 - PROGRESS: at 38.70% examples, 237517 words/s, in_qsize 4, out_qsize 0\n",
            "2022-05-17 23:04:19,876 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:19,881 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:19,884 : INFO : EPOCH - 8 : training on 6982745 raw words (614801 effective words) took 2.7s, 223967 effective words/s\n",
            "2022-05-17 23:04:20,899 : INFO : EPOCH 9 - PROGRESS: at 36.08% examples, 223881 words/s, in_qsize 3, out_qsize 1\n",
            "2022-05-17 23:04:22,823 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:22,826 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:22,830 : INFO : EPOCH - 9 : training on 6982745 raw words (614877 effective words) took 2.9s, 209466 effective words/s\n",
            "2022-05-17 23:04:23,844 : INFO : EPOCH 10 - PROGRESS: at 26.13% examples, 167721 words/s, in_qsize 3, out_qsize 0\n",
            "2022-05-17 23:04:27,290 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:27,293 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:27,302 : INFO : EPOCH - 10 : training on 6982745 raw words (614956 effective words) took 4.5s, 137780 effective words/s\n",
            "2022-05-17 23:04:28,310 : INFO : EPOCH 11 - PROGRESS: at 40.09% examples, 247375 words/s, in_qsize 3, out_qsize 0\n",
            "2022-05-17 23:04:29,796 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:29,805 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:29,807 : INFO : EPOCH - 11 : training on 6982745 raw words (614601 effective words) took 2.5s, 246057 effective words/s\n",
            "2022-05-17 23:04:30,819 : INFO : EPOCH 12 - PROGRESS: at 40.92% examples, 251716 words/s, in_qsize 3, out_qsize 0\n",
            "2022-05-17 23:04:32,248 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:32,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:32,256 : INFO : EPOCH - 12 : training on 6982745 raw words (614753 effective words) took 2.4s, 252110 effective words/s\n",
            "2022-05-17 23:04:33,265 : INFO : EPOCH 13 - PROGRESS: at 40.92% examples, 252005 words/s, in_qsize 4, out_qsize 0\n",
            "2022-05-17 23:04:35,168 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:35,170 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:35,177 : INFO : EPOCH - 13 : training on 6982745 raw words (614985 effective words) took 2.9s, 211082 effective words/s\n",
            "2022-05-17 23:04:36,191 : INFO : EPOCH 14 - PROGRESS: at 18.75% examples, 118495 words/s, in_qsize 3, out_qsize 0\n",
            "2022-05-17 23:04:39,556 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:39,563 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:39,566 : INFO : EPOCH - 14 : training on 6982745 raw words (615017 effective words) took 4.4s, 140540 effective words/s\n",
            "2022-05-17 23:04:40,576 : INFO : EPOCH 15 - PROGRESS: at 34.38% examples, 214869 words/s, in_qsize 3, out_qsize 0\n",
            "2022-05-17 23:04:42,184 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2022-05-17 23:04:42,191 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2022-05-17 23:04:42,193 : INFO : EPOCH - 15 : training on 6982745 raw words (615014 effective words) took 2.6s, 234857 effective words/s\n",
            "2022-05-17 23:04:42,196 : INFO : training on a 104741175 raw words (9223798 effective words) took 52.2s, 176710 effective words/s\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time to train the model: 0.87 mins\n"
          ]
        }
      ],
      "source": [
        "# train del modelo\n",
        "t = time()\n",
        "biobio_w2v.train(df[\"spoken_words\"], total_examples=biobio_w2v.corpus_count, epochs=15, report_delay=10)\n",
        "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Zrms_F3NDWC",
        "outputId": "fcd35a48-c27e-462f-cea6-516bfe1572db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-17 23:04:43,020 : INFO : precomputing L2-norms of word weight vectors\n"
          ]
        }
      ],
      "source": [
        "# Termino del train\n",
        "biobio_w2v.init_sims(replace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "O0eDn5iVRO-u"
      },
      "outputs": [],
      "source": [
        "# names\n",
        "word_counts = dict()\n",
        "for item in biobio_w2v.wv.vocab:\n",
        "    word_counts[item]=biobio_w2v.wv.vocab[item].count\n",
        "sorted_word_counts = {k: v for k, v in sorted(word_counts.items(), key=lambda item: item[1], reverse=True)}  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "BBR-Q866RZb0"
      },
      "outputs": [],
      "source": [
        "words_to_visualize = list(sorted_word_counts.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFc81RwaR7mb",
        "outputId": "e4812de4-818c-47eb-8427-3989cff3ba6d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['the',\n",
              " 'I',\n",
              " 'you',\n",
              " 'a',\n",
              " 'to',\n",
              " 'of',\n",
              " 'and',\n",
              " 'it',\n",
              " 'in',\n",
              " 'that',\n",
              " 'my',\n",
              " 'is',\n",
              " 'for',\n",
              " 'your',\n",
              " 'this',\n",
              " 'Im',\n",
              " 'on',\n",
              " 'have',\n",
              " 'Oh']"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words_to_visualize[0:19]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "C7I-sOSfSOOS"
      },
      "outputs": [],
      "source": [
        "# Vecs\n",
        "wv_to_visualize = np.array([biobio_w2v.wv[word] for word in words_to_visualize])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CUVFWN-SMbq",
        "outputId": "b3ff5cff-15c8-46b8-e509-eddd298172be"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.0500755 , -0.02210195, -0.12486412, ...,  0.03071526,\n",
              "        -0.10333012, -0.076689  ],\n",
              "       [-0.07210258, -0.07787194,  0.04032613, ...,  0.01740151,\n",
              "        -0.01382055, -0.08306526],\n",
              "       [ 0.00617781,  0.03379272,  0.07769702, ..., -0.01149294,\n",
              "         0.071757  , -0.12407428],\n",
              "       ...,\n",
              "       [-0.11271187, -0.04072125,  0.03460788, ..., -0.11286736,\n",
              "         0.00730961, -0.08717292],\n",
              "       [-0.06523874,  0.05414155,  0.05279206, ...,  0.04678392,\n",
              "         0.04564717, -0.0875266 ],\n",
              "       [ 0.02678107,  0.01191526,  0.07265962, ...,  0.0818354 ,\n",
              "        -0.11959471,  0.05059315]], dtype=float32)"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wv_to_visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vBkF3hreGjg"
      },
      "source": [
        "**Pregunta 2**: Cree una matriz palabra contexto usando el mismo dataset. Configure el largo del vocabulario a 1000 o 2000 tokens, puede agregar valores mayores pero tenga en cuenta que la construcción de la matriz puede tomar varios minutos. Puede que esto tarde un poco. **(0.75 punto)** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzLuH6MneWIY"
      },
      "source": [
        "**Respuesta:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "DY2OpUuxLqui"
      },
      "outputs": [],
      "source": [
        "# Se inicializa la clase\n",
        "matrix=WordContextMatrix(1000,1,df[\"spoken_words\"],wordpunct_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "pJTI1xZFLqui"
      },
      "outputs": [],
      "source": [
        "matrix.build_matrix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAK2sY5xLquj",
        "outputId": "1cd17f29-a16c-4116-a000-ac785ad054a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Representaciones: dict_keys(['agin', 'job', 'final', 'hol', 'cross', 'long', 'three', 'Think', 'sometimes', 'runs', 'both', 'alone', 'trusts', 'Lake', 'profanely', 'electrical', 'Mace', 'pressed', 'Reverend', 'Planters', 'forgot', 'Homer', 'turvy', 'nails', 'gallon', 'Sorry', 'just', 'Harpies', 'martini', 'third', 'honking', 'milk', 'line', 'only', 'gym', 'them', 'desperate', 'sends', 'dare', 'babysit', 'Half', 'afternoon', 'So', 'Away', 'admit', 'of', 'money', 'sticks', 'scarred', 'plan', 'girls', 'punch', 'Someone', 'apologize', 'beautiful', 'invitin', 'letting', 'No', 'any', 'Ned', 'that', 'pine', 'What', 'turn', 'hurts', 'year', 'often', 'That', 'stretch', 'annoying', 'had', 'pounds', 'Ohhh', 're', 'over', 'talks', 'saw', 'wish', 'seat', 'unless', 'shoes', 'along', 'Gentlemen', 'woman', 'blows', 'chews', 'today', 'gone', 'You', 'Bleed', 'Milhouse', 'kids', 'cigar', 'themselves', 'Really', 'embarrassed', 'from', 'arcade', 'met', 'tune', 'eat', 'did', 'it', 'aren', 'try', 'Nothing', 'case', 'delight', 'mind', 'City', 'al', 'birthdays', 'at', 'shaken', 'where', 'Up', 'Didn', 'anybody', 'parenting', 'Daddy', 'emotion', 'better', 'Holy', 'if', 'She', 'because', 'into', 'able', 'have', 'Hell', 'was', 'Ph', 'uh', '...?', 'care', 'eh', 'bringing', 'cute', 'parts', 'tolerant', 'introduce', 'Last', 'our', '€¦..', 'clothes', 'Mister', 'speaking', 'six', 'retreat', 'gin', 'passages', 'And', 'wrong', 'mouth', 'manwise', 'here', 'ten', 'puts', 'Simpsons', 'Could', 'Grief', 'been', 'Okay', 'ugliest', '?!', 'smelliest', 'sense', 'then', 'myself', 'crown', 'Gloria', 'good', 'statements', 'big', 'his', 'matter', 'who', 'flavor', 'Is', 'costume', 'lousy', 'saving', 'pass', 'dear', 'wait', 'Neddie', 'next', 'Fry', 'Sometimes', 'Forever', 'boy', 'hold', 'supposed', 'another', 'idea', 'he', 'Everyone', 'sure', 'weekend', 'bed', 'painful', 'quit', 'needs', 'scratches', 'puttin', 'talk', 'come', 'tired', 'glass', 'John', 'around', 'compete', 'It', 'Never', 'Let', 'homes', 'The', 'Lis', 'called', 'Parkville', 'kicks', 'Yes', 'couldn', 'Pure', 'Oh', 'think', 'whole', 'patient', 'Aw', 'kind', 'Who', 'couple', 'easy', 'noises', 'A', 'spine', 'allowed', 'for', 'collection', 'one', 'miss', 'problem', 'upstairs', 'First', 'knows', 'holding', 'champagne', 'years', 'bit', 'Cool', 'polls', 'promised', 'Copernicus', 'Enjoying', 'teeth', 'wakes', 'hours', 'Krusty', 'witty', 'None', 'bottom', 'question', 'fudge', 'Other', 'worst', 'going', 'hamper', 'bathtub', 'grab', 'touched', 'Ironically', 'Martin', 'vote', 'happened', 'get', 'biggest', 'tail', 'said', 'Leland', 'new', 'also', 'Sprinkles', 'men', 'friends', 'thanks', 'wanna', 'feel', 'Catfish', 'gambles', 'Did', 'seem', 'please', 'Maude', 'season', 'husband', 'holidays', 'few', 'freakishly', 'drove', 'fools', 'times', 'Mixology', 'weighs', 'Hey', 'were', 'goes', 'yellow', 'write', 'to', 'fun', '\"', 'toenails', 'saying', 'president', 'picture', 'Sleep', 'motto', 'chest', 'sister', 'list', 'hooks', 'guy', 'participate', 'slide', 'shorts', 'lives', 'carton', 'ssslurring', 'happy', 'note', 'Thank', 'Whenever', 'topsy', 'Mom', 'fighting', 'Lord', 'cassis', 'short', 'be', 'let', 'crying', 'onto', 'Lucky', 'names', 'banter', 'encounter', 'After', 'ice', 'find', 'Satan', 's', 'yourselves', 'hope', 'together', 'service', 'counted', 'bowling', 'isn', 'He', 'lady', 'ever', 'away', 'novelty', 'Maggie', 'toxic', 'five', 'upwards', 'eyes', 'keep', 'tomorrow', 'live', 'But', 'last', 'number', 'afraid', 'spilled', 'adults', 'suggest', 'old', 'trunk', 'young', 'two', 'Helen', 'help', 'by', 'do', 'seems', 'psychological', 'man', '\",', 'car', 'straighten', 'knew', 'sorry', 'actually', 'me', 'shpeech', 'acting', 'doll', 'President', 'Are', 'fly', 'lesson', 'operation', '..', 'Aisle', 'hilarity', 'class', 'maybe', 'looked', 'under', 'bash', 'Hi', 'everybody', 'Lisa', 'Call', 'Care', 'bless', 'as', 'cubs', 'More', 'backbone', 'centered', 'taste', 'magazines', 'lucky', 'throw', 'behave', 'holy', 'huh', 'left', 'slipped', 'stay', 'honesty', 'himself', 'always', 'hers', 'anyone', 'will', 'food', 'towels', 'handful', 'silly', 'bar', 'they', 'faith', 'know', 'Put', 'side', 'cleanser', 'Dad', 'though', 'sticking', 'voted', 'turnout', '--', 'fraud', 'guys', 'mopey', 'warm', 'nose', 'chocolate', 'him', 'made', 'speeds', 'series', 'against', 'Although', 'm', \"'\", 'except', 'alongside', 'Mr', 'feeb', 'track', 'guess', 'look', 'argue', 'environmentally', 'party', 'each', 'Johnny', 'Just', 'doing', 'Victory', 'Would', 'she', 'Flanders', 'sanitary', 'lose', 'back', 'such', 'with', 'odor', 'brownie', 'extra', 'yeah', 'sweat', 'de', 'serve', 'but', 'love', 'Give', 'Au', 'spit', 'ball', 'day', 'falls', 'All', 'gut', 'are', 'Ow', 'couples', 'true', 'first', 'explain', 'dab', 'demand', 'not', 'hors', 'never', 'alcohol', 'cares', 'best', 'underlines', 'nerve', 'Shhh', 'right', 'dry', 'Mrs', 'everyone', 'substitute', 'didn', 'God', 'whose', 'place', 'Remember', 'resort', 'keys', 'want', 'is', 'interrupt', 'hey', 'ky', 'Look', 'How', 'face', 'wondering', 'traditional', 'done', 'up', 'stinks', 'mean', 'glad', 'Winfield', 'lie', 'beau', 'children', 'got', 'acted', 'jape', 'remember', 'Will', 'I', 'chance', 'little', 'Lovejoy', 'defeat', 'Capital', 'banana', 'wouldn', 'preview', 'night', 'catch', 'tell', 'Bart', 'since', 'sermon', 'Welcome', 'expressions', 'hasn', 'self', 'Mmmm', 'cigars', 'wasted', 'hello', 'this', 'll', 'teachers', 'indeed', 'available', 'understand', 'would', 'If', 'well', 'much', 'Uh', 'ones', 'read', 'almost', 'can', 'Hello', 'things', 'after', 'Gramps', 'My', 'room', 'geeks', 'Baboon', 'about', 'does', 'squeaking', 'hurt', 'Homie', 'nothing', 'Why', 'About', 'Grampa', 'Stanford', 'priceless', 'stink', 'Queen', 'forgotten', 'Check', 'thread', 'seedy', 'finger', 'bums', 'd', 'drinks', 'announcements', 'knowing', 'more', 'before', 'Hmmmm', 'now', 'those', 'contraire', 'attending', 'Majesty', 'again', 'hand', 'a', 'must', 'paid', 'taking', 'king', 'has', 'Today', 'counseling', 'fight', 'guns', 'wonderful', 'wearing', 'off', 'hundred', 'shine', 'rely', 'name', 'bartender', 'Not', 'forgive', 'on', 'give', 'legs', 'teach', 'Get', 'son', 'cooks', 'monkey', 'fishing', 'Mmm', 't', 'Promontory', 'down', 'nobody', 'There', 'noticed', 'roof', 'decided', 'world', 'er', 'swine', 'highly', 'chill', 'parents', 'you', 'plate', 'One', 'ago', 'put', 'sour', 'College', 'head', 'gag', 'anything', 'Bergstrom', 'pamphlets', 'strange', 'don', 'least', 'Somebody', 'Alright', 'fell', 'Bafflers', 'enough', 'Simpson', 'law', 'Where', 'wantin', 'call', 'honey', 'Hmmm', 'neat', 'pronounce', '...', 'band', 'ape', 'go', 'Huh', 'the', 'Go', 'cut', 'roll', 'Do', 'sir', 'boarding', '5', 'wedding', 'Yeah', 'Children', 'something', 'okay', 'time', 'what', 'Whoa', 'won', 'lowlifes', 'doesn', '-', 'business', 'brings', 'creme', 'baby', 'gee', 'sign', 'Please', 'way', 'bourbon', 'laundry', 'cubes', 'bottle', 'run', 'in', 'pretending', 'music', 'whimsical', 'forget', 'sitter', 'confidence', 'supermarket', 'point', 'Sit', 'hanging', 'Come', 'chewing', 'until', 'news', 'say', 'projects', 'changes', 'socket', 'lot', 'see', 'very', 'fever', 'realize', 'tempting', 'Goodbye', 'all', 'aboard', 'take', 'took', 'Thanks', 'could', 'says', 'shots', 'Burger', 'notice', 'wife', '!', 'At', 'Fill', 'ask', 'these', 'special', 'kick', 'am', 'sophisticated', 'To', 'and', 'threw', 'keeps', 'Anybody', 'lots', 'disease', 'real', 'based', 'annual', 'feelings', '?...', 'someone', 'stop', 'newsrack', 'yet', 'We', 'week', 'Now', 'out', 'through', 'leave', 'thrown', 'country', 'glasses', 'roo', 'most', 'us', 'Can', 'heard', 'her', 'Don', 'catfish', 'Nuts', ',\"', 'an', 'Your', 'French', 'drink', 'house', 'Old', 've', 'like', 'hurry', 'bound', 'Two', 'They', '.\"', 'General', 'marriage', 'thank', 'advantage', 'Voting', 'baboon', 'step', 'Boners', 'happiness', 'oh', 'touch', 'work', 'tone', 'being', 'girl', 'perfect', 'peanuts', 'This', 'loving', 'Me', 'older', 'should', 'middle', 'Gimme', 'respect', 'open', 'chemicals', 'forgets', 'jigger', 'hangs', 'teacher', 'recess', 'Her', 'lord', 'dress', 'shopping', 'syrup', 'Well', 'somebody', 'faults', 'Wait', 'insane', 'kindly', 'doovers', 'natural', 'so', 'In', 'musicians', 'your', 'anniversaries', 'other', 'Jeez', 'minute', 'moly', 'coming', 'Marriage', 'smoke', 'known', 'spike', 'exactly', 'my', 'there', 'people', 'probably', 'end', 'train', 'lo', 'shows', 'believe', 'sleep', 'when', 'invited', 'hated', 'funniest', 'ya', 'really', 'Here', 'gas', 'need', 'or', 'serving', 'worth', 'record', 'gonna', 'church', ':', 'pay', 'thought', 'reconciled', 'bait', 'sneak', 'how', 'half', 'Moved', 'recount', 'Good', 'teens', 'Sherman', ',', 'too', 'tiful', 'Dr', 'Bible', 'babysitter', 'makes', 'sound', 'video', 'telling', 'golden', 'headed', 'might', '?', 'including', 'gotten', 'hate', 'Hel', 'wet', 'stupidest', 'Yayyyyyyyyyyyyyy', 'living', 'why', 'crazy', 'Shelbyville', 'andâ', 'abandon', 'allright', 'store', 'Lewis', 'thing', 'hear', 'Marge', 'no', 'marriages', 'we', 'inside', 'drunk', 'religious', 'lost', 'fix', 'filthy', 'life', 'Ah', 'morning', 'father', 'takes', '.', 'rum', 'make', 'their', 'some', 'When', 'secular', 'dagnabit', 'unk'])\n"
          ]
        }
      ],
      "source": [
        "matrix.matrix2dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "Dwhy5LHILquk"
      },
      "outputs": [],
      "source": [
        "embeddings=matrix.dic_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "ieXPVjCOLquk",
        "outputId": "8dd8b79d-d3ed-49ad-bf2c-51264bde26f8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f49aa576-1268-4dc8-a063-3452d67712fe\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>agin</th>\n",
              "      <th>job</th>\n",
              "      <th>final</th>\n",
              "      <th>hol</th>\n",
              "      <th>cross</th>\n",
              "      <th>long</th>\n",
              "      <th>three</th>\n",
              "      <th>Think</th>\n",
              "      <th>sometimes</th>\n",
              "      <th>runs</th>\n",
              "      <th>...</th>\n",
              "      <th>takes</th>\n",
              "      <th>.</th>\n",
              "      <th>rum</th>\n",
              "      <th>make</th>\n",
              "      <th>their</th>\n",
              "      <th>some</th>\n",
              "      <th>When</th>\n",
              "      <th>secular</th>\n",
              "      <th>dagnabit</th>\n",
              "      <th>unk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>88</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>141</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>0</td>\n",
              "      <td>151</td>\n",
              "      <td>71</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>161</td>\n",
              "      <td>477</td>\n",
              "      <td>11</td>\n",
              "      <td>29</td>\n",
              "      <td>14</td>\n",
              "      <td>...</td>\n",
              "      <td>43</td>\n",
              "      <td>51595</td>\n",
              "      <td>2</td>\n",
              "      <td>277</td>\n",
              "      <td>976</td>\n",
              "      <td>1760</td>\n",
              "      <td>44</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1001 rows × 1001 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f49aa576-1268-4dc8-a063-3452d67712fe')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f49aa576-1268-4dc8-a063-3452d67712fe button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f49aa576-1268-4dc8-a063-3452d67712fe');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      agin  job  final  hol  cross  long  three  Think  sometimes  runs  ...  \\\n",
              "0        0    0      0    0      0     0      0      0          0     0  ...   \n",
              "1        0    0      0    0      0     0      0      0          0     0  ...   \n",
              "2        0    0      0    0      0     0      0      0          0     0  ...   \n",
              "3        0    0      0    0      0     0      0      0          0     0  ...   \n",
              "4        0    0      0    0      0     0      0      0          0     0  ...   \n",
              "...    ...  ...    ...  ...    ...   ...    ...    ...        ...   ...  ...   \n",
              "996      0    0      0    0      1     1      0      0          0     0  ...   \n",
              "997      0    0      0    0      0     0      0      0          0     0  ...   \n",
              "998      0    0      0    0      0     0      0      0          0     0  ...   \n",
              "999      0    0      0    0      0     0      0      0          0     0  ...   \n",
              "1000     0  151     71    0      8   161    477     11         29    14  ...   \n",
              "\n",
              "      takes      .  rum  make  their  some  When  secular  dagnabit  unk  \n",
              "0         0      0    0     0      0     0     0        0         0    0  \n",
              "1         1     88    0     0      0     0     0        0         0    0  \n",
              "2         0      3    0     0      2     0     0        0         0    0  \n",
              "3         0      1    0     0      0     0     0        0         0    0  \n",
              "4         0      0    0     0      0     1     0        0         0    0  \n",
              "...     ...    ...  ...   ...    ...   ...   ...      ...       ...  ...  \n",
              "996       0     12    0    28      0     0     0        0         0    0  \n",
              "997       0    141    0     0      0     0     0        0         0    0  \n",
              "998       0      0    0     0      0     0     0        0         0    0  \n",
              "999       0      0    0     0      0     0     0        0         0    0  \n",
              "1000     43  51595    2   277    976  1760    44        2         0    0  \n",
              "\n",
              "[1001 rows x 1001 columns]"
            ]
          },
          "execution_count": 125,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRCB-jqgTNcs"
      },
      "source": [
        "#### **Parte C (1.5 puntos): Aplicar embeddings para clasificar**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlqzlJRSTNcs"
      },
      "source": [
        "Ahora utilizaremos los embeddings que acabamos de calcular para clasificar palabras basadas en su polaridad (positivas o negativas). \n",
        "\n",
        "Para esto ocuparemos el lexicón AFINN incluido en la tarea, que incluye una lista de palabras y un 1 si su connotación es positiva y un -1 si es negativa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "CMskFDmHTNcs"
      },
      "outputs": [],
      "source": [
        "AFINN = 'AFINN_full.csv'\n",
        "df_afinn = pd.read_csv(AFINN, sep='\\t', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "FCkyEukrTgXu",
        "outputId": "77c48c35-a1b8-4e01-cb64-1c18e60a26b6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-81f478be-95cc-42f9-835d-fba7d592f645\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tops</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>groan</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>perfects</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>spammer</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>saluting</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3377</th>\n",
              "      <td>mediocrity</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3378</th>\n",
              "      <td>bold</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3379</th>\n",
              "      <td>hating</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3380</th>\n",
              "      <td>unfavorable</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3381</th>\n",
              "      <td>scapegoats</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3382 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81f478be-95cc-42f9-835d-fba7d592f645')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-81f478be-95cc-42f9-835d-fba7d592f645 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-81f478be-95cc-42f9-835d-fba7d592f645');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                0  1\n",
              "0            tops  1\n",
              "1           groan -1\n",
              "2        perfects  1\n",
              "3         spammer -1\n",
              "4        saluting  1\n",
              "...           ... ..\n",
              "3377   mediocrity -1\n",
              "3378         bold  1\n",
              "3379       hating -1\n",
              "3380  unfavorable -1\n",
              "3381   scapegoats -1\n",
              "\n",
              "[3382 rows x 2 columns]"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_afinn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaKl8hsCTNcs"
      },
      "source": [
        "Hint: Para w2v y la wcm son esperables KeyErrors debido a que no todas las palabras del corpus de los simpsons tendrán una representación en AFINN. Para el caso de la matriz palabra contexto se recomienda convertir su matrix a un diccionario. Pueden utilizar esta función auxiliar para filtrar las filas en el dataframe que no tienen embeddings (como w2v no tiene token UNK se deben ignorar)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWSSuctiTNcs"
      },
      "outputs": [],
      "source": [
        "def try_apply(model,word):\n",
        "    try:\n",
        "        aux = model[word]\n",
        "        return True\n",
        "    except KeyError:\n",
        "        #logger.error('Word {} not in dictionary'.format(word))\n",
        "        return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrVPeEzgTNcs"
      },
      "source": [
        "**Pregunta 1**: Transforme las palabras del corpus de AFINN a la representación en embedding que acabamos de calcular (con ambos modelos). \n",
        "\n",
        "Su dataframe final debe ser del estilo [embedding, sentimiento], donde los embeddings corresponden a $X$ y el sentimiento asociado con el embedding a $y$ (positivo/negativo, 1/-1). \n",
        "\n",
        "Para ambos modelos, separar train y test de acuerdo a la siguiente función. **(0.5 puntos)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "L1R4ZIbNcX_4"
      },
      "outputs": [],
      "source": [
        "# Se inicializa la clase\n",
        "matrix=WordContextMatrix(1000,1,df_afinn[0],wordpunct_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "OQ_vYSnbcZB_"
      },
      "outputs": [],
      "source": [
        "matrix.build_matrix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4kh3LwIcaCo",
        "outputId": "43a241e8-d1b0-4aa2-94fa-503a30ab5b6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Representaciones: dict_keys(['threat', 'disconsolation', 'vomited', 'aggravate', 'join', 'annoy', 'nerves', 'homicide', 'menaces', 'romantically', 'threatens', 'possessive', 'emptiness', 'warfare', 'interesting', 'profit', 'mesmerizing', 'ruining', 'boost', 'breached', 'changing', 'deports', 'swift', 'homesick', 'wishing', 'corrupting', 'cheer', 'charmless', 'moping', 'misbehave', 'transgressing', 'bribed', 'rage', 'forgot', 'lololol', 'delightfully', 'weary', 'suicide', 'empower', 'boycotted', 'harasses', 'smartest', 'burdened', 'unpleasant', 'mock', 'motherfucking', 'desperate', 'victimization', 'intimidates', 'blind', 'impairment', 'immune', 'coziness', 'substantial', 'ill', 'violate', 'insufficiently', 'scapegoat', 'cherishes', 'embittered', 'disregarded', 'despairing', 'havoc', 'ubiquitous', 'contaminant', 'solemn', 'gallantry', 'excited', 'suspend', 'annoying', 'censor', 'exacerbate', 'justifiably', 'nurturing', 'developed', 'infection', 'wish', 'fear', 'killing', 'hopes', 'handpicked', 'punished', 'derision', 'stabs', 'awaited', 'secure', 'lackadaisical', 'poison', 'hinder', 'vitality', 'hospitalized', 'hehe', 'denied', 'mocked', 'romantical', 'regretting', 'deceitful', 'chic', 'hardy', 'gem', 'tremors', 'injuries', 'loom', 'growing', 'misread', 'misfortune', 'distract', 'humourous', 'slick', 'sues', 'greenwashing', 'deploring', 'anxious', 'groaning', 'offense', 'disadvantage', 'heroes', 'yes', 'stabbed', 'contagions', 'carefully', 'uninvolving', 'arrested', 'politeness', 'scare', 'terribly', 'worse', 'worries', 'blesses', 'numb', 'dizzy', 'rise', 'cute', 'resign', 'passively', 'maldevelopment', 'exhilarating', 'exclusion', 'snubbed', 'insult', 'disruptive', 'adequate', 'drowned', 'naive', 'dishonest', 'appreciated', 'reaches', 'obliterated', 'exposed', 'gross', 'underperformed', 'gleeful', 'imprisoned', 'asset', 'wanker', 'maladaption', 'fearfully', 'effect', 'fucking', 'good', 'phobic', 'mislead', 'headache', 'blissful', 'misinformed', 'donating', 'mature', 'enslaves', 'burdening', 'distracts', 'die', 'infatuated', 'bribes', 'vibrant', 'underperforms', 'recommends', 'vomiting', 'lunatic', 'spammers', 'chaos', 'engrossed', 'caring', 'prudence', 'kinder', 'murder', 'frightening', 'favour', 'painful', 'abandons', 'indifferent', 'litigious', 'seditious', 'tired', 'despondent', 'rotfl', 'jubilant', 'seamlessly', 'fascinating', 'cowardly', 'disquiet', 'boldly', 'belittle', 'sentencing', 'slashed', 'touts', 'fresh', 'visionary', 'kidnapped', 'excluded', 'charges', 'worry', 'zealot', 'hatred', 'nasty', 'goofy', 'invincible', 'lobbyists', 'assassinations', 'tyrannical', 'backs', 'reprimand', 'stank', 'questioning', 'exclude', 'exhilarates', 'vomits', 'screwed', 'excitement', 'weakness', 'brilliant', 'avenge', 'naïve', 'easy', 'joyfully', 'fuckhead', 'tops', 'defender', 'homicides', 'disappointed', 'trapped', 'chilling', 'indestructible', 'despairs', 'wins', 'ashamed', 'lifeless', 'impress', 'infracting', 'shortages', 'championed', 'promised', 'merry', 'amused', 'empowerment', 'disgust', 'infects', 'aggressiveness', 'forbidden', 'crisis', 'compassion', 'once', 'appease', 'innovates', 'working', 'shame', 'dignity', 'sappy', 'chastised', 'strengthens', 'succeeding', 'unlikely', 'libelous', 'carefulness', 'fatigues', 'loathes', 'devastations', 'talent', 'infected', 'irritated', 'contemptible', 'ghost', 'fools', 'avoid', 'youthful', 'deceive', 'fun', 'rigged', 'controversial', 'impairing', 'criticizes', 'fallen', 'glamorous', 'clears', 'illnesses', 'infringement', 'death', 'false', 'harmony', 'collapses', 'obscene', 'idiot', 'losses', 'irresistibly', 'dumbass', 'stuck', 'barbarous', 'collisions', 'elation', 'blockbuster', 'distrustful', 'undeserving', 'fan', 'indoctrinate', 'intimidated', 'insolvent', 'betrayed', 'validate', 'heartbroken', 'game', 'bored', 'offline', 'mumpish', 'stressors', 'collides', 'disruption', 'ghastly', 'raptures', 'approved', 'disgusting', 'altruistic', 'ensure', 'overlooked', 'misreport', 'fag', 'panics', 'drowns', 'mispricing', 'ha', 'favourited', 'cuts', 'entrusted', 'thoughtless', 'exonerated', 'looms', 'smuggled', 'exploiting', 'inexcusable', 'help', 'inflict', 'enraging', 'haters', 'commitment', 'rejoiced', 'interrupts', 'accomplish', 'defeated', 'salute', 'clash', 'curious', 'scorn', 'oversimplified', 'dissatisfied', 'trust', 'joke', 'laugh', 'dejecting', 'stimulate', 'unwanted', 'thwarted', 'integrity', 'impaired', 'unfitted', 'cherishing', 'heavyhearted', 'promptly', 'fakes', 'winning', 'wisdom', 'poisoned', 'discards', 'firing', 'fascinate', 'ban', 'ignores', 'goofiness', 'disrupting', 'irate', 'righteousness', 'choked', 'infract', 'menaced', 'corruption', 'successfully', 'lucky', 'deceptive', 'ignorant', 'shoody', 'passion', 'frikin', 'unfunny', 'plaguing', 'depressing', 'funny', 'cool', 'apologised', 'adored', 'favors', 'attacked', 'humor', 'virulent', 'ranter', 'expertly', 'perfects', 'block', 'dauntless', 'worsens', 'worthless', 'side', 'expelling', 'restoring', 'solidifying', 'significance', 'unsecured', 'shy', 'abilities', 'hilarious', 'fated', 'deceit', 'fines', 'forbid', 'drained', 'xo', 'tears', 'disparages', 'trauma', 'alert', 'zealous', 'cheers', 'competencies', 'disrupts', 'cheering', 'share', 'frustrated', 'misused', 'excellent', 'brave', 'spammer', 'solidify', 'traps', 'bribe', 'cheaters', 'hypocritical', 'chagrined', 'grace', 'lonely', 'improve', 'misunderstands', 'contentious', 'lifetime', 'conspiracy', 'bothered', 'miserable', 'mourned', 'oversight', 'escapes', 'evacuation', 'amuse', 'meditative', 'innovate', 'sexy', 'aggravating', 'harried', 'devoted', 'love', 'sulking', 'contamination', 'misusing', 'noob', 'overweight', 'fatiguing', 'confuse', 'pleasure', 'true', 'contaminating', 'pitied', 'desperately', 'racism', 'not', 'protects', 'unsophisticated', 'exciting', 'inconsiderate', 'astoundingly', 'douche', 'beaten', 'violent', 'skeptical', 'outrageous', 'passionate', 'moron', 'loss', 'stereotype', 'bamboozles', 'perpetrator', 'enthral', 'collapsing', 'rejects', 'terrifically', 'dysfunction', 'killed', 'plodding', 'up', 'inappropriate', 'bitterly', 'distracted', 'deft', 'bankruptcy', 'colluding', 'bitterest', 'limitation', 'impairs', 'fearlessness', 'advisable', 'defeat', 'cutbacks', 'whitewash', 'award', 'attractiveness', 'braveness', 'cock', 'cancer', 'murderous', 'lawl', 'disproportionate', 'lobby', 'applause', 'smuggle', 'free', 'successful', 'bastard', 'wasted', 'serene', 'clean', 'overstatements', 'comforts', 'gloom', 'discarded', 'criticises', 'sloppy', 'non', 'validating', 'jovial', 'effortlessly', 'accomplishments', 'incoherent', 'imprisonment', 'well', 'suicides', 'strike', 'shared', 'disliked', 'improper', 'cancel', 'inexorable', 'rightfully', 'unfocused', 'insecure', 'restriction', 'goddamn', 'corrupts', 'racists', 'hooligan', 'hurt', 'sufferer', 'contaminate', 'dont', 'ambitious', 'memoriam', 'terrible', 'catastrophe', 'misbehaves', 'incompetent', 'jailed', 'lmfao', 'filth', 'complaints', 'dilligence', 'stinker', 'unmatched', 'outbreak', 'clueless', 'a', 'misrepresents', 'survivor', 'chided', 'vulnerability', 'appropriately', 'nervous', 'gun', 'wonderful', 'alas', 'cynic', 'yeees', 'boosting', 'distorted', 'evil', 'fidgety', 'fame', 'disastrous', 'dilemma', 'obstructing', 'embezzlement', 'celebrated', 'scars', 'stinky', 'pleasant', 'complicating', 'skeptic', 'misbehaving', 'loved', 'stuff', 'unacceptable', 'overran', 'traumatic', 'hid', 'effectiveness', 'oversells', 'frenzy', 'deny', 'detain', 'vigilant', 'bullshit', 'dirtiest', 'falsely', 'saluting', 'parley', 'charged', 'unresearched', 'fairness', 'adoring', 'awful', 'criticized', 'fine', 'falling', 'damages', 'oppressive', 'destroys', 'super', 'exquisite', 'rapture', 'empathetic', 'inquisitive', 'polished', 'annoys', 'interruption', 'horrendous', 'smiled', 'spirit', 'comfortably', 'raptured', 'encourages', 'damnit', 'wasting', 'bailout', 'refused', 'provokes', 'hates', 'strengthening', 'accomplishment', 'discomfort', 'goodness', 'champion', 'cheery', 'drags', 'problems', 'warmth', 'expelled', '-', 'advantages', 'adventurous', 'complaining', 'disorganized', 'deteriorates', 'fucker', 'grey', 'disappointment', 'bastards', 'brightest', 'pretending', 'whimsical', 'reached', 'bamboozle', 'unintelligent', 'in', 'worshiped', 'harming', 'confidence', 'agreed', 'unbelievable', 'mistakes', 'convince', 'smart', 'offending', 'deficit', 'inoperative', 'overselling', 'endorse', 'avoids', 'welcomed', 'humiliation', 'amusements', 'envious', 'pushy', 'exasperating', 'noble', 'strangled', 'prominent', 'faking', 'affected', 'ineptitude', 'absorbed', 'idiotic', 'wronged', 'sympathy', 'downhearted', 'lags', 'proactive', 'loose', 'looool', 'celebrates', 'hellish', 'barbaric', 'ennui', 'inept', 'assfucking', 'weakened', 'glorious', 'revolting', 'wealthy', 'restored', 'beating', 'aggressions', 'cynicism', 'misfire', 'violence', 'unsound', 'spotless', 'failed', 'laughs', 'scar', 'insulted', 'macabre', 'laughed', 'greedy', 'badass', 'memorable', 'cocky', 'defraud', 'irreversible', 'proudly', 'exacerbated', 'stop', 'subversive', 'squander', 'luckily', 'restricted', 'imposter', 'lifesaver', 'ostracizes', 'cheerless', 'torture', 'comic', 'saluted', 'starved', 'unapproved', 'tribulation', 'groans', 'post', 'gloomy', 'gracious', 'inflamed', 'dipshit', 'xoxoxo', 'hooligans', 'disregard', 'safely', 'comprehensive', 'breakthrough', 'deadly', 'infantile', 'demands', 'terrorizes', 'lool', 'enlightening', 'hail', 'sceptical', 'lag', 'crazier', 'incapacitates', 'lethal', 'responsibility', 'reassures', 'promoting', 'capability', 'like', 'abhorrent', 'douchebag', 'unaware', 'straight', 'inaction', 'infect', 'offender', 'groan', 'disrespected', 'injustice', 'horrible', 'miserably', 'qualities', 'threatened', 'undermine', 'ease', 'pessimistic', 'wow', 'unequaled', 'flawlessly', 'tortures', 'greeted', 'happiness', 'save', 'evacuate', 'squandered', 'rotflol', 'droopy', 'n00b', 'greatest', 'slashing', 'scold', 'coward', 'being', 'entitled', 'unhappiness', 'eager', 'strikers', 'loathsome', 'loving', 'rejection', 'demoralizing', 'horrific', 'warned', 'ability', 'inspirational', 'smear', 'blamed', 'likeable', 'transgresses', 'dead', 'burdens', 'violently', 'obstacle', 'cocksuckers', 'mourning', 'sinister', 'likers', 'insane', 'tyrannic', 'indignant', 'natural', 'complaint', 'justice', 'profits', 'exultant', 'huckster', 'diseases', 'amazing', 'unparliamentary', 'haunting', 'leadership', 'apeshit', 'promotes', 'prohibit', 'kidnappings', 'avenges', 'pardons', 'plagued', 'fearsome', 'cheated', 'hated', 'stealing', 'grateful', 'unable', 'gold', 'influential', 'sweeter', 'demonstration', 'dubious', 'assets', 'stressed', 'discriminates', 'worth', 'accept', 'solves', 'humane', 'bankster', 'unsupported', 'bizarre', 'tumor', 'discord', 'lol', 'accepting', 'congratulate', 'disagree', 'casualty', 'destructive', 'looses', 'ashame', 'stalled', 'shares', 'failure', 'insensitive', 'contend', 'pained', 'sentence', 'hugs', 'wrongdoing', 'aborts', 'dejects', 'collapsed', 'misinterpreted', 'delectable', 'acquits', 'favored', 'wreck', 'salvation', 'attracting', 'hate', 'eerie', 'reassure', 'unoriginal', 'harassed', 'gagged', 'accuse', 'prevents', 'chiding', 'cried', 'swiftly', 'fiasco', 'gr8', 'sneaky', 'cry', 'commit', 'endorses', 'hoax', 'comfort', 'agree', 'forced', 'respected', 'misconducted', 'boosted', 'congratulation', 'charmingly', 'disqualified', 'mess', 'inhuman', 'no', 'doubting', 'rises', 'suave', 'litigation', 'drunk', 'sorrow', 'blocks', 'pillage', 'misreports', 'intransigence', 'stampede', 'debonair', 'lost', 'arsehole', 'disillusioned', 'clever', 'antagonistic', 'neglect', 'significant', 'encouraging', 'favourite', 'stout', 'fatigue', 'conciliates', 'fascist', 'upsetting', 'endorsement', 'fails', 'exploration', 'exuberant', 'fondness', 'dreams', 'exposing', 'startling', 'support', 'punishes', 'unk'])\n"
          ]
        }
      ],
      "source": [
        "matrix.matrix2dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "QrbhRgQ4cae8"
      },
      "outputs": [],
      "source": [
        "embeddings=matrix.dic_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "hdvcSSIxca8U",
        "outputId": "8fac8763-b1ce-4817-dea5-75dee85f8feb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2833adae-4125-4ea7-b52c-24fe65c9dd6d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>threat</th>\n",
              "      <th>disconsolation</th>\n",
              "      <th>vomited</th>\n",
              "      <th>aggravate</th>\n",
              "      <th>join</th>\n",
              "      <th>annoy</th>\n",
              "      <th>nerves</th>\n",
              "      <th>homicide</th>\n",
              "      <th>menaces</th>\n",
              "      <th>romantically</th>\n",
              "      <th>...</th>\n",
              "      <th>fails</th>\n",
              "      <th>exploration</th>\n",
              "      <th>exuberant</th>\n",
              "      <th>fondness</th>\n",
              "      <th>dreams</th>\n",
              "      <th>exposing</th>\n",
              "      <th>startling</th>\n",
              "      <th>support</th>\n",
              "      <th>punishes</th>\n",
              "      <th>unk</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1001 rows × 1001 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2833adae-4125-4ea7-b52c-24fe65c9dd6d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2833adae-4125-4ea7-b52c-24fe65c9dd6d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2833adae-4125-4ea7-b52c-24fe65c9dd6d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      threat  disconsolation  vomited  aggravate  join  annoy  nerves  \\\n",
              "0          0               0        0          0     0      0       0   \n",
              "1          0               0        0          0     0      0       0   \n",
              "2          0               0        0          0     0      0       0   \n",
              "3          0               0        0          0     0      0       0   \n",
              "4          0               0        0          0     0      0       0   \n",
              "...      ...             ...      ...        ...   ...    ...     ...   \n",
              "996        0               0        0          0     0      0       0   \n",
              "997        0               0        0          0     0      0       0   \n",
              "998        0               0        0          0     0      0       0   \n",
              "999        0               0        0          0     0      0       0   \n",
              "1000       0               0        0          0     0      0       0   \n",
              "\n",
              "      homicide  menaces  romantically  ...  fails  exploration  exuberant  \\\n",
              "0            0        0             0  ...      0            0          0   \n",
              "1            0        0             0  ...      0            0          0   \n",
              "2            0        0             0  ...      0            0          0   \n",
              "3            0        0             0  ...      0            0          0   \n",
              "4            0        0             0  ...      0            0          0   \n",
              "...        ...      ...           ...  ...    ...          ...        ...   \n",
              "996          0        0             0  ...      0            0          0   \n",
              "997          0        0             0  ...      0            0          0   \n",
              "998          0        0             0  ...      0            0          0   \n",
              "999          0        0             0  ...      0            0          0   \n",
              "1000         0        0             0  ...      0            0          0   \n",
              "\n",
              "      fondness  dreams  exposing  startling  support  punishes  unk  \n",
              "0            0       0         0          0        0         0    0  \n",
              "1            0       0         0          0        0         0    0  \n",
              "2            0       0         0          0        0         0    0  \n",
              "3            0       0         0          0        0         0    0  \n",
              "4            0       0         0          0        0         0    0  \n",
              "...        ...     ...       ...        ...      ...       ...  ...  \n",
              "996          0       0         0          0        0         0    0  \n",
              "997          0       0         0          0        0         0    0  \n",
              "998          0       0         0          0        0         0    0  \n",
              "999          0       0         0          0        0         0    0  \n",
              "1000         0       0         0          0        0         0    0  \n",
              "\n",
              "[1001 rows x 1001 columns]"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "uxP_fHxzUTF4"
      },
      "outputs": [],
      "source": [
        "class BaseFeature(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "aryjAI6_UBSx"
      },
      "outputs": [],
      "source": [
        "class Doc2VecTransformer(BaseFeature):\n",
        "    \"\"\" Transforma tweets a representaciones vectoriales usando algún modelo de Word Embeddings.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, aggregation_func):\n",
        "        # extraemos los embeddings desde el objeto contenedor. ojo con esta parte.\n",
        "        self.model = model.wv \n",
        "        \n",
        "        # indicamos la función de agregación (np.min, np.max, np.mean, np.sum, ...)\n",
        "        self.aggregation_func = aggregation_func\n",
        "\n",
        "    def simple_tokenizer(self, doc, lower=False):\n",
        "        \"\"\"Tokenizador. Elimina signos de puntuación, lleva las letras a minúscula(opcional) y \n",
        "           separa el tweet por espacios.\n",
        "        \"\"\"\n",
        "        if lower:\n",
        "            doc.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
        "        return doc.translate(str.maketrans('', '', string.punctuation)).split()\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        \n",
        "        doc_embeddings = []\n",
        "        \n",
        "        for doc in X:\n",
        "            tokens = self.simple_tokenizer(doc, lower = True) \n",
        "            \n",
        "            selected_wv = []\n",
        "            for token in tokens:\n",
        "                if token in self.model.vocab:\n",
        "                    selected_wv.append(self.model[token])\n",
        "                    \n",
        "            if len(selected_wv) > 0:\n",
        "                doc_embedding = self.aggregation_func(np.array(selected_wv), axis=0)\n",
        "                doc_embeddings.append(doc_embedding)\n",
        "            else: \n",
        "                #print('No pude encontrar ningún embedding en el tweet: {}. Agregando vector de ceros.'.format(doc))\n",
        "                doc_embeddings.append(np.zeros(self.model.vector_size)) # la dimension del modelo \n",
        "\n",
        "        return np.array(doc_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "0Bkt26BwTNcs",
        "outputId": "f4056caa-fc6d-4639-cab9-388325319826"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-34a047f3-f425-4d6b-9ef9-7d1ca8a3fb6b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tops</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>groan</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>perfects</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>spammer</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>saluting</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3377</th>\n",
              "      <td>mediocrity</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3378</th>\n",
              "      <td>bold</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3379</th>\n",
              "      <td>hating</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3380</th>\n",
              "      <td>unfavorable</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3381</th>\n",
              "      <td>scapegoats</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3382 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34a047f3-f425-4d6b-9ef9-7d1ca8a3fb6b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-34a047f3-f425-4d6b-9ef9-7d1ca8a3fb6b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-34a047f3-f425-4d6b-9ef9-7d1ca8a3fb6b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          content  category\n",
              "0            tops         1\n",
              "1           groan        -1\n",
              "2        perfects         1\n",
              "3         spammer        -1\n",
              "4        saluting         1\n",
              "...           ...       ...\n",
              "3377   mediocrity        -1\n",
              "3378         bold         1\n",
              "3379       hating        -1\n",
              "3380  unfavorable        -1\n",
              "3381   scapegoats        -1\n",
              "\n",
              "[3382 rows x 2 columns]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# creamos una nueva columna titulo y contenido.\n",
        "content = df_afinn[0] \n",
        "\n",
        "# obtenemos las clases\n",
        "subcategory = df_afinn[1] \n",
        "\n",
        "# dejamos en el dataset solo contenido de la noticia y categoria\n",
        "dataset = pd.DataFrame({'content': content, 'category': subcategory})\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "z0xU1_yrUAOY"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dataset.content,\n",
        "                                                    dataset.category,\n",
        "                                                    test_size=0.33,\n",
        "                                                    random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "PXynaLffUvaa"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(max_iter=1000000)\n",
        "\n",
        "doc2vec_mean = Doc2VecTransformer(biobio_w2v, np.mean)\n",
        "doc2vec_sum = Doc2VecTransformer(biobio_w2v, np.sum)\n",
        "doc2vec_max = Doc2VecTransformer(biobio_w2v, np.max)\n",
        "\n",
        "pipeline = Pipeline([('doc2vec', doc2vec_sum), ('clf', clf)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRGX_hPgVNU0",
        "outputId": "d46cfd24-fd77-428e-ad14-fe69cd355597"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('doc2vec',\n",
              "                 Doc2VecTransformer(aggregation_func=<function sum at 0x7f484bca5560>,\n",
              "                                    model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f48163a2e90>)),\n",
              "                ('clf', LogisticRegression(max_iter=1000000))])"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "-91RvJycVS8e"
      },
      "outputs": [],
      "source": [
        "y_pred = pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWJtmXMsVf3J",
        "outputId": "4523bc1a-7f91-4f1e-ccc4-fb49e333b065"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[712  14]\n",
            " [373  18]]\n"
          ]
        }
      ],
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o7zJYy-VisC",
        "outputId": "9ea489c2-1de9-4dd7-a16a-4a710028a7c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.66      0.98      0.79       726\n",
            "           1       0.56      0.05      0.09       391\n",
            "\n",
            "    accuracy                           0.65      1117\n",
            "   macro avg       0.61      0.51      0.44      1117\n",
            "weighted avg       0.62      0.65      0.54      1117\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVak3rSQXBm-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDcq5czXTNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upAn_eT4TNct"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDKe4gA3TNct"
      },
      "source": [
        "**Pregunta 2**: Entrenar una regresión logística (vista en auxiliar) y reportar accuracy, precision, recall, f1 y confusion_matrix para ambos modelos. Por qué se obtienen estos resultados? Cómo los mejorarías? Como podrías mejorar los resultados de la matriz palabra contexto? es equivalente al modelo word2vec? **(1 punto)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJMzq_dETNct"
      },
      "source": [
        "**Respuesta**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IIUKuLiXMpo"
      },
      "source": [
        "# WCM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9omFUaBrXOk6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk2Z6UMKXPc1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue_yOZlkXG7J"
      },
      "source": [
        "# Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "bj1r_BnKn_7L",
        "outputId": "3bdd2949-91a8-4cae-ce30-43f5dc147532"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f1820756-02be-4794-99ec-cd852e4e3192\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tops</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>groan</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>perfects</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>spammer</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>saluting</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3377</th>\n",
              "      <td>mediocrity</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3378</th>\n",
              "      <td>bold</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3379</th>\n",
              "      <td>hating</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3380</th>\n",
              "      <td>unfavorable</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3381</th>\n",
              "      <td>scapegoats</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3382 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1820756-02be-4794-99ec-cd852e4e3192')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f1820756-02be-4794-99ec-cd852e4e3192 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f1820756-02be-4794-99ec-cd852e4e3192');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "          content  category\n",
              "0            tops         1\n",
              "1           groan        -1\n",
              "2        perfects         1\n",
              "3         spammer        -1\n",
              "4        saluting         1\n",
              "...           ...       ...\n",
              "3377   mediocrity        -1\n",
              "3378         bold         1\n",
              "3379       hating        -1\n",
              "3380  unfavorable        -1\n",
              "3381   scapegoats        -1\n",
              "\n",
              "[3382 rows x 2 columns]"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# creamos una nueva columna titulo y contenido.\n",
        "content = df_afinn[0] \n",
        "\n",
        "# obtenemos las clases\n",
        "subcategory = df_afinn[1] \n",
        "\n",
        "# dejamos en el dataset solo contenido de la noticia y categoria\n",
        "dataset = pd.DataFrame({'content': content, 'category': subcategory})\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "I5He_e-AWlzW"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(dataset.content,\n",
        "                                                    dataset.category,\n",
        "                                                    test_size=0.33,\n",
        "                                                    random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "dsYAmFQfWrbn"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(max_iter=1000000)\n",
        "\n",
        "doc2vec_mean = Doc2VecTransformer(biobio_w2v, np.mean)\n",
        "doc2vec_sum = Doc2VecTransformer(biobio_w2v, np.sum)\n",
        "doc2vec_max = Doc2VecTransformer(biobio_w2v, np.max)\n",
        "\n",
        "pipeline = Pipeline([('doc2vec', doc2vec_sum), ('clf', clf)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tx_Z-ce_WwNv",
        "outputId": "0aa9468d-d39e-4fd7-d646-7a220f563e76"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Pipeline(steps=[('doc2vec',\n",
              "                 Doc2VecTransformer(aggregation_func=<function sum at 0x7f484bca5560>,\n",
              "                                    model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f48163a2e90>)),\n",
              "                ('clf', LogisticRegression(max_iter=1000000))])"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "eQvmdE3XW4Hr"
      },
      "outputs": [],
      "source": [
        "y_pred = pipeline.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRj45ir6W-UT",
        "outputId": "deea17ec-a58e-47d3-b8fc-15038f1a9588"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[712  14]\n",
            " [373  18]]\n"
          ]
        }
      ],
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mdx_CTbzXCWw",
        "outputId": "3eb0130f-29e2-491a-b57f-2ac78177a1ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1       0.66      0.98      0.79       726\n",
            "           1       0.56      0.05      0.09       391\n",
            "\n",
            "    accuracy                           0.65      1117\n",
            "   macro avg       0.61      0.51      0.44      1117\n",
            "weighted avg       0.62      0.65      0.54      1117\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(classification_report(y_test, y_pred))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Tarea_3_Word_Embeddings_new.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "65e97ffa20fbdd895c76c3a1d2c2ade704754874d6e45df701862667e65d73d7"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 ('Programacion_Cientifica')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
