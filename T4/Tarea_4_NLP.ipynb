{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarea 4 - NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "oiwJb_vmkKLZ",
        "YChwpNrrNRBe"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwaDuQqCOyLJ"
      },
      "source": [
        "# **Tarea 4 - CC6205 Natural Language Processing üìö**\n",
        "\n",
        "**Integrantes:** Sebasti√°n Tinoco, Jos√© Luis C√°diz\n",
        "\n",
        "**Fecha l√≠mite de entrega üìÜ:** Martes 14 de junio.\n",
        "\n",
        "**Tiempo estimado de dedicaci√≥n:** 6 horas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4lL5hGw07yP"
      },
      "source": [
        "Bienvenid@s a la cuarta tarea del curso de Natural Language Processing (NLP). \n",
        "En esta tarea estaremos tratando el problema de **tagging** (generaci√≥n de secuencias de etiquetas del mismo largo que la secuencia de input), el uso de **Convolutional Neural Networks** y **Recurrent Neural Networks**, e implementaremos una red usando PyTorch. \n",
        "\n",
        "Usen $\\LaTeX$ para las f√≥rmulas matem√°ticas. En la parte de programaci√≥n pueden usar lo que quieran, pero la [Auxiliar 3](https://youtu.be/36WTXvg3zh0) les puede ser de *gran ayuda*.\n",
        "\n",
        "**Instrucciones:**\n",
        "- La tarea se realiza en grupos de **m√°ximo** 2 personas. Puede ser invidivual pero no es recomendable.\n",
        "- La entrega es a trav√©s de u-cursos a m√°s tardar el d√≠a estipulado arriba. No se aceptan atrasos.\n",
        "- El formato de entrega es este mismo Jupyter Notebook.\n",
        "- Al momento de la revisi√≥n tu c√≥digo ser√° ejecutado. Por favor verifica que tu entrega no tenga errores de compilaci√≥n.\n",
        "- En el horario de auxiliar pueden realizar consultas acerca de la tarea a trav√©s del canal de Discord del curso.\n",
        "\n",
        "Si a√∫n no han visto las clases, se recomienda visitar los links de las referencias.\n",
        "\n",
        "**Referencias:**\n",
        "\n",
        "- [Tagging, and Hidden Markov Models ](http://www.cs.columbia.edu/~mcollins/cs4705-spring2019/slides/tagging.pdf) (slides by Michael Collins), [notes](http://www.cs.columbia.edu/~mcollins/hmms-spring2013.pdf), [video 1](https://youtu.be/-ngfOZz8yK0), [video 2](https://youtu.be/Tjgb-yQOg54), [video 3](https://youtu.be/aaa5Qoi8Vco), [video 4](https://youtu.be/4pKWIDkF_6Y)       \n",
        "- [MEMMs and CRFs](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-CRF.pdf): [notes 1](http://www.cs.columbia.edu/~mcollins/crf.pdf), [notes 2](http://www.cs.columbia.edu/~mcollins/fb.pdf), [video 1](https://youtu.be/qlI-4lSUDkg), [video 2](https://youtu.be/PLoLKQwkONw), [video 3](https://youtu.be/ZpUwDy6o28Y)\n",
        "- [Convolutional Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-CNN.pdf): [video](https://youtu.be/lLZW5Fn40r8)\n",
        "- [Recurrent Neural Networks](https://github.com/dccuchile/CC6205/blob/master/slides/NLP-RNN.pdf): [video 1](https://youtu.be/BmhjUkzz3nk), [video 2](https://youtu.be/z43YFR1iIvk), [video 3](https://youtu.be/7L5JxQdwNJk)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hidden Markov Models (HMM), Maximum Entropy Markov Models (MEMM) and Conditional Random Field(CRF) (1,5 puntos)"
      ],
      "metadata": {
        "id": "ANqzQ3G9WNw3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWXD3D7RYKJ-"
      },
      "source": [
        "### Pregunta 1 (1 pt)\n",
        "Para un problema de POS tagging se define el conjunto de etiquetas $S = \\{ \\text{DET}, \\text{NOUN}, \\text{VERB}, \\text{ADP} \\}$ y se tiene un Hidden Markov Model con los siguientes par√°metros estimados a partir de un corpus de entrenamiento:\n",
        "\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "q(\\text{NOUN}| \\text{ VERB}, \\text{DET}) &= 0.3 \\\\\n",
        "q(\\text{NOUN}|\\ w, \\text{DET}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
        "q(\\text{DET}| \\text{ VERB}, \\text{NOUN}) &= 0.4 \\\\\n",
        "q(\\text{DET}|\\ w, \\text{NOUN}) &= 0 \\qquad \\forall w \\in S, w \\neq \\text{VERB} \\\\\n",
        "e(the|\\text{ DET}) &= 0.5 \\\\\n",
        "e(pasta|\\text{ NOUN}) &= 0.6\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Luego para la oraci√≥n: `the man is pouring sauce on the pasta`, se tiene una tabla de programaci√≥n din√°mica con los siguientes valores:\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\pi(7,\\text{DET},\\text{DET})&=0.1\\\\\n",
        "\\pi(7,\\text{NOUN},\\text{DET})&=0.2\\\\\n",
        "\\pi(7,\\text{VERB},\\text{DET})&=0.01\\\\\n",
        "\\pi(7,\\text{ADP},\\text{DET})&=0.5\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "Con esta informaci√≥n, calcule el valor de $\\pi(8,\\text{DET},\\text{NOUN})$. Puede dejar el resultado expresado como una fracci√≥n.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Respuesta**\n",
        "\n",
        "$$\\pi (8, \\text{DET}, \\text{NOUN}) = \\max_{w \\in S} \\pi(7, w, \\text{DET}) \\times q(\\text{NOUN}|w, \\text{DET}) \\times e(pasta|\\text{NOUN})$$\n",
        "\n",
        "Para $w = \\text{DET}$:\n",
        "\n",
        "$$\\pi(7, \\text{DET}, \\text{DET}) \\times q(\\text{NOUN}|\\text{DET}, \\text{DET}) \\times e(pasta|\\text{NOUN})$$\n",
        "\n",
        "$$0.1 \\times 0 \\times 0.6 = 0$$\n",
        "\n",
        "Para $w = \\text{NOUN}$:\n",
        "\n",
        "$$\\pi(7, \\text{NOUN}, \\text{DET}) \\times q(\\text{NOUN}|\\text{NOUN}, \\text{DET}) \\times e(pasta|\\text{NOUN})$$\n",
        "\n",
        "$$0.2 \\times 0 \\times 0.6 = 0$$\n",
        "\n",
        "Para $w = \\text{VERB}$:\n",
        "\n",
        "$$\\pi(7, \\text{VERB}, \\text{DET}) \\times q(\\text{NOUN}|\\text{VERB}, \\text{DET}) \\times e(pasta|\\text{NOUN})$$\n",
        "\n",
        "$$0.01 \\times 0.3 \\times 0.6 = 0.0018$$\n",
        "\n",
        "Para $w = \\text{ADP}$:\n",
        "\n",
        "$$\\pi(7, \\text{ADP}, \\text{DET}) \\times q(\\text{NOUN}|\\text{ADP}, \\text{DET}) \\times e(pasta|\\text{NOUN})$$\n",
        "\n",
        "$$0.5 \\times 0 \\times 0.6 = 0$$"
      ],
      "metadata": {
        "id": "hXs1l-CY0mbe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiwJb_vmkKLZ"
      },
      "source": [
        "### Pregunta 2 (0.5 pts)\n",
        "Comente  sobre las similitudes o diferencias entre los HMMs, MEMMs y CRFs. Para esto, responda las siguientes preguntas.\n",
        "\n",
        "#### 2.1. ¬øPara qu√© tipo de tarea sirven? D√© dos ejemplo de este tipo de tarea y descr√≠balos brevemente. (0.1 pts)\n",
        "\n",
        "**Respuesta:** \n",
        "\n",
        "Los HMMs, MEMMs y CRFs sirven para resolver tareas de *tagging* (tambi√©n llamado *sequence labeling*). Este problema se caracteriza por tener como input una oraci√≥n y salida una secuencia de categor√≠as del mismo largo. Un ejemplo de esta tarea es el *Part of Speech Tagging*, donde se busca clasificar el rol sintactico de cada palabra en la oraci√≥n. Otro ejemplo de esta tarea es el *Named Entity Recognition* (NER), donde se busca detectar todas las entidades nombradas en un texto. \n",
        "\n",
        "#### 2.2. ¬øQu√© modelos usan features? ¬øQu√© ventajas conlleva esto? (0.1 pts)\n",
        "\n",
        "**Respuesta:** \n",
        "\n",
        "Los modelos que usan features (representadas por $\\vec \\phi$) son los modelos MEMMs y CRFs. La gran ventaja de usar features es poder usar informaci√≥n que no pueda ser codificada en una HMM (por ejemplo, 1 si la etiqueta $s_i$ es un adverbio y la palabra $x_i$ termina en \"-ly\"). De esta forma, es posible usar *Features Templates*, que son features creadas dise√±adas ex-ante y de esta forma generar autom√°ticamente muchas features y no tener que crearlas a mano. Desde ah√≠, es posible a√±adir features mas espec√≠ficas que involucren prefijos y sufijos.\n",
        "\n",
        "#### 2.3. ¬øC√≥mo maneja cada uno de los modelos las palabras con baja frecuencia en el set de train? (0.1 pts)\n",
        "\n",
        "**Respuesta:** \n",
        "\n",
        "En el caso de las HMM, un truco para tratar con las palabras con baja o nula frecuencia es dividir el vocabulario en 2 conjuntos: palabras frecuentes y palabras infrecuentes. La idea es mapear las palabras con poca frecuencia a un espacio peque√±o y finito que dependa de reglas gramaticas como sufijos, prefijos, etc. De esta forma, se puede rescatar la informaci√≥n de estas palabras y evitar que las probabilidades caigan a 0. Por otra parte, tanto las MEMM como las CRF asignan pesos cercanos a 0 (o incluso pesos negativos) para eventos infrecuentes, favoreciendo as√≠ con una mayor probabilidad a los sucesos que ocurren con mayor frecuencia en el corpus de entrenamiento.\n",
        "\n",
        "#### 2.4. ¬øQu√© le permite a los CRF realizar decisiones globales? ¬øQu√© diferencia con respecto a los MEMMs permite lograr esto? ¬øPor qu√© los HMMs tampoco son capaces de tomar decisiones globales? (0.1 pts)\n",
        "\n",
        "**Respuesta:** \n",
        "\n",
        "Los CRF logran realizar decisiones globales pues no esta restringida a la posici√≥n de la etiqueta sino que toma la secuencia completa. Para esto, se restringe el vector de features sea la suma de vectores locales para todas las transiciones de etiquetas. De esta forma, mientras que las MEMMs se construyen a partir de estos vectores locales, la CRF es mucho mas poderosa pues toma en cuenta todos los vectores locales (secuencias) posibles. Por √∫ltimo, los HMM tampoco son capaces de tomar decisiones globales pues se construyen en base a un modelo de n-gramas con supuestos markovianos y no siempre ser√°n capaces de modelar toda el largo de la secuencia de una sola vez.\n",
        "\n",
        "#### 2.5 Dado una secuencia de $x_1, ..., x_m$ ¬øCu√°ntas posibles secuencias de etiquetas se pueden generar para un conjunto de etiquetas $S$ con $|S|=k$ ? ¬øAnalizarlas todas ser√≠a computacionalmente tratable? (0.1 pts)\n",
        "\n",
        "**Respuesta:**\n",
        "\n",
        "Para una secuencia dada de palabras $x_1, ..., x_m$ existen $k^m$ posibles secuencias que se pueden generar. Para que sea tratable computacionalmente se puede imponer supuestos markovianos y as√≠ descomponer las probabilidades seg√∫n la *memoria* del modelo (dada por el supuesto). Una alternativa mas ambiciosa de esto es lo que se hace con las CRF, donde se quita el supuesto de independencia y se obtienen las probabilidades de todas las secuencias posibles (se hace uso del algoritmo de Viterbi). Al ser un modelo tan complejo, es necesario hacer uso del Forward-backward algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks (0,5 puntos)"
      ],
      "metadata": {
        "id": "44ACHHZIWGF1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClRAHR95Y8aB"
      },
      "source": [
        "### Pregunta 3 (0,5 puntos)\n",
        "\n",
        "Considere la frase $w_{1..7}=$ `El agua moja y el fuego quema` $=[El, agua, moja, y, el, fuego, quema]$.\n",
        "\n",
        "La siguiente matriz de embeddings, donde la i-√©sima fila corresponde al vector de embedding de la i-√©sima palabra, ordenadas seg√∫n aparecen en la frase. (vectores de largo 2).\n",
        "\\begin{equation}\n",
        "E = \\begin{pmatrix}\n",
        "2 & 2\\\\\n",
        "0 & -2\\\\\n",
        "0 & 1\\\\\n",
        "-2 & 1\\\\\n",
        "1 & 0\\\\\n",
        "-1 & 1\\\\\n",
        "1 & 1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Los siguientes 3 filtros\n",
        "\\begin{equation}\n",
        "U = \\begin{pmatrix}\n",
        "-1 & 1 & 0\\\\\n",
        "1 & 1 & 0\\\\\n",
        "0 & 0 & -1\\\\\n",
        "1 & -1 & -1\\\\\n",
        "-1 & -1 & 1\\\\\n",
        "1 & 0 & -1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Y la funci√≥n de activaci√≥n\n",
        "\\begin{equation}\n",
        "tanh = \\frac{e^{2x} - 1}{e^{2x} + 1}\n",
        "\\end{equation}\n",
        "\n",
        "Usando estos param√°tros escriba los pasos para calcular la representaci√≥n (vector) resultante de aplicar la operaci√≥n de convoluci√≥n (sin padding) + max pooling. ¬øDe qu√© tama√±o ser√≠a la ventana que debemos usar?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlQ30Arkq0u4"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "Tomando como punto inicial la matriz de embeddings $E$, el primer paso es generar el vector concatenado $\\vec x_i$, el cual se define por:\n",
        "\n",
        "$$\\vec x_i = \\oplus(w_{i:i+k-1}) = [\\vec w_i; \\vec w_{i+k}, ... , \\vec w_{i+k-1}]$$\n",
        "\n",
        "donde $k$ es el tama√±o de la ventana, $\\vec w_i$ es la fila i-√©sima de la matriz de embeddings $E$ y $\\vec x_i$ tiene dimensiones $1x6$ (la raz√≥n de esto √∫ltimo se explica m√°s adelante).\n",
        "\n",
        "Luego, se realiza la operaci√≥n convoluci√≥n para obtener el vector $\\vec p_i$:\n",
        "\n",
        "$$\\vec p_i = tanh(\\vec x_i \\cdot U)$$\n",
        "\n",
        "donde $p_i$ es de dimensiones $1x3$.\n",
        "\n",
        "Finalmente, tomando el conjunto de vectores $p_i$ generados por la convoluci√≥n, se hace max pooling para cada dimensi√≥n $j$:\n",
        "\n",
        "$$\\vec c_{[j]}= \\max_{1<i‚â§m} \\vec p_{i[j]} \\forall j \\in [1, l]$$\n",
        "\n",
        "donde $m$ es la cantidad de vectores generados y $l$ es la cantidad de dimensiones. Todo lo anterior resulta en un vector $c$ de $l$ dimensiones. Como en este caso $l = 3$, $c$ es un vector de dimensiones $1x3$.\n",
        "\n",
        "Por √∫ltimo, considerando que el filtro $U$ tiene dimensiones de $6x3$, el vector input a multiplicar necesariamente debe ser de $1x6$. Luego, como los embeddings son de tama√±o 2 y no se debe realizar padding, la ventana $k$ necesariamente debe ser de tama√±o 3 (es decir, es un modelo de trigramas)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks (1 punto)\n"
      ],
      "metadata": {
        "id": "A0rCwen3WREC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0et78Z4oKIq"
      },
      "source": [
        "### Pregunta 4 (0,5 puntos)\n",
        "Usando los embeddings de dos dimensiones de la pregunta anteror, la oraci√≥n `el fuego quema` la podemos representar por una secuencia de vectores $(\\vec{x}_1,\\vec{x}_2,\\vec{x}_3)$, con $\\vec{x}_i \\in \\mathbb{R}^{d_x}$ y $d_x=2$.\n",
        "\n",
        "Tenemos una red recurrente *Elman* definidad como: \n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\vec{s}_i &= R_{SRNN}\\left (\\vec{x}_i, \\vec{s}_{i-1}\\right ) = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right ) \\\\\n",
        "\\vec{y}_i &= O_{SRNN}\\left(\\vec{s}_i\\right) = \\vec{s}_i \\\\\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "donde\n",
        "\\begin{equation}\n",
        "\\vec{s}_i, \\vec{y}_i \\in \\mathbb{R}^{d_s}, \\quad W^x \\in \\mathbb{R}^{d_x \\times d_s}, \\quad W^s \\in \\mathbb{R}^{d_s \\times d_s}, \\quad \\vec{b} \\in \\mathbb{R}^{d_s},\n",
        "\\end{equation}\n",
        "y los vectores de estado $s_i$ son de tres dimensiones, $ds= 3$.\n",
        "\n",
        "Sea\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\vec{s}_0 &= [0,0,0]\\\\\n",
        "W^x &= \\begin{pmatrix}\n",
        "0 &  0 & 1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix} \\\\\n",
        "W^s &= \\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix} \\\\\n",
        "\\vec{b} &= [0, 0, 0] \\\\\n",
        "g(x) &= ReLu(x) = max(0, x)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "<br>\n",
        "\n",
        "Calcule manualmente los valores de los vectores $\\vec{s}_1, \\vec{s}_2,\\vec{s}_3$ y de $\\vec{y}_1, \\vec{y}_2,\\vec{y}_3$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fim2W8JioPhL"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "Sea $z_i = g \\left (\\vec{s}_{i-1}W^s + \\vec{x}_i W^x + \\vec{b}\\right )$ y $\\vec s_i = ReLu(z_i)$:\n",
        "\n",
        "Tomando como input \"el\":\n",
        "\n",
        "\\begin{equation}\n",
        "z_1 = \\begin{pmatrix}\n",
        "0 &  0 & 0\n",
        "\\end{pmatrix} \\cdot \n",
        "\\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix}\n",
        "+ \\begin{pmatrix}\n",
        "1 & 0\n",
        "\\end{pmatrix} \\cdot\n",
        "\\begin{pmatrix}\n",
        "0 & 0 &  1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "z_1 = \\begin{pmatrix}\n",
        "0 & 0 & 1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\vec s_1 = \\vec y_1 = \\begin{pmatrix}\n",
        "0 & 0 & 1\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Usando $\\vec s_1$ y tomando como input \"fuego\":\n",
        "\n",
        "\\begin{equation}\n",
        "z_2 = \\begin{pmatrix}\n",
        "0 & 0 & 1\n",
        "\\end{pmatrix} \\cdot \n",
        "\\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix}\n",
        "+ \\begin{pmatrix}\n",
        "-1 & 1\n",
        "\\end{pmatrix} \\cdot\n",
        "\\begin{pmatrix}\n",
        "0 & 0 &  1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "z_2 = \\begin{pmatrix}\n",
        "2 & 0 & 0\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\vec s_2 = \\vec y_2 = \\begin{pmatrix}\n",
        "2 & 0 & 0\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "Finalmente, Usando $\\vec s_2$ y tomando como input \"quema\":\n",
        "\n",
        "\\begin{equation}\n",
        "z_3 = \\begin{pmatrix}\n",
        "2 & 0 & 0\n",
        "\\end{pmatrix} \\cdot \n",
        "\\begin{pmatrix}\n",
        "1 & 0 &  1\\\\\n",
        "0 & 1 & -1\\\\\n",
        "1 & 1 &  1\n",
        "\\end{pmatrix}\n",
        "+ \\begin{pmatrix}\n",
        "1 & 1\n",
        "\\end{pmatrix} \\cdot\n",
        "\\begin{pmatrix}\n",
        "0 & 0 &  1\\\\\n",
        "1 & -1 & 0\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "z_3 = \\begin{pmatrix}\n",
        "3 & -1 & 3\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\vec s_3 = \\vec y_3 = \\begin{pmatrix}\n",
        "3 & 0 & 3\n",
        "\\end{pmatrix}\n",
        "\\end{equation}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4rAT6ELxRZW"
      },
      "source": [
        "### Pregunta 5 (0.5 puntos)\n",
        "¬øDe qu√© forma las RNN y las CNN logran aprender representaciones espec√≠ficas\n",
        "para la tarea objetivo? Compare la forma en que las RNN y las CNN aprenden con los modelos que usan *features* dise√±adas manualmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6AXbQSgA_t8"
      },
      "source": [
        "**Respuesta**\n",
        "\n",
        "Una de las virtudes de las redes RNN y CNN es el *representation learning*, es decir, la capacidad de \"aprender\" y generar features en funci√≥n de lograr un mejor ajuste y as√≠ minimizar la funci√≥n loss. En el caso de la CNN, las features son generadas a trav√©s de la operaci√≥n convoluci√≥n, donde lo que la red aprende es a optimizar los pesos del kernel, aprendiendo as√≠ a resaltar informaci√≥n relevante para la predicci√≥n. Un aspecto importante a considerar es que la CNN es invariante a la traslaci√≥n en el sentido de que no le es importante en que orden se le entrega informaci√≥n, lo cual es contradictorio con el paradigma de NLP en el cual s√≠ importa el orden en el que se escribe la oraci√≥n.\n",
        "\n",
        "Por otro lado, la RNN genera features de forma secuencial, donde el procesamiento del componente i-√©simo de una secuencia impacta sobre el procesamiento del componente $i+1$. Aplicando lo anterior al contexto de NLP, esto quiere decir que, dado un documento, la RNN genera features a trav√©s del procesamiento iterativo (donde la salida de un token impacta sobre la salida del siguiente) y de esta forma la feature condensa informaci√≥n de toda la oraci√≥n y no s√≥lo del token en s√≠ mismo. As√≠, las RNN son capaces de extraer informaci√≥n que fue importante almacen√°ndola en sus estados. Por otro lado, es importante notar que los pesos de todas las etapas de la RNN son compartidos. \n",
        "\n",
        "Finalmente, las features creadas por ambas redes son mejores que las dise√±adas manualmente pues son aprendidas de forma autom√°tica en funci√≥n de la loss y no involucran el juicio del programador."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pregunta 6: Redes Neuronales con Pytorch (3 puntos) üí¨\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.anda.cl/wp-content/uploads/2021/03/0_5vNAtimPjYQr4W72.gif\" alt=\"chatbot\" width=\"400\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "FRJkBpjWyHnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "example_data = pd.read_json('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json')\n",
        "print(\"Cantidad de tags: \", example_data['intents'].shape[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eKOGlMs3Dx-",
        "outputId": "00ef8c95-75b0-42cf-88e3-0dc4472e9ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de tags:  16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_data[\"intents\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ptubwfO7pF1",
        "outputId": "8180a89d-9496-4138-909d-33abacd98ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'patterns': ['Hi',\n",
              "  'Hey',\n",
              "  'How are you',\n",
              "  'Is anyone there?',\n",
              "  'Hello',\n",
              "  'Good day',\n",
              "  \"What's up\",\n",
              "  'Yo!',\n",
              "  'Howdy',\n",
              "  'Nice to meet you.'],\n",
              " 'responses': ['Hey',\n",
              "  'Hello, thanks for visiting.',\n",
              "  'Hi there, what can I do for you?',\n",
              "  'Hi there, how can I help?',\n",
              "  'Hello, there.',\n",
              "  'Hello Dear',\n",
              "  'Ooooo Hello, looking for someone or something?',\n",
              "  'Yes, I am here.',\n",
              "  'Listening carefully.',\n",
              "  'Ok, I am with you.'],\n",
              " 'tag': 'greeting'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_data[\"intents\"][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BnYez1oGtx3",
        "outputId": "38c30753-e43c-403d-bd4d-8913862dec35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'patterns': ['Bye',\n",
              "  'See you later.',\n",
              "  'Goodbye',\n",
              "  'Have a great day.',\n",
              "  'See you next time.',\n",
              "  'It was my pleassure.',\n",
              "  'Take care.',\n",
              "  'See ya!',\n",
              "  'Catch you later.',\n",
              "  'Ciao.'],\n",
              " 'responses': ['See you later, thanks for visiting.',\n",
              "  'May the force be with you!',\n",
              "  'See next time.',\n",
              "  'Was my pleassuare to meet you.',\n",
              "  'Hope will cath up sortly.',\n",
              "  'Have a nice day.',\n",
              "  'Bye! Come back again soon.',\n",
              "  'So, till next time.',\n",
              "  'If you need anything just text me anytime. Bye.',\n",
              "  'Well, hope see you soon!'],\n",
              " 'tag': 'goodbye'}"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_data[\"intents\"][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ritXSqEg8W9o",
        "outputId": "02f7cbae-5a48-436e-9ddb-272433340f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'patterns': ['Thanks',\n",
              "  'Thank you',\n",
              "  \"That's helpful\",\n",
              "  \"Thank's a lot!\",\n",
              "  'Tnx',\n",
              "  'Wow',\n",
              "  'Great!',\n",
              "  'Good!',\n",
              "  'That nice!',\n",
              "  'Amazing!'],\n",
              " 'responses': ['Happy to help!',\n",
              "  'Any time!',\n",
              "  'My pleasure!',\n",
              "  'No problem!',\n",
              "  'Thans does not ',\n",
              "  'Glad to help!',\n",
              "  'No worries!',\n",
              "  'It was the least I could do!',\n",
              "  'If I had a cent for every time I appreciate you, I‚Äôd be a millionaire.',\n",
              "  \"You can't put thanks in your pocket!\"],\n",
              " 'tag': 'thanks'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Del dataset cargado podemos notar que este viene en un formato `JSON`, por lo que sus datos est√°n almacenados en diccionarios. Las llaves de los diccionarios no son aleatorias y estos nos sirven para identificar puntos relevantes en el desarrollo del bot. A continuaci√≥n, se realiza una peque√±a descripci√≥n de las llaves:\n",
        "\n",
        "- `patterns`: Almacena los patrones con los que entrenaremos el modelo üòÆ, en otras palabras, es el corpus de entrenamiento que contiene solo preguntas o expresiones que deber√° responder el bot.\n",
        "- `responses`: Son las respuestas üôã relacionadas a los `patterns`, estas las utilizaremos en una etapa posterior a la clasificaci√≥n, para dar una respuesta aleator√≠a al usuario.\n",
        "- `tag`: Son las labels con las que entrenaremos nuestro modelo üíª. \n",
        "\n",
        "En s√≠ntesis, las `keys` relevantes para el entrenamiento de nuestra red neuronal ser√°n `patterns` (corpus) y `tag` (etiquetas)."
      ],
      "metadata": {
        "id": "v6BvAWCw3zPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Explicaci√≥n de la tarea a realizar:"
      ],
      "metadata": {
        "id": "KlOAdMjSSzNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explicaci√≥n de la tarea a realizar**: Implemente una Class llamada `CNNClassifier` que sea capaz de entrenar un modelo de texto a trav√©s de una red neuronal Feed Forward y una arquitectura convolucional (CNN 1D) [`torch.nn.Conv1d`](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html#conv1d) . Para el dise√±o de las redes tienen completa libertad, pero se le aconseja que se gu√≠en de la √∫ltima auxiliar para la construcci√≥n. Es **important√≠simo** que el modelo a crear posea una capa de `Embedding` que se genere en base al entrenamiento del modelo. Creado el modelo, construya una funci√≥n batch para cargar los datos de entrenamiento del modelo.\n",
        "\n",
        "Construido el modelo, compare los resultados obtenidos para una red feed forward y una cnn. Para la comprobaci√≥n de sus resultados ejecute el chatbot y pruebelo, ¬øqu√© configuraci√≥n tiene mejores resultados?, ¬øa qu√© se deberan estos resultados?\n",
        "\n",
        "Ojo que un ejemplo de prueba con el chatbot puede ser (agregue mas preguntas ud):\n",
        "\n",
        "```\n",
        "Let's chat! (type 'finish_chat' to finish the chat)\n",
        "You: hi\n",
        "GA-97: Yes, I am here.\n",
        "You: can you tell me a joke?\n",
        "GA-97: Have you tried the gluten-free Wookiee treats? No, but I heard they are a little Chewy.\n",
        "```\n",
        "\n",
        "El resto del c√≥digo referido a la ejecuci√≥n del chat se los entregamos, por lo que no deber√≠an tener mayores problemas üò∏ (en caso de tener problemas con su c√≥digo, puede modificar cualquier parte sugerida siempre y cuando cumpla lo solicitado).\n",
        "\n",
        "**Igual [mucho texto](https://i0.wp.com/elgeneracionalpost.com/wp-content/uploads/2020/07/mucho-texto.jpg?fit=1280%2C720&ssl=1).... En resumen, ¬øQu√© se solicita?:**\n",
        "\n",
        "- [x] Dise√±ar una red neuronal Feed Forward.\n",
        "- [x] Dise√±ar un red convolucional.\n",
        "- [x] Utilizar una capa de embeddings para generar representaciones vectoriales del corpus.\n",
        "- [x] Crear el m√©todo forward de la clase `CNNClassifier`.\n",
        "- [x] Crear la funci√≥n BATCH.\n",
        "- [x] Probar el modelo y comparar los resultados obtenidos con la red Feed Forward y la red CNN. Comente sus resultados de forma cualitativa, se√±alando con qu√© tipo de red obtuvo mejores resultados con el chatbot.\n",
        "\n",
        "**Nota-1:** El modelo creado debe tener la opci√≥n de entrenar a traves de una feed forward y una CNN. Esto no significa que entrenar√° una FF y una CN, el modelo deber√° recibir un booleano que especifique que tipo de red utilizar√°.\n",
        "\n",
        "**Nota-2:** El dataset se descargar√° autom√°ticamente en la secci√≥n `Carga de Dataset üìö`, no os preocup√©is."
      ],
      "metadata": {
        "id": "9yGApnWVI4cO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pasemos al C√≥digo ü¶æ\n",
        "\n",
        "Esqueleto propuesto (se **RECOMIENDA** que cambien **SOLO** la red neuronal y la funci√≥n Batch) ü¶¥:"
      ],
      "metadata": {
        "id": "a4bKfAdEy3oD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Instalamos librerias necesarias e importamos üòÄ"
      ],
      "metadata": {
        "id": "RUwxivx2MpMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Esto toma su tiempo en ejecutarse\n",
        "%%capture\n",
        "!pip install torch==1.8.0+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torchtext==0.9.0"
      ],
      "metadata": {
        "id": "TjSZkBsk1H4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import torch\n",
        "import random\n",
        "from random import choice\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torch.optim import SGD, lr_scheduler\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from itertools import zip_longest\n",
        "\n",
        "import plotly.express as px\n",
        "\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "RfZ6SL-Q1Kwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Carga de Dataset üìö"
      ],
      "metadata": {
        "id": "oj-Epe7XJLrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we obtain the dataset\n",
        "!wget 'https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvlLqYRrVN6l",
        "outputId": "0fd17052-16f0-481c-a733-a73db131e83d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-19 22:46:07--  https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/star_wars_chatbot.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14469 (14K) [text/plain]\n",
            "Saving to: ‚Äòstar_wars_chatbot.json.1‚Äô\n",
            "\n",
            "\rstar_wars_chatbot.j   0%[                    ]       0  --.-KB/s               \rstar_wars_chatbot.j 100%[===================>]  14.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-06-19 22:46:07 (36.0 MB/s) - ‚Äòstar_wars_chatbot.json.1‚Äô saved [14469/14469]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset using json\n",
        "with open('star_wars_chatbot.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Create a vocab with the dataset and get the number of classes that have\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "vocab = build_vocab_from_iterator(tokenizer(x) for list_words in dataset['intents'] for x in list_words['patterns'])\n",
        "num_classes = len(dataset['intents'])\n",
        "\n",
        "# Define a list with the labels\n",
        "labels = sorted(set([tag for tag in [intents['tag'] for intents in dataset['intents']]]))\n",
        "# Define a train_list where we can find the info in the format: [(tag_0, text_0)...,(tag_n-1, text_n-1)]\n",
        "train_list = [(labels.index(intents['tag']), text) for intents in dataset['intents'] for text in intents['patterns']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4cTRK1fkXsuy",
        "outputId": "79b9aff4-c214-44c0-bb23-02adaad0a3c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "97lines [00:00, 31504.37lines/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(dataset[\"intents\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "rqK1lT5K_fdm",
        "outputId": "e3a80cfa-ee6b-4e93-df6d-8e212d965583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "               tag                                           patterns  \\\n",
              "0         greeting  [Hi, Hey, How are you, Is anyone there?, Hello...   \n",
              "1          goodbye  [Bye, See you later., Goodbye, Have a great da...   \n",
              "2           thanks  [Thanks, Thank you, That's helpful, Thank's a ...   \n",
              "3            tasks  [What can you do?, What are your features?, Wh...   \n",
              "4            alive    [Are you alive., Do you breathe., Can you run.]   \n",
              "5             Menu  [Which items do you have in your bar?, What ki...   \n",
              "6             help  [I am looking for help., I need help., Can you...   \n",
              "7          mission  [I am on mission., I need assistance in my mis...   \n",
              "8             jedi  [Tell me top 10 jedi?, Who is the best jedi in...   \n",
              "9             sith  [Tell me top 10 sith?, Who is the best sith in...   \n",
              "10  bounti hounter  [Tell me top 10 bounti hounter?, Who is the bo...   \n",
              "11           funny  [Tell me a joke!, Can you be a bit funny, Tell...   \n",
              "12        about me  [Do you know me?, Who am I, Tell me about myse...   \n",
              "13         creator  [Who is your creator?, Who created you, Who is...   \n",
              "14          myself  [Tell me about Mr. Fbravo?, Who is Mr. Fbravo,...   \n",
              "15         stories       [Tell me a story?, Can you tell me a story.]   \n",
              "\n",
              "                                            responses context_set  \n",
              "0   [Hey, Hello, thanks for visiting., Hi there, w...         NaN  \n",
              "1   [See you later, thanks for visiting., May the ...         NaN  \n",
              "2   [Happy to help!, Any time!, My pleasure!, No p...         NaN  \n",
              "3   [I can do whatever you asks me to do, I can ta...         NaN  \n",
              "4   [I'm in doubt about that, No, i don't think so...         NaN  \n",
              "5   [I could serve for your: Fuzzy Tauntaun, Blood...         NaN  \n",
              "6   [Sure, how can in help you., Tell me what do y...         NaN  \n",
              "7   [Just tell me, you are looking for jedi or sit...         NaN  \n",
              "8   [Here is top 10 jedi you are looking for. Luke...         NaN  \n",
              "9   [Here is top 10 sith you are looking for: Dart...         NaN  \n",
              "10  [Here is top 10 bounti hounter you are looking...         NaN  \n",
              "11  [Which Jedi became a rock star? Bon Jovi-Wan K...         NaN  \n",
              "12  [Yes, you are a human, You are a dumb person a...         NaN  \n",
              "13  [That would be you Mr. Fbravo., I was created ...              \n",
              "14  [A very intelligent being who created me, My c...         NaN  \n",
              "15  [I can't think of anything right now., It woul...         NaN  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d0b918d8-5b65-4ad5-b3e7-e7668edafc4b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>patterns</th>\n",
              "      <th>responses</th>\n",
              "      <th>context_set</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>greeting</td>\n",
              "      <td>[Hi, Hey, How are you, Is anyone there?, Hello...</td>\n",
              "      <td>[Hey, Hello, thanks for visiting., Hi there, w...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>goodbye</td>\n",
              "      <td>[Bye, See you later., Goodbye, Have a great da...</td>\n",
              "      <td>[See you later, thanks for visiting., May the ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>thanks</td>\n",
              "      <td>[Thanks, Thank you, That's helpful, Thank's a ...</td>\n",
              "      <td>[Happy to help!, Any time!, My pleasure!, No p...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tasks</td>\n",
              "      <td>[What can you do?, What are your features?, Wh...</td>\n",
              "      <td>[I can do whatever you asks me to do, I can ta...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>alive</td>\n",
              "      <td>[Are you alive., Do you breathe., Can you run.]</td>\n",
              "      <td>[I'm in doubt about that, No, i don't think so...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Menu</td>\n",
              "      <td>[Which items do you have in your bar?, What ki...</td>\n",
              "      <td>[I could serve for your: Fuzzy Tauntaun, Blood...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>help</td>\n",
              "      <td>[I am looking for help., I need help., Can you...</td>\n",
              "      <td>[Sure, how can in help you., Tell me what do y...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mission</td>\n",
              "      <td>[I am on mission., I need assistance in my mis...</td>\n",
              "      <td>[Just tell me, you are looking for jedi or sit...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>jedi</td>\n",
              "      <td>[Tell me top 10 jedi?, Who is the best jedi in...</td>\n",
              "      <td>[Here is top 10 jedi you are looking for. Luke...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>sith</td>\n",
              "      <td>[Tell me top 10 sith?, Who is the best sith in...</td>\n",
              "      <td>[Here is top 10 sith you are looking for: Dart...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>bounti hounter</td>\n",
              "      <td>[Tell me top 10 bounti hounter?, Who is the bo...</td>\n",
              "      <td>[Here is top 10 bounti hounter you are looking...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>funny</td>\n",
              "      <td>[Tell me a joke!, Can you be a bit funny, Tell...</td>\n",
              "      <td>[Which Jedi became a rock star? Bon Jovi-Wan K...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>about me</td>\n",
              "      <td>[Do you know me?, Who am I, Tell me about myse...</td>\n",
              "      <td>[Yes, you are a human, You are a dumb person a...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>creator</td>\n",
              "      <td>[Who is your creator?, Who created you, Who is...</td>\n",
              "      <td>[That would be you Mr. Fbravo., I was created ...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>myself</td>\n",
              "      <td>[Tell me about Mr. Fbravo?, Who is Mr. Fbravo,...</td>\n",
              "      <td>[A very intelligent being who created me, My c...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>stories</td>\n",
              "      <td>[Tell me a story?, Can you tell me a story.]</td>\n",
              "      <td>[I can't think of anything right now., It woul...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d0b918d8-5b65-4ad5-b3e7-e7668edafc4b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d0b918d8-5b65-4ad5-b3e7-e7668edafc4b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d0b918d8-5b65-4ad5-b3e7-e7668edafc4b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creaci√≥n del modelo (2 puntos en total)"
      ],
      "metadata": {
        "id": "a52SUNKPJQxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construya el modelo\n",
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim=32, num_classes=10, \n",
        "                 use_cnn=False, cnn_pool_channels=24, cnn_kernel_size=3):\n",
        "      super().__init__()\n",
        "      self.use_cnn=use_cnn\n",
        "      self.cnn_kernel_size=cnn_kernel_size\n",
        "      \n",
        "      if self.use_cnn==False:\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        \n",
        "        # Capa MLP           \n",
        "        self.fc1 = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Init weights\n",
        "        self.init_weights()\n",
        "      else:\n",
        "\n",
        "        # Capa de embedding\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # capa de convoluci√≥n\n",
        "        self.conv = nn.Conv1d(\n",
        "            in_channels=1,\n",
        "            out_channels=cnn_pool_channels,\n",
        "            kernel_size=cnn_kernel_size * embed_dim,\n",
        "            stride=embed_dim,\n",
        "        )\n",
        "        fc_in_size = cnn_pool_channels\n",
        "\n",
        "        # Capa MLP          \n",
        "        self.fc1 = nn.Linear(fc_in_size, num_classes)\n",
        "\n",
        "        # Init weights\n",
        "        self.init_weights()\n",
        "\n",
        "\n",
        "    def init_weights(self):\n",
        "      # Inicializar pesos\n",
        "      initrange = 0.5\n",
        "      self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "      self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "      self.fc1.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "      \n",
        "      # Agregamos padding para evitar errores en predicci√≥n, este es en funci√≥n del kernel size\n",
        "      text=torch.cat([text]+[torch.tensor([1]) for i in range(self.cnn_kernel_size)], 0)      \n",
        "\n",
        "      text = torch.tensor(\n",
        "          list(\n",
        "              zip(\n",
        "                  *zip_longest(\n",
        "                      *([text[o:offsets[i+1]] for i, o in enumerate(offsets[:-1])] + [text[offsets[-1]:len(texts)]]), \n",
        "                      fillvalue=vocab[\"<pad>\"]\n",
        "                  )\n",
        "              )\n",
        "          )\n",
        "      ).to(text.device)\n",
        "\n",
        "      #print(text.shape)\n",
        "\n",
        "      if self.use_cnn==False:\n",
        "        # La representacion de un documento sera el promedio de los\n",
        "        # embeddings de sus palabras.\n",
        "        h = self.embedding(text)\n",
        "        h = h.mean(dim=1)  \n",
        "        # computar las capas de la red MLP\n",
        "        h = self.fc1(h)\n",
        "        \n",
        "        return h\n",
        "      else:\n",
        "        \n",
        "        h = self.embedding(text)\n",
        "\n",
        "        # (N x pool_channels)\n",
        "        h = h.view(h.size(0), 1, -1)\n",
        "        h = torch.relu(self.conv(h))\n",
        "        h = h.mean(dim=2)\n",
        "\n",
        "        # computar las capas de la red MLP\n",
        "        h = self.fc1(h)\n",
        "\n",
        "        return h"
      ],
      "metadata": {
        "id": "n-vQ24tMJG5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Funci√≥n Batch üë∑ (0,5 puntos)"
      ],
      "metadata": {
        "id": "dGN-T0JoJtmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defina su funci√≥n de BATCH\n",
        "def generate_batch(batch):\n",
        "  label = torch.tensor([entry[0] for entry in batch])\n",
        "  texts = [tokenizer(entry[1]) for entry in batch]\n",
        "  offsets = [0] + [len(text) for text in texts]\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  big_text = torch.cat([torch.tensor([vocab.stoi[t] for t in text]) for text in texts])\n",
        "\n",
        "  return big_text, offsets, label"
      ],
      "metadata": {
        "id": "K1AZpXc7JxTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Entrenamiento ü•ä"
      ],
      "metadata": {
        "id": "YChwpNrrNRBe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sin CNN"
      ],
      "metadata": {
        "id": "emaRT0x7Qu0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device=\"cpu\"\n",
        "print(f\"GPU is avaible: {device}\")\n",
        "\n",
        "# Define the different inputs in our model\n",
        "num_epochs = 1000\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-1\n",
        "INPUT_SIZE = len(vocab)\n",
        "OUTPUT_SIZE = num_classes\n",
        "USE_CNN = False\n",
        "\n",
        "# Define model, optimizer, loss and scheduler (Q: ¬øWhat is it?)\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
        "optimizer = SGD(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: .9 ** (epoch // 10)])\n",
        "\n",
        "print(f'train: {len(train_list)} elements')\n",
        "\n",
        "# We train the model using the intents\n",
        "loss_list= []\n",
        "for epoch in range(1, num_epochs):\n",
        "  train_loader = DataLoader(train_list, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for i, (texts, offsets, cls) in enumerate(train_loader):\n",
        "    texts = texts.to(device)\n",
        "    offsets = offsets.to(device)\n",
        "    cls = cls.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(texts, offsets)\n",
        "    loss = criterion(output, cls)\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  loss_list.append(loss.item())\n",
        "  sys.stdout.write('\\rEpoch: {0:03d} \\t iter-Loss: {1:.3f}'.format(epoch+1, loss.item()))\n",
        "\n",
        "print(f'final loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwoLpytCYv-P",
        "outputId": "60627607-369a-4807-f169-3ee1a56d8240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is avaible: cpu\n",
            "train: 97 elements\n",
            "Epoch: 1000 \t iter-Loss: 0.001final loss: 0.0008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# guardamos Loss para comparar posteriormente\n",
        "loss_list_sin_cnn=loss_list"
      ],
      "metadata": {
        "id": "AiC4u8isVu76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### A probar! üß™"
      ],
      "metadata": {
        "id": "9dlS4_X-L3DN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is working?, Try the next example!\n",
        "qText = \"'Do you know any joke?'\" # this must classify the label \"funny\"\n",
        "\n",
        "X = torch.tensor([vocab.stoi[t] for t in tokenizer(qText)]).to(device)\n",
        "\n",
        "model.eval()\n",
        "output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
        "_, predicted = torch.max(output, dim=1)\n",
        "labels[predicted]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "6IhhAKFXL3eH",
        "outputId": "b9baf3ce-31a9-4ddf-f578-05c61bc1683b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'funny'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya pero prometiste hacer un chatbot, no una simple clasificaci√≥n.... "
      ],
      "metadata": {
        "id": "udemze3zL549"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Guardamos modelo ü¶∫ (opcional)"
      ],
      "metadata": {
        "id": "OpSYGx2tL0tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We save de model using pytorch \n",
        "data = {\n",
        "\"model_state\": model.state_dict(),\n",
        "\"input_size\": INPUT_SIZE,\n",
        "\"output_size\": OUTPUT_SIZE,\n",
        "\"use_cnn\": USE_CNN,\n",
        "\"labels\": labels\n",
        "        }\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(f'training complete. file saved to {FILE}')"
      ],
      "metadata": {
        "id": "ZBC4TyiqLzDv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc7edd4-7de5-4348-bab8-02fea1488ee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training complete. file saved to data.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Chatbot üí¨"
      ],
      "metadata": {
        "id": "ZYClbTtsMCjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device=\"cpu\"\n",
        "with open('star_wars_chatbot.json', 'r') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "data = torch.load(FILE)\n",
        "\n",
        "INPUT_SIZE = data[\"input_size\"]\n",
        "OUTPUT_SIZE = data[\"output_size\"]\n",
        "USE_CNN = data[\"use_cnn\"]\n",
        "labels = data['labels']\n",
        "model_state = data[\"model_state\"]\n",
        "\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()\n",
        "\n",
        "# Dictionary with the answers\n",
        "responses = {key['tag']: key['responses'] for key in dataset['intents']}\n",
        "\n",
        "bot_name = \"GA-97\"\n",
        "print(\"Let's chat! (type 'finish_chat' to finish the chat)\")\n",
        "while True:\n",
        "    q_text = input(\"You: \")\n",
        "    q_text = q_text#+\" <pad> \"+\" <pad> \"\n",
        "    if q_text == 'finish_chat':\n",
        "        break\n",
        "\n",
        "    X = torch.tensor([vocab.stoi[t] for t in tokenizer(q_text)]).to(device)\n",
        "    output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "    tag = labels[predicted.item()]\n",
        "\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "    if prob.item() > 0.50:\n",
        "      print(f\"{bot_name}: {random.choice(responses[tag])}\")\n",
        "    else:\n",
        "      print(f\"{bot_name}: My model can't understand you...\")"
      ],
      "metadata": {
        "id": "c249zUwiMBxb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9444d0db-e17a-479b-b9c0-1b685c497b52"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's chat! (type 'finish_chat' to finish the chat)\n",
            "You: Hello, how are you?\n",
            "GA-97: Yes, I am here.\n",
            "You: how can you help me?\n",
            "GA-97: Ok, you problem is my problem, if your are paying.\n",
            "You: Tell me a Joke\n",
            "GA-97: Why was the droid angry? Because people kept pushing its buttons.\n",
            "You: Do you know me?\n",
            "GA-97: You are a dumb person asking a machine about yourself\n",
            "You: Tell me about Mr bravo?\n",
            "GA-97: My model can't understand you...\n",
            "You: Tell me about Mr Fbravo\n",
            "GA-97: A very intelligent being who created me\n",
            "You: finish_chat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Con CNN"
      ],
      "metadata": {
        "id": "yWEQhGcuQyqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device=\"cpu\"\n",
        "print(f\"GPU is avaible: {device}\")\n",
        "\n",
        "# Define the different inputs in our model\n",
        "num_epochs = 1000\n",
        "BATCH_SIZE = 16\n",
        "LR = 1e-1\n",
        "INPUT_SIZE = len(vocab)\n",
        "OUTPUT_SIZE = num_classes\n",
        "USE_CNN = True\n",
        "\n",
        "# Define model, optimizer, loss and scheduler (Q: ¬øWhat is it?)\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
        "optimizer = SGD(model.parameters(), lr=LR)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=[lambda epoch: .9 ** (epoch // 10)])\n",
        "\n",
        "print(f'train: {len(train_list)} elements')\n",
        "\n",
        "# We train the model using the intents\n",
        "loss_list= []\n",
        "for epoch in range(1, num_epochs):\n",
        "  train_loader = DataLoader(train_list, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  for i, (texts, offsets, cls) in enumerate(train_loader):\n",
        "    texts = texts.to(device)\n",
        "    offsets = offsets.to(device)\n",
        "    cls = cls.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(texts, offsets)\n",
        "    loss = criterion(output, cls)\n",
        "    total_loss += loss.item()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  loss_list.append(loss.item())\n",
        "  sys.stdout.write('\\rEpoch: {0:03d} \\t iter-Loss: {1:.3f}'.format(epoch+1, loss.item()))\n",
        "\n",
        "print(f'final loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ad86539-50c0-4bce-8a42-ff2b397a108c",
        "id": "mlmrffziQ8D2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is avaible: cpu\n",
            "train: 97 elements\n",
            "Epoch: 1000 \t iter-Loss: 0.000final loss: 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# guardamos Loss para comparar posteriormente\n",
        "loss_list_con_cnn=loss_list"
      ],
      "metadata": {
        "id": "Q1_wEU0SYIKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### A probar! üß™"
      ],
      "metadata": {
        "id": "7KXGzid-Q8D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is working?, Try the next example!\n",
        "qText = \"'Do you know any joke?'\" # this must classify the label \"funny\"\n",
        "\n",
        "X = torch.tensor([vocab.stoi[t] for t in tokenizer(qText)]).to(device)\n",
        "\n",
        "model.eval()\n",
        "output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
        "_, predicted = torch.max(output, dim=1)\n",
        "labels[predicted]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "e620515f-86e4-4bef-eb4a-da6074187598",
        "id": "2oWQvAU3Q8D3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'funny'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya pero prometiste hacer un chatbot, no una simple clasificaci√≥n.... "
      ],
      "metadata": {
        "id": "Kajpuw7WQ8D3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Guardamos modelo ü¶∫ (opcional)"
      ],
      "metadata": {
        "id": "a9JzljL_Q8D3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We save de model using pytorch \n",
        "data = {\n",
        "\"model_state\": model.state_dict(),\n",
        "\"input_size\": INPUT_SIZE,\n",
        "\"output_size\": OUTPUT_SIZE,\n",
        "\"use_cnn\": USE_CNN,\n",
        "\"labels\": labels\n",
        "        }\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "torch.save(data, FILE)\n",
        "\n",
        "print(f'training complete. file saved to {FILE}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c60e28c-d41f-4b78-fdda-35a542c7d845",
        "id": "WdZcvWHrQ8D4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training complete. file saved to data.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Chatbot üí¨"
      ],
      "metadata": {
        "id": "fa6GNikNQ8D4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "#device=\"cpu\"\n",
        "with open('star_wars_chatbot.json', 'r') as json_data:\n",
        "    intents = json.load(json_data)\n",
        "\n",
        "FILE = \"data.pth\"\n",
        "data = torch.load(FILE)\n",
        "\n",
        "INPUT_SIZE = data[\"input_size\"]\n",
        "OUTPUT_SIZE = data[\"output_size\"]\n",
        "USE_CNN = data[\"use_cnn\"]\n",
        "labels = data['labels']\n",
        "model_state = data[\"model_state\"]\n",
        "\n",
        "model = CNNClassifier(INPUT_SIZE, num_classes=OUTPUT_SIZE, use_cnn=USE_CNN).to(device)\n",
        "model.load_state_dict(model_state)\n",
        "model.eval()\n",
        "\n",
        "# Dictionary with the answers\n",
        "responses = {key['tag']: key['responses'] for key in dataset['intents']}\n",
        "\n",
        "bot_name = \"GA-97\"\n",
        "print(\"Let's chat! (type 'finish_chat' to finish the chat)\")\n",
        "while True:\n",
        "    q_text = input(\"You: \")\n",
        "    q_text = q_text#+\" <pad> \"+\" <pad> \"\n",
        "    if q_text == 'finish_chat':\n",
        "        break\n",
        "\n",
        "    X = torch.tensor([vocab.stoi[t] for t in tokenizer(q_text)]).to(device)\n",
        "    output = model(X, torch.tensor([0], dtype=torch.long).to(device))\n",
        "    _, predicted = torch.max(output, dim=1)\n",
        "\n",
        "    tag = labels[predicted.item()]\n",
        "\n",
        "    probs = torch.softmax(output, dim=1)\n",
        "    prob = probs[0][predicted.item()]\n",
        "    if prob.item() > 0.50:\n",
        "      print(f\"{bot_name}: {random.choice(responses[tag])}\")\n",
        "    else:\n",
        "      print(f\"{bot_name}: My model can't understand you...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d078ebec-7686-4e7e-c7d5-e327e876be75",
        "id": "dH8qbFolQ8D4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Let's chat! (type 'finish_chat' to finish the chat)\n",
            "You: Hello, how are you?\n",
            "GA-97: Ok, I am with you.\n",
            "You: how can you help me?\n",
            "GA-97: You are at the address.\n",
            "You: Tell me a Joke\n",
            "GA-97: Why was the droid angry? Because people kept pushing its buttons.\n",
            "You: Do you know me?\n",
            "GA-97: Sorry I can't tell that in public, maybe you are jedi\n",
            "You: Tell me about Mr Fbravo\n",
            "GA-97: My creator, and he is a really intelligent man\n",
            "You: Do you know me?\n",
            "GA-97: Sorry I can't tell that in public, maybe you are jedi\n",
            "You: Do you know me?\n",
            "GA-97: Sorry I can't tell that in public, maybe you are jedi\n",
            "You: Do you know me?\n",
            "GA-97: Yes, you are a human\n",
            "You: Do you know me?\n",
            "GA-97: Sorry I can't tell that in public, maybe you are jedi\n",
            "You: finish_chat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "plt.plot(loss_list_sin_cnn,'r')\n",
        "plt.plot(loss_list_con_cnn,'g')\n",
        "plt.legend(['Sin CNN','Con CNN'])\n",
        "plt.title(\"Loss vs √âpoca\")\n",
        "plt.ylabel('Cross-Entropy')\n",
        "plt.xlabel('√âpoca')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "2ZRqRu1RYXZ-",
        "outputId": "574d2f27-af2a-465e-e448-fab1222b9717"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Comente los resultados aqu√≠ (0,5 puntos)"
      ],
      "metadata": {
        "id": "5Hu2QTuSURCt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "``En general ambos modelos tuvieron un buen compartamiento, sin embargo la red con redes convolucionales alcanzo un menor Loss y redujo su error en un n√∫mero menor de √©pocas, lo cual tiene sentido debido a que la capa de convoluci√≥n permite centrarse solamente en las caracter√≠sticas que realmente si son importantes para la predicci√≥n. Adem√°s, se observa que si bien ambas redes obtienen un buen desempe√±o en la etapa de entrenamiento, tienen una mala capacidad de generalizaci√≥n pues al modificarse levemente algunas frases arrojan predicciones incorrectas (\"Tell me about Mr Fbravo\" vs \"Tell me about mr Fbravo?\"). Finalmente, estos resultados se pueden explicar en que las redes dise√±adas no conten√≠an capas de regularizaci√≥n, lo que induc√≠a al sobreajuste y por lo tanto una mala capacidad de generalizar con datos fuera de muestra.``\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fdFV63WVUX32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lRLCOOI22oTk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}